{"0": {
    "doc": "API Reference Documentation",
    "title": "API Reference Documentation",
    "content": "The Open Integration Hub framework consists of a variety of service. Most services interact via a REST API: . Integration Framework Identity Access Management Secret Service Flow Repository Component Repository Attachment Storage-Service Smart Data Framework Reporting and Analytics APIs Audit Log Metadata Repository Data Hub Integration Layer Service ",
    "url": "http://localhost:4000/docs/API%20Reference/APIReferenceOverview.html#api-reference-documentation",
    "relUrl": "/docs/API%20Reference/APIReferenceOverview.html#api-reference-documentation"
  },"1": {
    "doc": "API Reference Documentation",
    "title": "API Reference Documentation",
    "content": ". ",
    "url": "http://localhost:4000/docs/API%20Reference/APIReferenceOverview.html",
    "relUrl": "/docs/API%20Reference/APIReferenceOverview.html"
  },"2": {
    "doc": "Actions and Triggers",
    "title": "Actions and Triggers",
    "content": "As laid out in the development example, actions and triggers are the part of any component, that really defines what you can do with it. Most parts like credentials or the component.json are fairly standard and reusable. Actions and triggers however have to be specifically adapted to your solutions API. This guide helps to classify your API and derive a set of functionalities that can be performed by an Adapter. We focus on the Adapter, as the Transformer is even simpler. So once you understand the Adapter, the rest is a piece of cake. The Adapter is a single, reusable piece of functionality that stands between your solution’s API and the Transformer. To enable communication between you and Open Integration Hub, the Adapter syntactically normalizes and transforms your applications data into a JSON object. For example, transforming CSV-, JS-, XML- files into JSON objects. The Adapter exposes the endpoint of your SaaS solution’s API via pre-defined actions and triggers. Those make sure that the four basic operations of persistent storage are available, such as create, read, update and delete a file. ",
    "url": "http://localhost:4000/docs/Connectors/ActionsAndTriggers.html",
    "relUrl": "/docs/Connectors/ActionsAndTriggers.html"
  },"3": {
    "doc": "Actions and Triggers",
    "title": "Table of Contents",
    "content": ". | Given an API how should an Adapter behave . | Questions . | Question 1: Is the list of business objects dynamic | Question 2: Is the structure of objects dynamic | Question 3: Does the API support Webhooks | . | . | Desired Adapter Behavior | . ",
    "url": "http://localhost:4000/docs/Connectors/ActionsAndTriggers.html#table-of-contents",
    "relUrl": "/docs/Connectors/ActionsAndTriggers.html#table-of-contents"
  },"4": {
    "doc": "Actions and Triggers",
    "title": "Given an API how should an Adapter behave",
    "content": "The expected actions and triggers of an adapter depend on the behavior of the API. If the API supports CRUD operations the following diagram explains which triggers and actions should exist in the adapter. The triggers and actions should aim at covering 100% of the objects provided by the API. Note: Although RESTful APIs are preferred, the API does not necessarily have to be a REST API. It is possible for the above functionality to be exposed via a SOAP API, a SQL (or other) DB connection, etc. Is the list of business objects dynamic?Is the list of business objects dynamic?XORXORDoes the API support webhooks?Does the API support webhooks?Is the structure of objects dynamic?Is the structure of objects dynamic?Does the API support webhooks?Does the API support webhooks?Does the API support webhooks?Does the API support webhooks?XORXORCase 5Case 5Case 6Case 6XORXORCase 3Case 3Case 4Case 4XORXORCase 1Case 1Case 2Case 2XORXORYESYESNONOYESYESNONOYESYESNONOYESYESNONOYESYESNONO Questions . Question 1: Is the list of business objects dynamic . Some systems have a fixed list of objects (and corresponding API endpoints) which exist in the system. Other systems allow users and admins to create new types of objects (which result in the system dynamically creating API endpoints). In this case, it is common for the system to provide an API endpoint which can provide a list of all dynamic objects as well as the structure of every object. If the system has a fixed list of objects, then the adapter developer can provide a hard-coded list of objects that the adapter can interact with. If the list of objects is dynamic and it is possible to use the API to learn the list of existing objects, the developer should write code to fetch that list and provide that as dynamic configuration. Furthermore, in the case of a dynamic list of business objects, since the list of objects is unknown, the structure of objects is also unknown. This means that an answer of yes to Question 1 implies an answer of yes to Question 2. Question 2: Is the structure of objects dynamic . As explained above, an answer of yes to Question 1 implies an answer of yes to Question 2. Some systems have a fixed list of fields on every object. Other systems allow users and admins to customize the structure of each object. In this case, it is common for the system to provide an API endpoint which can provide the structure of any object in the system. If the objects have a fixed structure, then the adapter developer can hard code the schema of the objects produced. Otherwise, the developer should write code to fetch the dynamic structure of that object. Question 3: Does the API support Webhooks . Some external systems support the concept of Webhooks. The idea of a hook is that when a change occurs to an object in that external system, that external system proactively informs other systems about this change. Webhooks are hooks where the information is transferred by having the system reporting the change make a REST API call to the system making the change. See here for more information about the webhook service. This can be more efficient (both in terms of speed and machine resources) than having a scheduled job periodically make calls for changes that may or may not have occurred. ",
    "url": "http://localhost:4000/docs/Connectors/ActionsAndTriggers.html#given-an-api-how-should-an-adapter-behave",
    "relUrl": "/docs/Connectors/ActionsAndTriggers.html#given-an-api-how-should-an-adapter-behave"
  },"5": {
    "doc": "Actions and Triggers",
    "title": "Desired Adapter Behavior",
    "content": "Case 1 . | The list of business objects is dynamic | The structure of the objects is dynamic (Implied by above statement) | The API supports webhooks | . Triggers . | getObjectsPolling including functionality to . | supply the list of readable objects | . | getObjectsWebhook including functionality to . | supply the list of readable objects | supply the structure of the incoming objects | . | getDeletedObjectsWebhook including functionality to . | supply the list of deletable objects | . | . Actions . | upsertObject including functionality to . | supply the list of writable objects | supply the structure of the incoming object | . | deleteObject including functionality to . | supply the list of deletable objects | . | lookupObjectByField including functionality to . | supply the list of readable objects | supply the list of fields that can be searched | . | . Case 2 . | The list of business objects is dynamic | The structure of the objects is dynamic (Implied by above statement) | The API does not support webhooks | . Triggers . | getObjectsPolling including functionality to . | supply the list of readable objects | . | getDeletedObjectsPolling (if possible) . | including functionality to supply the list of deletable objects | . | . Actions . | upsertObject including functionality to . | supply the list of writable objects | supply the structure of the incoming object | . | deleteObject including functionality to . | supply the list of deletable objects | . | lookupObjectByField including functionality to . | supply the list of readable objects | supply the list of fields that can be searched | . | . Case 3 . | The list of business objects is static | The structure of the objects is dynamic | The API supports webhooks | . Triggers . | getObjectsPolling including . | the static list of readable objects | . | getObjectsWebhook including . | the static list of readable objects | functionality to supply the structure of the incoming objects | . | getDeletedObjectsWebhook including . | the static list of deletable objects | . | . Actions . | upsertObject including . | the static list of writable objects | functionality to supply the structure of the incoming object | . | deleteObject including . | the static list of deletable objects | . | lookupObjectByField including functionality to . | the static list of readable objects | supply the list of fields that can be searched | . | . Case 4 . | The list of business objects is static | The structure of the objects is dynamic | The API does not support webhooks | . Triggers . | getObjectsPolling including . | the static list of readable objects | . | getDeletedObjectsPolling (if possible) . | including the static list of readable objects | . | . Actions . | upsertObject including . | the static list of writable objects | functionality to supply the structure of the incoming object | . | deleteObject including . | the static list of deletable objects | . | lookupObjectByField including functionality to . | the static list of readable objects | supply the list of fields that can be searched | . | . Case 5 . | The list of business objects is static | The structure of the objects is static | The API supports webhooks | . Triggers . | getObjectsPolling including . | the static list of readable objects | . | getObjectsWebhook including . | the static list of readable objects | the static structure of the incoming objects | . | getDeletedObjectsWebhook including . | the static list of deletable objects | . | . Actions . | upsertObject including . | the static list of writable objects | the static structure of the incoming object | . | deleteObject including . | the static list of deletable objects | . | lookupObjectByField including functionality to . | the static list of readable objects | the static list of fields that can be searched | . | . Case 6 . | The list of business objects is static | The structure of the objects is static | The API does not support webhooks | . Triggers . | getObjectsPolling including . | the static list of readable objects | . | getDeletedObjectsPolling (if possible) . | including the static list of readable objects | . | . Actions . | upsertObject including . | the static list of writable objects | the static structure of the incoming object | . | deleteObject including . | the static list of deletable objects | . | lookupObjectByField including functionality to . | the static list of readable objects | the static list of fields that can be searched | . | . ",
    "url": "http://localhost:4000/docs/Connectors/ActionsAndTriggers.html#desired-adapter-behavior",
    "relUrl": "/docs/Connectors/ActionsAndTriggers.html#desired-adapter-behavior"
  },"6": {
    "doc": "Addresses",
    "title": "Address Model",
    "content": " ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/Address%20Model.html#address-model",
    "relUrl": "/docs/Data%20Models/Addresses/Address%20Model.html#address-model"
  },"7": {
    "doc": "Addresses",
    "title": "Resources",
    "content": "UML Diagram . UML Diagram . JSON Schema . Person Organization Relation Shared Definitions . Description Table . Description Table . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/Address%20Model.html#resources",
    "relUrl": "/docs/Data%20Models/Addresses/Address%20Model.html#resources"
  },"8": {
    "doc": "Addresses",
    "title": "General Structure",
    "content": "The addresses data model consists of different types of objects: organizations and persons. A relations object is used to describes the connections between other objects. ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/Address%20Model.html#general-structure",
    "relUrl": "/docs/Data%20Models/Addresses/Address%20Model.html#general-structure"
  },"9": {
    "doc": "Addresses",
    "title": "Relations",
    "content": "Generally there are three types of relations: . | Persons to organizations | Persons to persons | Organizations to organizations | . Creating a specific relation object allows for a more individualized expression of connection between entities than linking them directly. You can name a relation and thus categorize or tag relations. This way you could use the relation to describe a persons function within a company, a companies relationship with other companies (like subsidiaries or suppliers) as well as connection between real people. You can solve for example the following user stories: . | User Stories | . | As a user I want to see the structure of a group of companies, to get a better overview of my business dealings. | . | As a user I want to assign one or more persons to an organization to communicate with all contacts of an organization | . | As a user I want to assign relations between contact persons of my customers to get an overview of their hierarchy. | . | As a user I want to see, if a specific person in an organization is the same person as in another organization | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/Address%20Model.html#relations",
    "relUrl": "/docs/Data%20Models/Addresses/Address%20Model.html#relations"
  },"10": {
    "doc": "Addresses",
    "title": "Duplicates",
    "content": "In order to reflect that some people in the real world have multiple roles in one or many different organizations, or that one organization might have several locations, we allow duplicates and link them by modeling the relation. Suppose you have a person who is the CEO of a company and he is also board member of an association. If you want to mail him in his role as a CEO you want to use the company mail address. If you want to mail him as a board member, you want to use the mail address from the association. For the example above, we will store the person with his contact details for his CEO role in the company and store another entity of this person for his role as a board member in the association. Then we link them via a relation and describe that relation as same person-relation. Some tools label the different kinds of mail accounts, but this does not scale. Here are some other example use cases you can solve the same way: . | User Stories | . | As a user I want to see the function of a person in his organization. | . | As a user I want to assign a person to different organizations with different contact data to see different roles of the same person. | . | As a user I want to store same persons in different organizations, to get in contact with them currently at the time with the particular contact data. | . | As a user I want to store an organization, with several offices in different locations. | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/Address%20Model.html#duplicates",
    "relUrl": "/docs/Data%20Models/Addresses/Address%20Model.html#duplicates"
  },"11": {
    "doc": "Addresses",
    "title": "Further information",
    "content": "As an open standard we are of course trying to be compatible with existing models out there. So for this model other industry standards were analyzed. If you are interested in learning more about the concepts behind this model, check out the workgroup on GitHub. GitHub . Do you have ideas, requirements or suggestions? Let us know and help to make this model more powerful. Contribution Guide . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/Address%20Model.html#further-information",
    "relUrl": "/docs/Data%20Models/Addresses/Address%20Model.html#further-information"
  },"12": {
    "doc": "Addresses",
    "title": "Addresses",
    "content": ". ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/Address%20Model.html",
    "relUrl": "/docs/Data%20Models/Addresses/Address%20Model.html"
  },"13": {
    "doc": "Description Table",
    "title": "Objects",
    "content": " ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/AddressDescriptionTable.html#objects",
    "relUrl": "/docs/Data%20Models/Addresses/AddressDescriptionTable.html#objects"
  },"14": {
    "doc": "Description Table",
    "title": "Person",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | title | String | - | Academic degree of the person | “Dr.” |   | . | salutation | String | - | Salutation of the person | “Mrs.” | - | . | firstname | String | - | First name of the person | “Silke | - | . | middleName | String | - | Middle name of the person | “Sandra” | - | . | lastName | String | - | Last name of the person | “Schaefer” | - | . | gender | String | enum | Gender of the person | “female” | “male”, “female”, “intersexual” | . | birthday | Date-time | - | Birthday of the person | “1982-06-24” | - | . | notes | String | - | Personal notes for the person | “Met at the winter summit” | - | . | displayName | String | - | Displayed name of the person within the application | “SiSchaefer” | - | . | language | String | - | First language of the person | “German” | - | . | nickname | String | - | Nickname of the person within the application | “Sisa” | - | . | jobTitle | String | - | Job title of the person | “Sales Manager” | - | . | photo | String | - | Photo related to the person | http://example.org/photo.jpg | - | . | anniversary | date-time | - | Anniversary of the person | 2018-07-01 | - | . | categories | Array (of category objects) | - | Category the person falls into | A category object array | - | . | addresses | Array (of address objects) | - | Address of the person | An address object array | - | . | contactData | Array (of contactData objects) | - | Contact data of a person | A contactData object array | - | . | PersonToOrganizationRelationship | Array (of personToOrganizationRelationships objects) | - | Relations of a person to organizations | A personToOrganizationRelationship object array | - | . | PersonToPersonRelationship | Array (of personToOrganizationRelationships objects) | - | Relations of two persons | An personToPersonRelationship object array | - | . | calendar | Array (of calendar objects) | - | Calendar information of the person | A calendar object array | - | . In order to secure proper data integration, please satisfy the prescribed standard for the title and language attributes. This ensures that every connected application (and its connector) knows the format of incoming values. In the following a list of common title is presented: . | dr. | dr.dr. | dr.mult. | dr.h.c. | prof. | prof.dr. | ph.d. | . The language attribute follows the IETF language tags format. In this format each language tag is composed of one or more \"subtags\" separated by hyphens (-). Each subtag is composed of basic Latin letters or digits only.(Source: Wikipedia. In the following a short list of the most used language tags is presented. | English name for language | Tag | . | English | en | . | English (United States) | en-US | . | English (Great Britain) | en-GB | . | French | fr | . | German | de | . | Polish | pl | . | Dutch | nl | . | Finnish | fi | . | Swedish | sv | . | Italian | it | . | Spanish (Spain) | es | . | Portuguese (Portugal) | pt | . | Russian | ru | . | Portuguese (Brazil) | pt-BR | . | Spanish (Mexico) | es-MX | . | Chinese (PRC) | zh-CN | . | Chinese (Taiwan) | zh-TW | . | Japanese | ja | . | Korean | ko | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/AddressDescriptionTable.html#person",
    "relUrl": "/docs/Data%20Models/Addresses/AddressDescriptionTable.html#person"
  },"15": {
    "doc": "Description Table",
    "title": "Organization",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | name | String | - | Name of the organization | “Cloud Ecosystem e.V.” | - | . | logo | String | - | Logo of the organization | “http://example.org/logo.png” | - | . | category | Array (of category objects) | - | Category the organization falls into | A category object array | - | . | address | Array (of address objects) | - | Address of the organization | An address object array | - | . | contactData | Array (of contactData objects) | - | Contact data of an organization | A contactData object array | - | . | OrganizationToPersonRelationship | Array (of personToOrganizationRelationships objects) | - | Relations of an organization to person | A personToOrganizationRelationship object array | - | . | OrganizationToOrganizationRelationship | Array (of personToOrganizationRelationships objects) | - | Relations of an organization to person | An OrganizationToOrganizationRelationship object array | - | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/AddressDescriptionTable.html#organization",
    "relUrl": "/docs/Data%20Models/Addresses/AddressDescriptionTable.html#organization"
  },"16": {
    "doc": "Description Table",
    "title": "Address",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | street | String | - | Street name of the address | “Hohestr.” | - | . | streetNumber | Number | - | Street number of the address | 3 | - | . | unit | String | - | Unit of the address | “a” | - | . | zipcode | String | - | Zipcode of the address | “50667” | - | . | city | String | - | City of the address | “Cologne” | - | . | district | String | - | District of the address | “Altstadt-Sued” | - | . | region | String | - | Region/County of the address | “NRW” | - | . | country | String | - | Country of the address | “Germany” | - | . | countryCode | String | - | Countrycode of the address | “DE” | - | . | primaryContact | String | - | Primary contact for the address | “Herbert Hermann” | - | . | label | String | - | Can be used for a textual description the address | primary | - | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/AddressDescriptionTable.html#address",
    "relUrl": "/docs/Data%20Models/Addresses/AddressDescriptionTable.html#address"
  },"17": {
    "doc": "Description Table",
    "title": "Category",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | name | String | - | Category the person / organization belongs to | “Customer” | - | . | label | String | - | Additional description of the category | “type a” | - | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/AddressDescriptionTable.html#category",
    "relUrl": "/docs/Data%20Models/Addresses/AddressDescriptionTable.html#category"
  },"18": {
    "doc": "Description Table",
    "title": "ContactData",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | type | String | - | Type of the contact data | email | - | . | label | - | Used to specify the contact data | E.g. “Mobile” (type: phone) or “facebook” (type: social media) |   |   | . | value | String | - | Value of the contact data | email@example.org | - | . | description | string | - | Can be used to further describe the contact data | “Primary” |   | . In order to secure proper data integration, please satisfy the prescribed standard for common contact data types. This ensures that every connected application (and its connector) knows the format of incoming values. In the following a list of common data types (and labels) is presented. If no further labelling is needed we suggest to leave the label field empty: . General contact types: . | email | phone | url | fax | social media | . | Type: | email | phone | url | fax | social media | . | Labels | assistant | privateMobile | homepage | assistant | facebook | . |   | privateEmail | privateLandline | subscription | privateFax | xing | . |   | businessEmail | assistant |   | businessFax | linkedin | . |   | support | businessMobile |   |   | skype | . |   |   | businessMobile |   |   | slack | . |   |   | support |   |   | google+ | . |   |   |   |   |   | twitter | . |   |   |   |   |   | youtube | . |   |   |   |   |   | reddit | . |   |   |   |   |   | flickr | . |   |   |   |   |   | stackoverflow | . |   |   |   |   |   | pinterest | . |   |   |   |   |   | wechat | . |   |   |   |   |   | qq | . |   |   |   |   |   | whatapp | . |   |   |   |   |   | viber | . |   |   |   |   |   | telegram | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/AddressDescriptionTable.html#contactdata",
    "relUrl": "/docs/Data%20Models/Addresses/AddressDescriptionTable.html#contactdata"
  },"19": {
    "doc": "Description Table",
    "title": "Calendar",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | calendar | String | - | URI to the persons’ calendar | http://cal.example.com/calA | - | . | requestCalendar | String | - | URL to request an appointment with the person | janedoe@example.com | - | . | busyCalendar | String | - | URL which described if the person is available or busy | http://www.example.com/busy/janedoe | - | . | label | String | - | Can be used for a textual description the calendar | primary | - | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/AddressDescriptionTable.html#calendar",
    "relUrl": "/docs/Data%20Models/Addresses/AddressDescriptionTable.html#calendar"
  },"20": {
    "doc": "Description Table",
    "title": "OrganizationToPersonRelationship",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | label | String | - | Describes the relationship between a person and an organization | “employee” | - | . | function | String | - | Function of the person within the organization | “project leader” | - | . | department | String | - | Name of the department of the person’s company | “Sales” | - | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/AddressDescriptionTable.html#organizationtopersonrelationship",
    "relUrl": "/docs/Data%20Models/Addresses/AddressDescriptionTable.html#organizationtopersonrelationship"
  },"21": {
    "doc": "Description Table",
    "title": "PersonToPersonRelationship",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | label | String | - | Describes the relationship between two persons | “employee” | - | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/AddressDescriptionTable.html#persontopersonrelationship",
    "relUrl": "/docs/Data%20Models/Addresses/AddressDescriptionTable.html#persontopersonrelationship"
  },"22": {
    "doc": "Description Table",
    "title": "PersonToOrganizationRelationship",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | label | String | - | Describes the relationship between two organizations | “employee” | - | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/AddressDescriptionTable.html#persontoorganizationrelationship",
    "relUrl": "/docs/Data%20Models/Addresses/AddressDescriptionTable.html#persontoorganizationrelationship"
  },"23": {
    "doc": "Description Table",
    "title": "Description Table",
    "content": ". | Objects . | Person | Organization | Address | Category | ContactData | Calendar | OrganizationToPersonRelationship | PersonToPersonRelationship | PersonToOrganizationRelationship | . | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Addresses/AddressDescriptionTable.html",
    "relUrl": "/docs/Data%20Models/Addresses/AddressDescriptionTable.html"
  },"24": {
    "doc": "Attachment Storage Service",
    "title": "Attachment Storage Service",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/AttachmentStorageService.html#attachment-storage-service",
    "relUrl": "/docs/Services/AttachmentStorageService.html#attachment-storage-service"
  },"25": {
    "doc": "Attachment Storage Service",
    "title": "Introduction",
    "content": "Stores files involved in an integration. API Reference Implementation Service File . ",
    "url": "http://localhost:4000/docs/Services/AttachmentStorageService.html#introduction",
    "relUrl": "/docs/Services/AttachmentStorageService.html#introduction"
  },"26": {
    "doc": "Attachment Storage Service",
    "title": "Technologies used",
    "content": ". | Node.js | Typescript | Redis | . ",
    "url": "http://localhost:4000/docs/Services/AttachmentStorageService.html#technologies-used",
    "relUrl": "/docs/Services/AttachmentStorageService.html#technologies-used"
  },"27": {
    "doc": "Attachment Storage Service",
    "title": "How it works",
    "content": "Provides the REST API for storing and getting files. As a backend uses Redis. Supported content types . | application/octet-stream | application/json | application/xml | text/xml | text/plain | text/csv | text/tsv | . Interaction with other Services . | Interacts with IAM to introspect provided IAM token. | Flow components are able to store and get their attachments into the Attachment Storage Service via the REST API. | . ",
    "url": "http://localhost:4000/docs/Services/AttachmentStorageService.html#how-it-works",
    "relUrl": "/docs/Services/AttachmentStorageService.html#how-it-works"
  },"28": {
    "doc": "Attachment Storage Service",
    "title": "Attachment Storage Service",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/AttachmentStorageService.html",
    "relUrl": "/docs/Services/AttachmentStorageService.html"
  },"29": {
    "doc": "Audit Log",
    "title": "Audit Log ",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/AuditLog.html#audit-log-",
    "relUrl": "/docs/Services/AuditLog.html#audit-log-"
  },"30": {
    "doc": "Audit Log",
    "title": "Introduction",
    "content": "The Audit Log receives, stores, and returns logging information about important user actions and system events. Messages spawned by other services are automatically stored, and can be retrieved via a REST API. API Reference Implementation . ",
    "url": "http://localhost:4000/docs/Services/AuditLog.html#introduction",
    "relUrl": "/docs/Services/AuditLog.html#introduction"
  },"31": {
    "doc": "Audit Log",
    "title": "Technologies used",
    "content": "MongoDB: MongoDB is used as the Audit Log’s storage solution. ",
    "url": "http://localhost:4000/docs/Services/AuditLog.html#technologies-used",
    "relUrl": "/docs/Services/AuditLog.html#technologies-used"
  },"32": {
    "doc": "Audit Log",
    "title": "How it works",
    "content": "The Audit Log serves as a persistent, immutable repository for pieces of information that are important to the operation of the Open Integration Hub. It receives this information through the Message Oriented Middleware, sent by the other services. These messages are called events, and are saved as logs. Each event has a name, e.g. user.created, and a payload containing the relevant information. As each Service knows best what information would be relevant, the Audit Log makes no requirements as to the format of this payload, and instead simply accepts arbitrary JSON in its place. The Audit Log maintains a list of event names it listens for. Each time an event with a name from the list is spawned somewhere in the Open Integration Hub, the Audit Log automatically saves it as a log. Events with names that are not on this list are ignored by the Audit Log. List of listened-for events . The names of events generally follow the schema of [context].[object].[action]. This is the list of all event names that the Audit Log currently listens for: . 'iam.user.created', 'iam.user.deleted', 'iam.user.modified', 'iam.user.removedFromTenant', 'iam.user.assignedToTenant', 'iam.user.loginFailed', 'iam.tenant.created', 'iam.tenant.deleted', 'iam.tenant.modified', 'iam.role.created', 'iam.role.deleted', 'iam.role.modified', 'iam.token.created', 'iam.token.deleted', 'iam.token.modified', 'iam.permission.created', 'iam.permission.deleted', 'iam.permission.modified', 'secret-service.secret.created', 'secret-service.secret.deleted', 'secret-service.token.get', 'metadata.domain.created', 'metadata.domain.deleted', 'metadata.domain.modified', 'metadata.schema.created', 'metadata.schema.deleted', 'metadata.schema.modified', 'flowrepo.flow.created', 'flowrepo.flow.modified', 'flowrepo.flow.deleted', . Log Schema . All logs are stored in this schema: . { \"headers\": { \"serviceName\": \"string\", \"createdAt\": \"string\", \"name\": \"string\" }, \"payload\": { \"user\": \"string\", \"tenant\": \"string\" } } . The headers object is automatically generated by the Message Oriented Middleware. As mentioned before, the payload object can contain arbitrary JSON. However, two keys are reserved: user, containing the id of the user that caused this event, and tenant, containing the id of the tenant in which this event has taken place. Both of these keys are optional, as events are not necessarily associated with a user or a tenant. Logs are immutable, and cannot be changed nor deleted by user input once they have been stored. The only time they are altered after the fact is when a user is deleted. In this case, the user’s id and username are anonymized in all logs that feature them. Audit Log API . The Audit Log offers a REST API for viewing and manually storing logs. For retrieving and viewing stored logs, the API offers a single endpoint GET /logs, which allows for detailed filtering of the results through query arguments. Regular users are limited to viewing logs that are directly associated with their own tenant, or with the user themselves. See the section “Ownership and Permissions” for further details. Additionally, there is another endpoint POST /logs that allows a user to manually save a log in the Audit Log. Please note that this end point exists primarily for testing and debugging purposes. During standard operation in a production environment, all logs should be automatically created by listening to events on the Message Oriented Middleware. For further information and examples about the API, please refer to the API Reference. Ownership and Permissions . As the Audit Log stores information about system-wide events, regular users are heavily restricted in which logs they can view. They can only see logs whose user or tenant ids match their own. Furthermore, regular users are required to have the relevant permission to be able to view logs. For further information about permissions, please refer to the documentation of the Identity Management. Open Integration Hub system administrators have neither of these restrictions, and can freely view all logs. Interaction with other Services . The Audit Log can receive events from any service, but only directly interacts with two of them: . | Message Oriented Middleware: The Audit Log receives all events it is supposed to store through the Message Oriented Middleware. | Identity Management: The Audit Log requires a bearer token created by the Identity Management to determine which logs the current user is allowed to view. | . ",
    "url": "http://localhost:4000/docs/Services/AuditLog.html#how-it-works",
    "relUrl": "/docs/Services/AuditLog.html#how-it-works"
  },"33": {
    "doc": "Audit Log",
    "title": "Audit Log",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/AuditLog.html",
    "relUrl": "/docs/Services/AuditLog.html"
  },"34": {
    "doc": "Collaboration",
    "title": "Collaboration Model",
    "content": " ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/Collaboration%20Model.html#collaboration-model",
    "relUrl": "/docs/Data%20Models/Collaboration/Collaboration%20Model.html#collaboration-model"
  },"35": {
    "doc": "Collaboration",
    "title": "Resources",
    "content": "UML Diagram . UML Diagram . JSON Schema . Collabrotation Calendar e-mail TaskToTaskRelation . Description Table . Description Table . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/Collaboration%20Model.html#resources",
    "relUrl": "/docs/Data%20Models/Collaboration/Collaboration%20Model.html#resources"
  },"36": {
    "doc": "Collaboration",
    "title": "General Structure",
    "content": "Collaboration is a broad term. This Data Model represents three key areas of collaboration: e-mail, calendar events and tasks. Through community feedback these were identified as the most important models in the collaboration domain. Due to the extensive professional use of Microsoft Outlook the chosen e-mail standard is derived mostly from Outlook An existing standard for calendar events is “iCalender” (RFC 5545). The properties of “iCalender” are incorporated and extended. In all three areas it is important to know which person created a certain element, when and with which properties. Therefore all three models have identical properties and are set in relation to a central element. The models inherit the most important properties from this main collaboration element. ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/Collaboration%20Model.html#general-structure",
    "relUrl": "/docs/Data%20Models/Collaboration/Collaboration%20Model.html#general-structure"
  },"37": {
    "doc": "Collaboration",
    "title": "Further information",
    "content": "As an open standard we are of course trying to be compatible with existing models out there. So for this model other industry standards were analyzed. If you are interested in learning more about the concepts behind this model, check out the workgroup on GitHub. GitHub . Do you have ideas, requirements or suggestions? Let us know and help to make this model more powerful. Contribution Guide . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/Collaboration%20Model.html#further-information",
    "relUrl": "/docs/Data%20Models/Collaboration/Collaboration%20Model.html#further-information"
  },"38": {
    "doc": "Collaboration",
    "title": "Collaboration",
    "content": ". ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/Collaboration%20Model.html",
    "relUrl": "/docs/Data%20Models/Collaboration/Collaboration%20Model.html"
  },"39": {
    "doc": "Description Table",
    "title": "Objects",
    "content": " ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#objects",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#objects"
  },"40": {
    "doc": "Description Table",
    "title": "CalendarEvent",
    "content": "|Attribute|Type|Properties|Description|Example|Possible Enumeration Options|—|—|—|—|—|—|collaborationElement|Array (of collaborationElement objects)|-|Collaboration properties values|A collaborationElement object array|-|contacts|Array (of contact objects)|-|Contact information of the person|A contact object array|-|calendars|Array (of calendar objects)|-|Calendar connected to contacts|A contact object array|-|location|String|-|Name of the location|”Room 123”|-|start|Date|-|startdate of the event|01.01.2018|-|end|Date|-|enddate of the event|31.12.2018|-| . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#calendarevent",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#calendarevent"
  },"41": {
    "doc": "Description Table",
    "title": "Contact",
    "content": "|Attribute|Type|Properties|Description|Example|Possible Enumeration Options|—|—|—|—|—|—|name|String|-|Name of the contact|”John Doe”|-|eMail|String|-|E-mail of the contact|”john.doe@email.com”|-|calendars|Array (of calendar objects)|-|Calendar information of the person|A calendar object array|-| . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#contact",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#contact"
  },"42": {
    "doc": "Description Table",
    "title": "Calendar",
    "content": "|Attribute|Type|Properties|Description|Example|Possible Enumeration Options|—|—|—|—|—|—|calendar|String|-|URI to the person’s calendar|http://cal.example.com/calA|-|requestCalendar|String|-|URL to request an appointment with the person|janedoe@example.com|-|status|String|-|URL which describes if the person is available or busy|http://www.example.com/busy/janedoe|-| . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#calendar",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#calendar"
  },"43": {
    "doc": "Description Table",
    "title": "Email",
    "content": "|Attribute|Type|Properties|Description|Example|Possible Enumeration Options|—|—|—|—|—|—|collaborationElements|Array (of collaboration objects)|-|Collaboration properties values|A collaborationElement object array|-|threads|Array (of thread objects)|-|thread information of the e-mail|A thread object array|-| . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#email",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#email"
  },"44": {
    "doc": "Description Table",
    "title": "Thread",
    "content": "|Attribute|Type|Properties|Description|Example|Possible Enumeration Options|—|—|—|—|—|—|threadName|String|-|Name of the Thread|”Subject of the E-Mail”|-|topic|String|-|Identification of the thread|randomized string|-| . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#thread",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#thread"
  },"45": {
    "doc": "Description Table",
    "title": "CollaborationElement",
    "content": "|Attribute|Type|Properties|Description|Example|Possible Enumeration Options|—|—|—|—|—|—|from|String|-|E-Mail of sender|”john.doe@email.com”|-|to|String|-|E-mail of receiver|”jane.doe@email.com”|-|cc|String|-|E-mail of the contact in copy|”janet.doe@email.com”|-|bcc|String|-|E-mail of the contact in blind copy|”jonathan.doe@email.com”|-|subject|String|-|Subject line of the e-mail|”RE: Your Question”|-|date|Date|-|Date|01.01.2018|-|day|String|Enum|Day of the week|”Monday”|”Monday”, “Tuesday”, “Wednesday”, “Thursday”, “Friday”, “Saturday”, “Sunday”|time|Time|-|Timestamp|10:10:10|-|messageID|String|-|Id of the message|some token|-|language|String|-|Language of the e-mail content|”en”|-|authentification|String|-|Authentification Result|”spf=pass smtp.mailfrom=email.com”|-|MIMEVersion|Decimal|-|Version of MIME|1.3|-|format|String|-|Format of the email content|”HTML”|-|content|String|-|Content of the e-mail|”Dear John, please find attached”|-|attachments|Array (of attachment objects)|-|attachment information of the element|An attachment object array|-| . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#collaborationelement",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#collaborationelement"
  },"46": {
    "doc": "Description Table",
    "title": "Attachment",
    "content": "|Attribute|Type|Properties|Description|Example|Possible Enumeration Options|—|—|—|—|—|—|type|String|-|Datatype of the attachment|”JPG”|-|size|String|-|Datasize of the e-mail attachment|”54 KB”|-| . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#attachment",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#attachment"
  },"47": {
    "doc": "Description Table",
    "title": "Task",
    "content": "|Attribute|Type|Properties|Description|Example|Possible Enumeration Options|—|—|—|—|—|—|collaborationElement|Array (of collaborationElement objects)|-|Collaboration properties values|A collaborationElement object array|-|category|Array (of category objects)|-|category information of the task|A category object array|-|taskToTaskRelation|String|-|Relation between two tasks|”is subtask to”|-|subject|String|-|Subject of the Task|”Create a datamodel”|-|startDate|Datetime|-|Date when the task starts|01.01.2018 00:00|-|endDate|Datetime|-|Date when the task is closed|31.01.2018 00:00|-|reminderDate|Datetime|-|Date when the task completion should be reminded|20.01.2018 00:00|-|content|String|-|description of the Task|”To create a datamodel we have to analyze different systems…”|-|status|String|Enum|status of the Task|”completed”|”started”, “in progress”, “completed”|urgency|String|-|urgency of the Task|”low”|”low”, “normal”, “high”| . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#task",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#task"
  },"48": {
    "doc": "Description Table",
    "title": "Category",
    "content": "|Attribute|Type|Properties|Description|Example|Possible Enumeration Options|—|—|—|—|—|—|category|String|-|category of the task or subtask|”Implementation”|-| . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#category",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#category"
  },"49": {
    "doc": "Description Table",
    "title": "TaskToTaskRelation",
    "content": "|Attribute|Type|Properties|Description|Example|Possible Enumeration Options|—|—|—|—|—|—|label|String|-|relation between two tasks|”is blocked by”|”subtask”, “blocks”, “is blocked by”|tasks|Array of task objects|-|Array (of task objects)|-|-|-|-|targetUiid|String|-|ID from the targeted task|-|-|sourceUiid|String|-|ID from the source task|-|-| . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#tasktotaskrelation",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html#tasktotaskrelation"
  },"50": {
    "doc": "Description Table",
    "title": "Description Table",
    "content": ". | CalendarEvent | Contacts | Calendars | Threads | CollaborationElement | Attachments | Task | Category | TaskToTaskRelation | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html",
    "relUrl": "/docs/Data%20Models/Collaboration/CollaborationDescriptionTable.html"
  },"51": {
    "doc": "Component Orchestrator",
    "title": "Component Orchestrator",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/ComponentOrchestrator.html#component-orchestrator",
    "relUrl": "/docs/Services/ComponentOrchestrator.html#component-orchestrator"
  },"52": {
    "doc": "Component Orchestrator",
    "title": "Introduction",
    "content": "The Component Orchestrator is responsible for a fair resource distribution. Implementation Service File . ",
    "url": "http://localhost:4000/docs/Services/ComponentOrchestrator.html#introduction",
    "relUrl": "/docs/Services/ComponentOrchestrator.html#introduction"
  },"53": {
    "doc": "Component Orchestrator",
    "title": "Technologies used",
    "content": ". | Node.js | Kubernetes | Docker | . ",
    "url": "http://localhost:4000/docs/Services/ComponentOrchestrator.html#technologies-used",
    "relUrl": "/docs/Services/ComponentOrchestrator.html#technologies-used"
  },"54": {
    "doc": "Component Orchestrator",
    "title": "How it works",
    "content": "In a multi-tenant environment it must be guaranteed that a user or tenant (intentionally or unintentionally) may not get an unfair usage of shared resources such as CPU, Memory, Network, etc. It must be guaranteed that every integration flow gets a chance to be executed, close to the intervals defined by its Cron expression. The Component Orchestrator is a micro-service defining the fairness policy and controlling that each user/flow/tenant is complying with that policy. The Component Orchestrator is responsible for: . | Protection from over-scheduling: if an execution of a flow takes longer than its scheduling interval, the following executions must be skipped. | Making sure that a flow or its steps are not deployed multiple times | If scaling is configured for a flow, the specified number of instances must be deployed | Detection of policy violations and punishment of “bad citizens” | . Interaction with other Services . | Interacts with IAM to introspect provided IAM token. | Gets events for starting a flow form the Flow Repository. | Deploys containers to the Kubernetes cluster. | Sends events into the event bus when the deployment is complete. | Creates IAM tokens in IAM service. | Gets Component information from the Component Repository. | Gets Snapshots from the Snapshots Service. | . ",
    "url": "http://localhost:4000/docs/Services/ComponentOrchestrator.html#how-it-works",
    "relUrl": "/docs/Services/ComponentOrchestrator.html#how-it-works"
  },"55": {
    "doc": "Component Orchestrator",
    "title": "Component Orchestrator",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/ComponentOrchestrator.html",
    "relUrl": "/docs/Services/ComponentOrchestrator.html"
  },"56": {
    "doc": "Component Repository",
    "title": "Component Repository",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/ComponentRepository.html#component-repository",
    "relUrl": "/docs/Services/ComponentRepository.html#component-repository"
  },"57": {
    "doc": "Component Repository",
    "title": "Introduction",
    "content": "The Component Repository stores information about integration components such as adapters &amp; transformer. API Reference Implementation Service File . ",
    "url": "http://localhost:4000/docs/Services/ComponentRepository.html#introduction",
    "relUrl": "/docs/Services/ComponentRepository.html#introduction"
  },"58": {
    "doc": "Component Repository",
    "title": "Technologies used",
    "content": ". | Node.js | MongoDB | . ",
    "url": "http://localhost:4000/docs/Services/ComponentRepository.html#technologies-used",
    "relUrl": "/docs/Services/ComponentRepository.html#technologies-used"
  },"59": {
    "doc": "Component Repository",
    "title": "How it works",
    "content": "Integration components are lightweight and stand-alone Docker images that include everything needed to run the component, including the component’s code, a runtime, libraries and dependencies. Each component is based on an Open Integration Hub parent image which provides the component runtime. For example, for Java component the parent images provides the JDK and for Node.js component the parent image provides NPM and Node.js. The component images are stored in a Docker Registry. A Docker Registry is a stateless, highly scalable storage for Docker images. Any open source integration component can be store and distributed in/from Docker Cloud so that they would be available to any OIH installations (cloud or on-prem) out of the box. For private components private Docker Registry can be maintained locally so that no components are exposed to the cloud. Each on prem installation could decide whether to use private repos on Docker Cloud or installing a private Docker Registry on prem for their private components. Interaction with other Services . | Interacts with IAM to introspect provided IAM token. | Component Orchestrator get the information about components from this service. | . ",
    "url": "http://localhost:4000/docs/Services/ComponentRepository.html#how-it-works",
    "relUrl": "/docs/Services/ComponentRepository.html#how-it-works"
  },"60": {
    "doc": "Component Repository",
    "title": "Component Repository",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/ComponentRepository.html",
    "relUrl": "/docs/Services/ComponentRepository.html"
  },"61": {
    "doc": "Conflict Management",
    "title": "Conflict Management ",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/ConflictManagement.html#conflict-management-",
    "relUrl": "/docs/Services/ConflictManagement.html#conflict-management-"
  },"62": {
    "doc": "Conflict Management",
    "title": "Introduction",
    "content": "The Open Integration Hub Conflict Management (CFM) is an optional module that can be included in an adapter. It automatically resolves potential conflicts according to a set of adapter-developer-defined rules. The CFM’s implementation is generic and class-based, allowing an adapter developer to create and add new rules specific to an application’s requirements. npm package Service File . ",
    "url": "http://localhost:4000/docs/Services/ConflictManagement.html#introduction",
    "relUrl": "/docs/Services/ConflictManagement.html#introduction"
  },"63": {
    "doc": "Conflict Management",
    "title": "How it works",
    "content": "Within a connector flow, the CFM can be included in an adapter that connects to a target application it pushes data to. To do this, an adapter developer can include the CFM moduel into their adapter, set it up as desired, and then use it to process potential conflicts. To use the CFM, the adapter needs to pass it two objects: First, the incoming object from the connecting flow, and secondly the corresponding object from the target application’s data store, if such an object exists. If the second object can not be uniquely identified, the adapter can also supply a best guess for a match, and configure the CFM to handle this case. These objects must both be converted to JSON before passing them on to the CFM. The CFM has no requirements to the object schema, only both objects use the same schema. After resolution is complete, the CFM will return a reconciled object in the same schema, which the adapter can then pass on to the target applications. The CFM operates on a set of Rules. Each rule is associated with a given number of object keys to which it applies. During conflict resolution, both the source and target values of that key are passed on to their associated Rule, which attempts to reconcile any differences according to a set of internal Resolvers, and returns the reconciled value. Both sets of Rules and Resolvers are expandable. Should the included default selection be unsuitable for a particular application, the adapter developer can add their own Rules and/or Resolvers in place and use them right away. ",
    "url": "http://localhost:4000/docs/Services/ConflictManagement.html#how-it-works",
    "relUrl": "/docs/Services/ConflictManagement.html#how-it-works"
  },"64": {
    "doc": "Conflict Management",
    "title": "Technical description",
    "content": "The CFM offers three classes, which each expose a number of functions for developer to customize its behaviour. CFM . The CFM itself is the top-level module. It is instantiated during the initialisation of an adapter, containing all configuration data. During actual data transfer, it can then be called to handle conflict management. It exposes these functions: . setRules({ruleName: [key, key2]}) . Accepts an object with the name of used rules as keys, and arrays of strings as values. The primary function of CFM configuration. This maps a particular rule to a list of key names. Each rule can be set only once, duplicate assignments within the same function call are rejected. Keys that have not been assigned to a rule automatically use a default rule. setDefaultRules(ruleName: somename}) . Accepts an object with the name of used rules as keys and an arbitrary name as value. Each key can only be set once, duplicate assignments within the same function call are rejected. This function adds a particular rule to a list of default rules. Default rules are only executed if there is key for which no other rule is specified. The default configuration is the rule onlyOverwriteEmpty which will only add values to the target if the target does not have the key or the value is empty. The default rules can be deactivated by calling with an empty object. Ie.: setDefaultRules({}); . setGlobalRules({ruleName: somename}) . Accepts an object with the name of used rules as keys and an arbitrary name as value. The primary function of global CFM configuration. This adds a particular rule to a list of global rules. Each key can only be set once, duplicate assignments within the same function call are rejected. addCustomRule(name, [resolver]) . Accepts a string as name for the rule, and an ordered array of resolvers to be used, referenced by name. Rule name cannot be a duplicate, all assigned resolvers must exist. This allows the developer to add a new rule, using a custom set of resolvers. Resolvers are applied in the order they are entered in the array. Check the example Rules for further information. addCustomResolver(name, function) . Accepts a string as a name for the resolver, and a javascript function as its function. Resolver name cannot be duplicate. The function must accept exactly two arguments, and return either a value of the same type, or false in case resolution is impossible. See the example Resolvers for further information. resolve(incomingObject, targetObject) . Accepts two JSON objects, which must use the same schema. Returns a JSON object of the same schema, or false. The core function of the CFM. This applies the rules set during configuration to both objects and creates a new one with all conflicts resolved. If reconciliation fails or the conflicts are irresolvable, it instead returns false. Rule . Rules are the core building block of the CFM. A Rule is essentially a sequence of predefined functions (called Resolvers) that receive two potentially conflicting data values (which can be in any Javascript format: String, Number, Object, Array, etc.) and attempt to resolve this conflict. If successful, the rule will return the new reconciled value. The primary consideration when creating a rule is the selection and order of Resolvers. Resolvers are applied in the specific order they were added to the Rule. The first Resolver that produces a resolution “wins”, and any further Resolvers are no longer used. Rules can be created and applied through the functions exposed by the CFM. The adapter should not manipulate rules directly. predefined rules . copyNew: Is copying data from incoming to target if incoming is not empty. rejectEmpty: Is copying data from incoming to target event if incoming is empty. onlyOverwriteEmpty: Only copies data from incoming to target if data in target is undefined or empty. ifUpdate: Is copying data from incoming to target if all keys exists in both objects and at least one value of incoming is not empty. uniqArray: Takes two array values as input and compares all elements to each other and returns an merged array without duplicates. predefined global rules . ResolverSkipDuplicateEntry: Compares incoming data to target data and makes CFM return {} if it is an exact duplicate. Resolver . Resolvers are the lowest-level component of the CFM. They contain a single arbitrary function that takes two data values and attempt to find a reconciliation for them. If successful, the reconciled value is returned as an object in the following form: {value:returnedData}. If no reconciliation can be found, they should instead return false. New Resolvers can be created and assigned through functions of the CFM, and are executed automatically by the Rules they are assigned to. The adapter should not manipulate Resolvers directly. Resolvers used in global rules should return an empty object “{}” if an entry is supposed to be skipped. ",
    "url": "http://localhost:4000/docs/Services/ConflictManagement.html#technical-description",
    "relUrl": "/docs/Services/ConflictManagement.html#technical-description"
  },"65": {
    "doc": "Conflict Management",
    "title": "Examples",
    "content": "Creating two custom resolvers, for handling a particular type of object . const CFM = require('@wice-devs/cfm') // This resolver checks whether a certain field in the incoming object matches the value of the target object // If it does not, the reference has been changed, any data of the target object is outdated, and only the incoming data is used // If it does match, this is not the case, the resolver returns false, and the issue is passed on to the next resolver in line. CFM.addCustomResolver('overwriteIfNew', (incoming, target) =&gt; { if (incoming.lastName !== target.lastName) return {value: incoming} else return false }) // This resolver performs a soft merge, where only empty fields of the target object // are filled with values from the incoming object CFM.addCustomResolver('fillOnlyEmptyFields', (incoming, target) =&gt; { const resolvedObject = target; for key in incoming { if (target[key] === undefined) { resolvedObject[key] = incoming[key]; } } return resolvedObject; }) . Creating a custom rule, using the resolvers created earlier: . // This rule applies first the \"overwriteIfNew\" resolver, then the \"fillOnlyEmptyFields\" one // If the incoming object lacks the required keys, it is discarded entirely by rejectIfIncomplete // If it does not, it is passed on and next handled by the fillOnlyEmptyFields resolver, which enriches the target object without overwriting existing data CFM.addCustomRule('enrichPersonData', ['overwriteIfNew', 'fillOnlyEmptyFields']) . Finally, assigning the rule to the appropriate object keys: . CFM.setRules({'enrichPersonData': ['recipient']}) . ",
    "url": "http://localhost:4000/docs/Services/ConflictManagement.html#examples",
    "relUrl": "/docs/Services/ConflictManagement.html#examples"
  },"66": {
    "doc": "Conflict Management",
    "title": "Local installation/development",
    "content": "Do npm install --save @wice-devs/cfm and implement the following code in your adapter: . const CFM = require('../index'); const cfm = new CFM(); // add your CFM Config here like explained above or see test folder for examples const result = cfm.resolve(incomingData, localData); . Where localData has to be provided from your system and incomingData is the data passed from the transformer. the result will be an newObject based on the applied resolvers. If the result object is an empty object then this means the incomingData should be discarded. For example if a global resolver identifies it as an exact copy of the localData. ",
    "url": "http://localhost:4000/docs/Services/ConflictManagement.html#local-installationdevelopment",
    "relUrl": "/docs/Services/ConflictManagement.html#local-installationdevelopment"
  },"67": {
    "doc": "Conflict Management",
    "title": "Conflict Management",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/ConflictManagement.html",
    "relUrl": "/docs/Services/ConflictManagement.html"
  },"68": {
    "doc": "Connectors",
    "title": "Connector Guidelines",
    "content": "The Open Integration Hub enables data synchronization across a variety of applications. To enable interaction with any software an technical component is needed to the Open Integration Hub - the connector. It consists of two distinct parts: the adapter and the transformer. An adapter is a module for the syntactic connection of an external application and its data to the Open Integration Hub. This includes protocol translation, data format transformation, etc. Furthermore it provides functionalities to perform e.g. CRUD operations within the source system. A transformer is responsible to semantically transform an incoming JSON object into another JSON object. Thus the mapping between two data models is done within the transformer. The following illustration provides a holistic overview of a connector: . ",
    "url": "http://localhost:4000/docs/Connectors/ConnectorBasics.html#connector-guidelines",
    "relUrl": "/docs/Connectors/ConnectorBasics.html#connector-guidelines"
  },"69": {
    "doc": "Connectors",
    "title": "Open Source Connectors",
    "content": "Like the Open Integration Hub services, connectors are also standardized components that can be reused in any implementation of the framework. There are several contributors that provide a wide range of open source connectors already. So before you start your own, check out what’s already there: . Open Integration Hub . For a quick start in developing your own connector, we offer a pair templates to start out with. These contain the basic necessary structure and functions for functionality. While these are specific to the contact data domain, they can easily be adjusted or expanded to suit any other type of data or application. Contacts Adapter Template Contacts Transformer Template . If you prefer a real world example, the Snazzy Contacts or Wice components are good place to get inspiration. Snazzy Contacts Adapter . Snazzy Contacts Transformer . Wice Adapter . Wice Transformer . ",
    "url": "http://localhost:4000/docs/Connectors/ConnectorBasics.html#open-source-connectors",
    "relUrl": "/docs/Connectors/ConnectorBasics.html#open-source-connectors"
  },"70": {
    "doc": "Connectors",
    "title": "Connectors",
    "content": ". ",
    "url": "http://localhost:4000/docs/Connectors/ConnectorBasics.html",
    "relUrl": "/docs/Connectors/ConnectorBasics.html"
  },"71": {
    "doc": "Contribution Guidelines",
    "title": "Contributor Guide",
    "content": "This document aggregates all information on how to contribute to the project. For technical specifications and guidelines like naming conventions, styleguides, definitions of done or operations, please refer to the corresponding workgroup and repository. ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#contributor-guide",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#contributor-guide"
  },"72": {
    "doc": "Contribution Guidelines",
    "title": "Table of Content",
    "content": "Welcome to the Open Integration Hub Community! . | Before you get started . | Sign the Contributor License Agreement | Code of Conduct | . | How to contribute . | GitHub Workflow | Pull Requests | Issues | . | Where to contribute . | Understanding the structure | Committees | Workgroups | . | . ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#table-of-content",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#table-of-content"
  },"73": {
    "doc": "Contribution Guidelines",
    "title": "Before you get started",
    "content": " ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#before-you-get-started",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#before-you-get-started"
  },"74": {
    "doc": "Contribution Guidelines",
    "title": "Sign the Contributor License Agreement",
    "content": "It is important for you as a Contributor as well as the project and all it’s users, that we are clear on who owns the code. All contributions have to be made according to the License Agreement. Before you can contribute code, you will need to sign the Contributor License Agreement. The Cloud Ecosystem defines the legal status of the contributed code in two different types of Contributor License Agreements (CLAs), individual contributors and companies. The Open Integration Hub Project can only accept original source code from CLA signatories. It is important to read and understand this legal agreement. ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#sign-the-contributor-license-agreement",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#sign-the-contributor-license-agreement"
  },"75": {
    "doc": "Contribution Guidelines",
    "title": "Code of Conduct",
    "content": "Please read and observe our Code of Conduct. ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#code-of-conduct",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#code-of-conduct"
  },"76": {
    "doc": "Contribution Guidelines",
    "title": "How to contribute",
    "content": " ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#how-to-contribute",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#how-to-contribute"
  },"77": {
    "doc": "Contribution Guidelines",
    "title": "GitHub Workflow",
    "content": "We follow the general GitHub workflow, if you are not familiar with working with GitHub, please refer to the GitHub Workflow Guide. ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#github-workflow",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#github-workflow"
  },"78": {
    "doc": "Contribution Guidelines",
    "title": "Pull Requests",
    "content": ". | Commit new file(s) e.g. data models | Select “Create a new branch for this commit and start a pull request.” | Name the branch with a descriptive name | Click on “propose new file” | Assign the workgroup manager to the pull request | Assign a descriptive label to the pull request | Click on “create pull request” | . If you don’t know whom to assign, choose: @philecs . ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#pull-requests",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#pull-requests"
  },"79": {
    "doc": "Contribution Guidelines",
    "title": "Issues",
    "content": "For any issue (suggestion, bug, fix, etc.) there are fundamentally two ways an individual can contribute: . | By helping to triage the issue: This can be done either by providing supporting details (a test case that demonstrates a bug), or providing suggestions on how to address the issue. | By helping to resolve the issue: Typically this is done either in the form of demonstrating that the issue reported is not a problem after all, or more often, by opening a Pull Request that changes some bit of something in Open Integration Hub in a concrete and reviewable manner. | . ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#issues",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#issues"
  },"80": {
    "doc": "Contribution Guidelines",
    "title": "Where to contribute",
    "content": " ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#where-to-contribute",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#where-to-contribute"
  },"81": {
    "doc": "Contribution Guidelines",
    "title": "Understanding the structure",
    "content": "The project is build by many enthusiasts from various backgrounds, countries and skillsets. To bring all of us together and work effectively, organization and structure is needed. To know where to get involved best, please take the time to understand how we work. ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#understanding-the-structure",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#understanding-the-structure"
  },"82": {
    "doc": "Contribution Guidelines",
    "title": "Committees",
    "content": "Committees are groups of elected Committers, responsible for the operative business and defining, prioritizing and executing all tasks necessary to achieve the project’s goals and to ensure a steady development. Committees set up workgroups for specific topics and function as escalation layer for problems within these workgroup. For more information, please see the Open Integration Hub Charter. We currently have two Committees, which are a good entry point for you, if are looking for something to work on. Technology Committee responsible for the development of the framework, it’s releases and management of the process via the product backlog. Business Committee responsible for all community-facing topics, as well as topics relevant for commercial use of the framework. ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#committees",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#committees"
  },"83": {
    "doc": "Contribution Guidelines",
    "title": "Workgroups",
    "content": ". | Each workgroup has at least one status call every two weeks | Every Committer of that workgroup must attend the status call | Decisions within a workgroup are made by all committers (unanimously) | If it is not possible to decide on a certain issue/topic it must be escalated to the corresponding committee | . ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#workgroups",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#workgroups"
  },"84": {
    "doc": "Contribution Guidelines",
    "title": "Community",
    "content": " ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#community",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#community"
  },"85": {
    "doc": "Contribution Guidelines",
    "title": "Get involved",
    "content": "Meetups . Contact: info@cloudecosystem.org . ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#get-involved",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#get-involved"
  },"86": {
    "doc": "Contribution Guidelines",
    "title": "Roles within the Community",
    "content": "There are several roles you can take on within the community, starting by being a Contributor. If you want to learn more, check out the Community Guide . ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html#roles-within-the-community",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html#roles-within-the-community"
  },"87": {
    "doc": "Contribution Guidelines",
    "title": "Contribution Guidelines",
    "content": ". ",
    "url": "http://localhost:4000/docs/Contributing/ContributionGuidelines.html",
    "relUrl": "/docs/Contributing/ContributionGuidelines.html"
  },"88": {
    "doc": "Data Hub",
    "title": "Data Hub",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/DataHub.html#data-hub",
    "relUrl": "/docs/Services/DataHub.html#data-hub"
  },"89": {
    "doc": "Data Hub",
    "title": "Introduction",
    "content": "The Data Hub is responsible for storing, retrieving and updating oihDataRecords. It functions as the central data storage (synchronized data) within the Open Integration Hub. API Reference Implementation . ",
    "url": "http://localhost:4000/docs/Services/DataHub.html#introduction",
    "relUrl": "/docs/Services/DataHub.html#introduction"
  },"90": {
    "doc": "Data Hub",
    "title": "Technologies used",
    "content": "MongoDB: MongoDB is used as the Data Hub’s storage solution. ",
    "url": "http://localhost:4000/docs/Services/DataHub.html#technologies-used",
    "relUrl": "/docs/Services/DataHub.html#technologies-used"
  },"91": {
    "doc": "Data Hub",
    "title": "How it works",
    "content": "ID Linking . As described in update propagation the Data Hub receives events by the SDF Adapter. These events contain meta information including the applicationUid of the source application and the recordUid of the incoming payload within the source application. The Data Hub stores information about each set of applicationUids and recordUids in an array called refs. In case of a create operation it creates an oihDataRecord and adds the refs array with one object in it i.e. applicationUid and recordUid of the source system. In case of an update operation it identifies the oihDataRecord by the incoming combination of applicationUid and recordUid and updates the content. In both cases, after the successful creation or update, it emits an event to dispatcher service containing either only the object that was received by sdf adapter (create) or the original object and in addition all existing combinations of applicationUid and recordUid (update). In case of a create operation the target applications adapter sends the recordUid of newly created data record to the SDF Adapter. The SDF Adapter then passes it to the DataHub, so that Data Hub can create a new reference in the refs array. The following code represents the structure of an oihDataRecord: . { \"data\": [ { \"domainId\": \"string\", \"schemaUri\": \"string\", \"content\": {}, \"refs\": [ { \"applicationUid\": \"string\", \"recordUid\": \"string\", \"modificationHistory\": [ { \"user\": \"string\", \"operation\": \"string\", \"timestamp\": \"2019-10-09T11:43:25.270Z\" } ] } ], \"owners\": [ { \"id\": \"123\", \"type\": \"user\" } ], \"id\": \"string\", \"createdAt\": \"2019-10-09T11:43:25.270Z\", \"updatedAt\": \"2019-10-09T11:43:25.270Z\" } ], \"meta\": { \"page\": 0, \"perPage\": 0, \"total\": 0, \"totalPages\": 0 } } . Interaction with other Services . | Dispatcher Service: Emits events for dispatcher service in order to enable the hub and spoke archticture (See: update propagation). | SDF Adapter: Receives events from SDF Adapter in order to create or update oihDataRecords (See: update propagation). | . ",
    "url": "http://localhost:4000/docs/Services/DataHub.html#how-it-works",
    "relUrl": "/docs/Services/DataHub.html#how-it-works"
  },"92": {
    "doc": "Data Hub",
    "title": "Data Hub",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/DataHub.html",
    "relUrl": "/docs/Services/DataHub.html"
  },"93": {
    "doc": "Master Data Models",
    "title": "Master Data Models",
    "content": "A fundamental challenge of data integration is mapping data, because applications usually use different properties and structures to represent their data. Traditionally the mapping is manually, where the data is copied between the objects. This is not very efficient, time consuming and does not scale. The Open Integration Hub’s goal is to make integration easier, by providing reusable standard components. In this spirit, also mapping of data should be easier and scalable. In order to map data only once per application and not for every single integration flow, we make use of standardized Transformers. Those contain mappings of an applications data schema to a Master Data Model. Using the (optional) Data Hub enables Master Data Management for the respective domain by centrally storing the data of the integrated applications. ",
    "url": "http://localhost:4000/docs/Data%20Models/DataModelBasics.html#master-data-models",
    "relUrl": "/docs/Data%20Models/DataModelBasics.html#master-data-models"
  },"94": {
    "doc": "Master Data Models",
    "title": "General Approach",
    "content": "There is not the one canonical data model for Open Integration Hub. Instead a model for each context (domain) is defined: . | A Master Data Model describes the data of a certain domain in a depth which is sufficient enough to map and synchronize the specific data of multiple applications in that domain. | The model is generic and it can be extended on the application level. | . ",
    "url": "http://localhost:4000/docs/Data%20Models/DataModelBasics.html#general-approach",
    "relUrl": "/docs/Data%20Models/DataModelBasics.html#general-approach"
  },"95": {
    "doc": "Master Data Models",
    "title": "Structure",
    "content": "Data models consists of numerous entities. Hence, it is necessary to split a data model into smaller parts to enable the transfer of optimized amounts of data: . | Every Master Data Model consists of one or multiple loosely coupled sub-models. | . Although it is necessary to split (especially big) data models into smaller sub-models, it often does not make sense to send each entity on its own, when there is a high cohesion between certain entities. Hence, . | each sub-model can consist of one or more entities. | In case a sub-model consists of multiple entities, it must be modeled as an aggregate: . | . Aggregate is a pattern in Domain-Driven Design. A DDD aggregate is a cluster of domain objects that can be treated as a single unit. An example may be an order and its line-items, these will be separate objects, but it’s useful to treat the order (together with its line items) as a single aggregate. An aggregate will have one of its component objects be the aggregate root. Any references from outside the aggregate should only go to the aggregate root. The root can thus ensure the integrity of the aggregate as a whole. [s. Martin Fowler: DDD_Aggregate] . JSON-Schema . All Master Data Models in Open Integration Hub use JSON Schema as the format that data is processed with. Open Integration Hub is able to validate data at runtime. As Master Data Models are split into sub-models, those sub-models must be processable independently. | for every sub-model there must be a seperate JSON schema describing the entity or aggregate. | . As there are situations where entities are reused in (i.e. are part of) two or more aggregates, it is of course adequate to encapsulate those entities in an additional schema file and reference them from the several sub-models to avoid redundancy. ",
    "url": "http://localhost:4000/docs/Data%20Models/DataModelBasics.html#structure",
    "relUrl": "/docs/Data%20Models/DataModelBasics.html#structure"
  },"96": {
    "doc": "Master Data Models",
    "title": "Master Data Models",
    "content": ". ",
    "url": "http://localhost:4000/docs/Data%20Models/DataModelBasics.html",
    "relUrl": "/docs/Data%20Models/DataModelBasics.html"
  },"97": {
    "doc": "Dispatcher Service",
    "title": "Dispatcher Service ",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/DispatcherService.html#dispatcher-service-",
    "relUrl": "/docs/Services/DispatcherService.html#dispatcher-service-"
  },"98": {
    "doc": "Dispatcher Service",
    "title": "Introduction",
    "content": "The Dispatcher Service is responsible for routing messages between individual connector flows. This allows for the exchange of data in a centralized fashion via the Hub &amp; Spoke model. API Reference Implementation . ",
    "url": "http://localhost:4000/docs/Services/DispatcherService.html#introduction",
    "relUrl": "/docs/Services/DispatcherService.html#introduction"
  },"99": {
    "doc": "Dispatcher Service",
    "title": "Technologies used",
    "content": "MongoDB: MongoDB is used as the Dispatcher Service’s storage solution. ",
    "url": "http://localhost:4000/docs/Services/DispatcherService.html#technologies-used",
    "relUrl": "/docs/Services/DispatcherService.html#technologies-used"
  },"100": {
    "doc": "Dispatcher Service",
    "title": "How it works",
    "content": "Dispatch Configurations. The Dispatcher Service works on the basis of Configurations. Each configuration contains a list of connected applications, each of which may have one or several outbound (application-to-OIH) and inbound (OIH-to-application) connections. These connections are distinguished based on the data schema and operation (i.e. create/update/delete) they use. Once such a configuration has been saved, the Dispatcher Service independently monitors incoming data from the listed outbound connections. Whenever a piece of data from one of these connections is successfully processed by the Data Hub, the Dispatcher Service then checks the configuration for any apps with matching inbound connections. To qualify, both the operation and data schema of the inbound connection must match that of the new data. For example, a piece of data in the “Organization” schema will not be routed to a connection configured to use the “Person” schema. Technological Background . The Dispatcher Service functions on the basis of flows. However, instead of direct app-to-app connector flows, it uses app-to-OIH and OIH-to-app integration flows. Whenever a dispatch configuration is created or updated, the Dispatcher Service automatically creates a number of integration flows, one each for connection of each connected app. These flows connect to and from the Smart Data Framework using the SDF-Adapter. The Dispatcher Service automatically keeps stock and maintains these flows. If an inbound flow is inactive when data for it comes in, it will automatically be started. If a configuration is deleted or an app removed, all associated flows are automatically deleted. Data handled in this manner is routed through the Message Oriented Middleware (MOM). Any time a new piece of data coming in from an outbound flow was successfully processed by the Data Hub, a message is published on the MOM containing both the actual data that was fetched from the app (the payload) as well as certain metadata denoting its source, schema, and so on. These messages are then received by the Dispatcher Service. Based on their metadata, the Dispatcher Service is able to identify the source of the data and find all appropriate consumers based on saved dispatch configurations. Each consumer has an associated integration flow, and the Dispatcher Service then sends a message with custom-tailored metadata (such as the recordUid that the target app can use to identify the data) to each of these flows using the MOM. Finally, these messages are received by instances of the SDF Adapter, which is the first node in each consuming integration flow. The SDF Adapter then passes on the payload of the message to the next node in line, at which point the data is treated just as in a regular connector flow, passing through a transformer and adapter and finally being passed on to the target application. Dispatcher Service API . The Dispatcher Service offers a REST API through which dispatch configurations can be created, modified, and deleted. To interact with this API, the user must supply a valid bearer token generated by the Identity Management. List of supported Methods and Routes . | endpoint | method | description | comments | . | /dispatches | GET | Returns a list of all dispatch configurations available to the user | Currently, all configurations are shared by tenant | . | /dispatches | POST | Stores a new dispatch configuration | Automatically creates all integration flows for this configuration | . | /dispatches/{id} | GET | Returns one particular dispatch configuration by its ID |   | . | /dispatches/{id} | DELETE | Deletes a dispatch configuration | Automatically deletes all associated flows | . | /dispatches/{id}/app | PUT | Adds a new application to a dispatch configuration | Automatically creates all integration flows for the app | . | /dispatches/{id}/app/{appId} | DELETE | Deletes one application from the dispatch configuration | Automatically deletes the app’s associated integration flows | . For further details and examples, please refer to the API Reference . Interaction with other Services and Components . The Dispatcher Service interacts with these Services: . | Identity Management: The Dispatcher Service relies on a bearer token supplied by the Identity Management to determine which dispatch configurations the current user may see. | Message Oriented Middleware: The Dispatcher Service receives and routes all incoming data through the Message Oriented Middleware. | Flow Repository: The Dispatcher Service communicates with the Flow Repository to create, delete, and start all integration flows necessary for a dispatch configuration. | . ",
    "url": "http://localhost:4000/docs/Services/DispatcherService.html#how-it-works",
    "relUrl": "/docs/Services/DispatcherService.html#how-it-works"
  },"101": {
    "doc": "Dispatcher Service",
    "title": "Dispatcher Service",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/DispatcherService.html",
    "relUrl": "/docs/Services/DispatcherService.html"
  },"102": {
    "doc": "Documents",
    "title": "Documents Model",
    "content": " ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/Document%20Model.html#documents-model",
    "relUrl": "/docs/Data%20Models/Documents/Document%20Model.html#documents-model"
  },"103": {
    "doc": "Documents",
    "title": "Resources",
    "content": "UML Diagram . Basic Version Metadata Extended Version . JSON Schema . Document Folder Object Relation Shared Definitions . Description Table . Description Table . ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/Document%20Model.html#resources",
    "relUrl": "/docs/Data%20Models/Documents/Document%20Model.html#resources"
  },"104": {
    "doc": "Documents",
    "title": "General Structure",
    "content": "One challenge of this model is to fit a wide array of services dealing with documents. The features provided by heterogeneous services like Dropbox and other more advanced systems like SharePoint or ELO are far apart. In order to be suitable for all scenarios, this model is split into several implementations. | Basic Sharing documents between services | Comfort Sharing documents and metadata between services | Extended Sharing documents, metadata, policies and sub-resources between services | . The basic model describes a basic implementation for sharing documents and files without the need of handling metadata or additional information. For the comfort model, Metadata definitions can be additionally queried. The extended model specification does contain all properties that are required in order to handle additional functionalities of DMS/ECM/EIM systems. The following may give you a better understanding of the variety of services and possible use cases. Managing documents by specialized systems . The following table classifies the complexity of document management or sharing services currently available. | Type | Description | Example | . | File storage | A service that can store documents (folders/ documents) in a hierarchically organized structure including limited metadata capabilities | FTP, S3, Network fileshare | . | Online file share | A service that can store information (folders/ documents) in a hierarchically organized structure. Allows sharing content easily. Some services do provide metadata capabilities. | DropBox, OneDrive, Box | . | ECM/EIM/Content services | A service that captures, stores, delivers, manages and organizes information based on additional metadata or hierarchically organized structures. In most cases DMS functionalities can be seen as a fundamental part of these services. | Alfresco, ELO, M-Files, OpenText, SharePoint | . Business services creating documents . In addition to systems that have been specifically designed to capture, store, deliver and manage documents and information, there are additional services that have been designed in order to produce content. | Service/ System | Sample document types | . | Inbox services | Incoming Invoices, Notifications, … | . | ERP | Outgoing Invoices, Purchase Orders, … | . | User Stories | . | As a user I want to automatically send an email with the invoice created by the ERP system. | . | As a user I want to automatically store generated outgoing documents in the document management system. | . | As a user I want to use inbox services for digitalizing invoices for further processing. | . Business services consuming documents . From a companywide perspective, there are a variety of systems that generate or receive documents that are related to business transactions of other systems. If a document management system is used, these documents are centralized in a repository that contains information that refers to the business transaction. Therefore ERP or CRM systems can view a list of related documents from document management systems or third party sources. The following table lists systems that create business transactions. | Service/ System | Sample document types | . | CRM | Invoices, Purchase Orders, Billing documents, E-Mails, Communication, etc. | . | ERP | Invoices, Purchase Orders, Billing documents, etc. | . | User Stories | . | As a user I want to view stored documents of the current invoice transaction in the ERP system. This gives me additional information like attached terms and conditions or the related delivery docket. | . | As a user I want to add additional documents while displaying the customer record in the CRM system. | . | As a user I want to pass all documents uploaded to a specific DropBox folder to be passed to the ERP system. | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/Document%20Model.html#general-structure",
    "relUrl": "/docs/Data%20Models/Documents/Document%20Model.html#general-structure"
  },"105": {
    "doc": "Documents",
    "title": "Further information",
    "content": "As an open standard we are of course trying to be compatible with existing models out there. So for this model other industry standards were analyzed. If you are interested in learning more about the concepts behind this model, check out the workgroup on GitHub. GitHub . Do you have ideas, requirements or suggestions? Let us know and help to make this model more powerful. Contribution Guide . ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/Document%20Model.html#further-information",
    "relUrl": "/docs/Data%20Models/Documents/Document%20Model.html#further-information"
  },"106": {
    "doc": "Documents",
    "title": "Documents",
    "content": ". ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/Document%20Model.html",
    "relUrl": "/docs/Data%20Models/Documents/Document%20Model.html"
  },"107": {
    "doc": "Description Table",
    "title": "Objects",
    "content": " ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/DocumentDescriptionTable.html#objects",
    "relUrl": "/docs/Data%20Models/Documents/DocumentDescriptionTable.html#objects"
  },"108": {
    "doc": "Description Table",
    "title": "Object",
    "content": "Objects describe base properties that are used by documents, folders or additional implementations. | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | label | String |   | Name of the object | “Invoice #2018061023” |   | . | description | String |   | Additional information describing the object | “Invoice was sent on March 28th” |   | . | baseType | String | enum | Base type of the object | “document” | “document”,”folder” | . | parentUid | String |   | Id of the parent element if hierarchically organized | “291ecb5e-cd8f-46fd-ae0d-40b5e280f23a” |   | . | path | String |   | Path from root to the objects parent | “/Invoices/Company Corp./2018/” |   | . | metadata | Map&lt;String, Object&gt; |   | Map that contains all metadata as specified by: specification of generic metadata |   |   | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/DocumentDescriptionTable.html#object",
    "relUrl": "/docs/Data%20Models/Documents/DocumentDescriptionTable.html#object"
  },"109": {
    "doc": "Description Table",
    "title": "Document",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options |   | . | currentVersion | Version |   | Current version of this document |   |   |   | . | versions | Array (of version objects) | - | Current version of this document |   |   |   | . | subRessources | Array (of subResource objects) | - | Describes sub resources |   |   |   | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/DocumentDescriptionTable.html#document",
    "relUrl": "/docs/Data%20Models/Documents/DocumentDescriptionTable.html#document"
  },"110": {
    "doc": "Description Table",
    "title": "Item",
    "content": "No additional properties available. ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/DocumentDescriptionTable.html#item",
    "relUrl": "/docs/Data%20Models/Documents/DocumentDescriptionTable.html#item"
  },"111": {
    "doc": "Description Table",
    "title": "Folder",
    "content": "No additional properties available. ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/DocumentDescriptionTable.html#folder",
    "relUrl": "/docs/Data%20Models/Documents/DocumentDescriptionTable.html#folder"
  },"112": {
    "doc": "Description Table",
    "title": "Version",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | label | String |   | Label of the document version. Most likely used for version numbering. | “1.0.1” |   | . | comment | String |   | Version comment by the author | “Rescanned document due to bad quality” |   | . | creation | Modification |   | Timestamp and user that created the document version |   |   | . | isLatestVersion | Boolean |   | Flag that indicates if this is the latest version of the document | true |   | . | isMajorVersion | Boolean |   | Flag that indicates if this is a major version of the document | true |   | . | size | Number |   | Filesize of the document version in bytes | 2097152 |   | . | mimeType | String |   | Mimetype of the file content | “image/png” | refer to http://www.iana.org/assignments/media-types/media-types.xhtml | . | url | String |   | Download url for the document version. | “http://myservice.com/api/document/get/9bd1f8dd-5040-4b19-bbc9-c5cbb9c8d4b8” |   | . | uid | String |   | Id of the document version | “9bd1f8dd-5040-4b19-bbc9-c5cbb9c8d4b8” |   | . | type | String | enum | Type of the document version. | “image” | “image”, “mail”, “word”, “audio”, “video” | . | extension | String |   | Type of the documents extension. | “pdf” | “jpg”, “png” | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/DocumentDescriptionTable.html#version",
    "relUrl": "/docs/Data%20Models/Documents/DocumentDescriptionTable.html#version"
  },"113": {
    "doc": "Description Table",
    "title": "Document Property",
    "content": "Document properties can describe additional information depending on the documents type, e.g. systems that allow processing images can add the image size (width/height) in pixels. | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | type | String | enum | Type or identifier of the property | “width” |   | . | value | Object |   | value of the property | 1920 |   | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/DocumentDescriptionTable.html#document-property",
    "relUrl": "/docs/Data%20Models/Documents/DocumentDescriptionTable.html#document-property"
  },"114": {
    "doc": "Description Table",
    "title": "Policy",
    "content": "Policies can describe how objects are handled by the system. Invoices, for example, must be stored in a tamper-proof environment in order to comply with local law. Therefore policies can be used to define retention or deletion policies. | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | type | String | enum | Type of the policy | “retention” | “retention”, “deletion”, “backup”, “encryption”, “storage” | . | value | String |   | Description of the policy | ”” |   | . | uid | String |   | Id of the policy | “9bd1f8dd-5040-4b19-bbc9-c5cbb9c8d4b8” |   | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/DocumentDescriptionTable.html#policy",
    "relUrl": "/docs/Data%20Models/Documents/DocumentDescriptionTable.html#policy"
  },"115": {
    "doc": "Description Table",
    "title": "Sub Resource",
    "content": "Sub resources can be used in order to provide additional information. DMS/ECM/EIM systems usually provide additional functionality like extracting fulltext content or generating preview images of the document. | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | type | String | enum | Type of the sub resource | “rendition” | “rendition”, “marker”, “fulltext”, “attachment” | . | info | String |   | Additional information or description of the subresource | “small” |   | . | size | Number |   | Filesize of the document version in bytes | 2097152 |   | . | mimeType | String |   | Mimetype of the sub resource content | “image/png” | refer to http://www.iana.org/assignments/media-types/media-types.xhtml | . | url | String |   | Download url for the sub resource | “http://myservice.com/api/document/subresource/get/9bd1f8dd-5040-4b19-bbc9-c5cbb9c8d4b8” |   | . | uid | String |   | Id of the sub resource | “9bd1f8dd-5040-4b19-bbc9-c5cbb9c8d4b8” |   | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/DocumentDescriptionTable.html#sub-resource",
    "relUrl": "/docs/Data%20Models/Documents/DocumentDescriptionTable.html#sub-resource"
  },"116": {
    "doc": "Description Table",
    "title": "Relation",
    "content": "| Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | label | String |   | Name of the relation | “Relates to” |   | . | type | String | enum | Type of the relation | “link” | “link”, “reference” | . | targetUid | String |   | target object | “9bd1f8dd-5040-4b19-bbc9-c5cbb9c8d4b8” |   | . | sourceUid | String |   | source object | “9bd1f8dd-5040-4b19-bbc9-c5cbb9c8d4b8” |   | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/DocumentDescriptionTable.html#relation",
    "relUrl": "/docs/Data%20Models/Documents/DocumentDescriptionTable.html#relation"
  },"117": {
    "doc": "Description Table",
    "title": "Description Table",
    "content": ". | Object | Document | Folder | Version | Document Property | Policy | Sub Resource | Relation | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Documents/DocumentDescriptionTable.html",
    "relUrl": "/docs/Data%20Models/Documents/DocumentDescriptionTable.html"
  },"118": {
    "doc": "Flow Repository",
    "title": "Flow Repository ",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/FlowRepository.html#flow-repository-",
    "relUrl": "/docs/Services/FlowRepository.html#flow-repository-"
  },"119": {
    "doc": "Flow Repository",
    "title": "Introduction",
    "content": "The Flow Repository is responsible for storing, retrieving and updating the integration flows. Additionally, it is used to start and stop existing integration flows. API Reference Implementation . ",
    "url": "http://localhost:4000/docs/Services/FlowRepository.html#introduction",
    "relUrl": "/docs/Services/FlowRepository.html#introduction"
  },"120": {
    "doc": "Flow Repository",
    "title": "Technologies used",
    "content": "MongoDB: MongoDB is used as the Flow Repository’s storage solution. ",
    "url": "http://localhost:4000/docs/Services/FlowRepository.html#technologies-used",
    "relUrl": "/docs/Services/FlowRepository.html#technologies-used"
  },"121": {
    "doc": "Flow Repository",
    "title": "How it works",
    "content": "Integration Flows . An integration flow is described by a JSON document, containing all information required to run the flow. The core of each integration flow is its graph, containing nodes and edges. Data is passed along between nodes in the direction indicated by the edges. For (a minimal) example: . { \"name\": \"Example Integration Flow\", \"description\": \"Transfers data from the example origin to the example destination\", \"graph\": { \"nodes\": [ { \"id\": \"step_1\", \"name\": \"An example origin\" }, { \"id\": \"step_2\", \"name\": \"An example destination\" } ], \"edges\": [ { \"source\": \"step_1\", \"target\": \"step_2\" } ] } } . Note that in practice, most nodes will have additional fields, such as what component they use or secrets/arguments to be used by that component. For a full overview of the integration flow schema and further examples, please refer to the Models section in the API Reference. Flow Repository API . The Flow Repository offers a REST API through which flows can be stored, retrieved, updated, started, and stopped. To interact with this API, the user must supply a valid bearer token generated by the Identity Management. List of supported Methods and Routes . | endpoint | method | description | comments | . | /flows | GET | Returns a list of all integration flows the user owns | See the section “Ownership and Permissions” for further details | . | /flows | POST | Stores a new integration flow | The supplied flow data is automatically validated for completeness and errors | . | /flows/{id} | GET | Returns one particular integration flow by ID |   | . | /flows/{id} | DELETE | Deletes an integration flow by ID | Flow may not be currently active | . | /flows/{id} | PATCH | Updates an integration flow with new data | Flow may not be currently active | . | /flows/{id}/start | POST | Starts an integration flow | See the section “Interaction with other Services” for further details | . | /flows/{id}/stop | POST | Stops an integration flow | See the section “Interaction with other Services” for further details | . For further details and examples, please refer to the API Reference . Ownership and Permissions . Each integration flow contains an arbitrarily long array of owners. An integration flow can only be accessed if the current user (as identified by their bearer token) is among those owners. Additionally, manipulation of flows is further limited by the permissions of the current user. There are separate permissions for viewing, posting/updating, and starting/stopping integration flows. For further information about permissions, please refer to the documentation of the Identity Management. Interaction with other Services . The Flow Repository interacts with these Services: . | Identity Management: The Flow Repository relies on a bearer token supplied by the Identity Management to determine which integration flows the current user may see, and which actions they may take. | Component Orchestrator: Whenever an integration flow is started or stopped, the Flow Repository notifies the Component Orchestrator to take the necessary actions. Conversely, when an integration flow successfully becomes active or inactive, the Component Orchestrator notifies the Flow Repository of this change. | Message Oriented Middleware: The Flow Repository uses the Message Oriented Middleware to send and receive updates about the current status (starting/stopping/active/inactive) of integration flows. | Component Repository: Components used in an integration flow are specified as references to documents in the Component Repository. | Secret Service: If an integration flow uses sensitive data (such as login credentials), this data can be securely and opaquely stored in the Secret Service and then referenced. | . ",
    "url": "http://localhost:4000/docs/Services/FlowRepository.html#how-it-works",
    "relUrl": "/docs/Services/FlowRepository.html#how-it-works"
  },"122": {
    "doc": "Flow Repository",
    "title": "Flow Repository",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/FlowRepository.html",
    "relUrl": "/docs/Services/FlowRepository.html"
  },"123": {
    "doc": "GCP Installation",
    "title": "Google Cloud Platform Deployment Guide",
    "content": "The following guide helps to deploy the Open Integration Hub on the Google Cloud Platform. | Installation . | Install Kubectl | Access to GCP K8s | Basic Open Integration Hub Infrastructure Setup | Setup Storage | Identity and Access Management Deployment | Service Account Creation . | Receive User Token | Login as Admin | Create a Service Account | Create persistent Service Token | Secret Creation | . | Service Availability | . | User Tutorial . | Creating Components | Creating Flows | Starting Flows | Lessons Learned | . | . ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#google-cloud-platform-deployment-guide",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#google-cloud-platform-deployment-guide"
  },"124": {
    "doc": "GCP Installation",
    "title": "Installation",
    "content": " ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#installation",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#installation"
  },"125": {
    "doc": "GCP Installation",
    "title": "Install Kubectl",
    "content": "Make sure you have kubectl installed locally. Official guide: Install Kubectl . ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#install-kubectl",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#install-kubectl"
  },"126": {
    "doc": "GCP Installation",
    "title": "Access to GCP K8s",
    "content": "Make sure you have created a project on gcp and receive the authentication information for your cluster: . | Create project | In case no cluster exists: Create one | Get authentication information for the cluster | . For detailed information see: Kubernetes Engine - Quickstart . ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#access-to-gcp-k8s",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#access-to-gcp-k8s"
  },"127": {
    "doc": "GCP Installation",
    "title": "Basic Open Integration Hub Infrastructure Setup",
    "content": "Please make sure to clone the monorepo before you start. You will need the files in the minikube folder. Set up the basic Open Integration Hub infrastructure. If you want to change you namespace in the namespace.yaml, you also need to adjust the namespace in each service.yaml and deployment.yaml. Before your apply ingress also make sure to replace host entries: . E.g for iam: . host: iam.openintegrationhub.com . After you changed the host entries execute the following commands: . | kubectl apply -f platform/namespace.yaml | kubectl apply -f platform/rabbitmq.yaml | kubectl apply -f platform/redis.yaml | kubectl apply -f platform/ingress.yaml | . ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#basic-open-integration-hub-infrastructure-setup",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#basic-open-integration-hub-infrastructure-setup"
  },"128": {
    "doc": "GCP Installation",
    "title": "Setup Storage",
    "content": "Next, you need to make sure to setup / connect to a storage solution such as MongoDB Atlas. Optionally, the mongodb.yaml provided under platform/mongodb.yaml could be used. To do so: . | apply minikube/1-Platform/volume.yaml | apply minikube/1-Platform/volumeClaim.yaml | apply minikube/1-Platform/platform/mongodb.yaml | . Note: We suggest to use the first variant i.e. an external storage solution . Irrespective of the solution you choose, ensure that each service uses its own database. This is necessary to ensure proper encapsulation and to avoid accidental data pollution. For more information about MongoDB databases, please refer to the official MongoDB Documentation. ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#setup-storage",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#setup-storage"
  },"129": {
    "doc": "GCP Installation",
    "title": "Identity and Access Management Deployment",
    "content": "Deploy the Open Integration Hub Identity and Access Management (IAM). To do so, simply create a secret for IAM: . | Create a temporary file (find an example secret below) . | Set mongourl to the connection string (base64 encoded) of your storage solution | Optionally set the other values of the other keys. Standard value: ‘somestring’ | . | Execute kubectl apply -f YOUR_TEMPORARY_FILE | Execute kubectl apply -f services/iam/k8s | . Wait until the service is fully deployed and ready. Afterwards, check if iam is existent on your cluster. Example Secret: . apiVersion: v1 kind: Secret metadata: name: iam namespace: oih-dev-ns type: Opaque data: mongourl: 'bW9uZ29kYjovL21vbmdvZGItc2VydmljZS5vaWgtZGV2LW5zLnN2Yy5jbHVzdGVyLmxvY2FsL2lhbQ==' jwtsecret: 'c29tZXN0cmluZw==' cookiesecret: 'c29tZXN0cmluZw==' admin_password: 'c29tZXN0cmluZw==' serviceacc_password: 'c29tZXN0cmluZw==' oidc_client_secret: 'c29tZXN0cmluZw==' . ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#identity-and-access-management-deployment",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#identity-and-access-management-deployment"
  },"130": {
    "doc": "GCP Installation",
    "title": "Service Account Creation",
    "content": "Create a service account and token for all of the services listed below. | Attachment-Storage-Service | Audit-Log | Component-Orchestrator | Component-Repository | DataHub | Dispatcher | Flow-Repository | MetaData-Repository | Secret-Service | . Use Postman (or another similar tool of your choice), to send these POST requests the IAM. Receive User Token . First, you have to request a user token (for admin account). This step only need to be perfomed once. Base URL: IAM_BASE_URL (received from ingress) . Header: Content-Type: application/json: . Login as Admin . Path: . /login . Request Body: . If you haven’t changed the admin_password in the secret you created a few steps ago, you can use the following json. Otherwise replace the password with the new value. { \"username\": \"admin@openintegrationhub.com\", \"password\": \"somestring\" } . Response Body Structure: . { \"token\": \"string\" } . Use the returned token as a Bearer token for the remaining requests. Create a Service Account . After you received the user token, a service account must be created for each service listed at the beginning of this chapter. Expect for component orchestrator, each service needs at least the following permission: . | iam.token.introspect | . Component Orchestrator needs the following permissions: . | Component Orchestrator: components.any.read, iam.token.create, iam.token.delete | . Replace the following values with a value of your choice: . | firstname | lastname | password | . Path: . /api/v1/users . Request Body: . { \"username\":\"servicename@serviceaccount.de\", \"firstname\":\"a\", \"lastname\":\"b\", \"role\":\"SERVICE_ACCOUNT\", \"status\":\"ACTIVE\", \"password\":\"asd\", \"permissions\":[ \"iam.token.introspect\", \"components.get\" ] } . Response Body Structure: . { \"id\": \"string\", \"username\": \"string\", \"firstname\": \"string\", \"lastname\": \"string\", \"status\": \"active\", \"tenant\": \"string\", \"roles\": [ { \"name\": \"string\", \"permissions\": [ \"all\" ], \"scope\": \"string\" } ], \"permissions\": [ \"all\" ], \"confirmed\": true, \"img\": \"string\" } . The returned id is later needed to create a service token. Create persistent Service Token . Path: . /api/v1/tokens . Request Body: . { \"accountId\": \"PASTE SERVICE ACCOUNT ID HERE\", \"expiresIn\": -1 } . The returned token is the service token that will be used by the other services to authenticate themselves to the IAM. Copy the value, encode it in base64 (for encoding you can use online tools such as: https://www.base64encode.org/), and then paste it into the secret files for each of the services listed at the beginning of this chapter. Secret Creation . For each services listed in ./services a secret file is needed. Thus, the following steps need to be performed for every service: . | Replace metadata.name with the current service name | Data must include all secretKeyRefs from the ./k8s/deployment.yaml of each service. E.g. flow repository: flow-repository deployment.yaml | For each service that was listed at the beginning of this chapter make sure to add the persistent token as the value for the iamtoken. | . Example secret file for flow-repository: . apiVersion: v1 kind: Secret metadata: name: flow-repository namespace: oih-dev-ns type: Opaque data: mongourl: CONNECTION_URL iamtoken: IAM TOKEN . Once you created the secret files, execute the following commands for each service: . | Execute kubectl apply -f YOUR_TEMPORARY_FILE | Execute kubectl apply -f services/CURRENT_SERVICE/k8s | . ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#service-account-creation",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#service-account-creation"
  },"131": {
    "doc": "GCP Installation",
    "title": "Service Availability",
    "content": "The Open Integration Hub is now running and ought to function just as it would in an online context. You can reach the various services via the following URLS: . | Identity and Access Management. Create and modify users, tenants, roles, and permissions. | YOUR IAM URL | . | Secret Service. Securely store authentication data for other applications. | YOUR Secret-Service URL | . | Flow Repository. Create, modify, and start/stop integration flows. | YOUR Flow-Repository URL | . | Audit Log. View event logs spawned by the other services. | YOUR Audit-Log URL | . | Metadata Repository. Create and modify master data models used by your connectors. | YOUR Meta-Data-Repository URL | . | Component Repository. Store and modify connector components. | YOUR Component-Repository URL | . | Attachment Storage Service. Temporarily store larger files for easier handling in flows. | YOUR Attachment-Storage-Service URL | . | Data Hub. Long-term storage for flow content. | YOUR Data-Hub URL | . | Integration Layer Service. Perform data operations such as merging or splitting objects. | YOUR Integration-Layer-Service URL | . | Web UI. A basic browser-based UI to control certain other services. | YOUR Web-UI URL | . Most of these services have an OpenAPI documentation of their API available through the path /api-docs. You can also check the API Reference Documentation. If you want to learn more about the services, check the Service Documentation or their readmes in the services folder of the GitHub Repository: Open Integration Hub Services . ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#service-availability",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#service-availability"
  },"132": {
    "doc": "GCP Installation",
    "title": "User Tutorial",
    "content": "The following step-by-step guide will show you how you can add your first components and create a flow with these components via the web ui which you deployed already. All actions are also performable via postman or similar tools. ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#user-tutorial",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#user-tutorial"
  },"133": {
    "doc": "GCP Installation",
    "title": "Creating Components",
    "content": "First, we have to create two components in order to have a source and target component. Below you will find code snippets for two exemplary components. For the beginning we recommend to use those but feel free to use your own. Example 1: . { \"data\":{ \"distribution\":{ \"type\":\"docker\", \"image\":\"elasticio/timer:ca9a6fea391ffa8f7c8593bd2a04143212ab63f6\" }, \"access\":\"public\", \"name\":\"Timer\", \"description\":\"Timer component that periodically triggers flows on a given interval\" } } . Example 2: . { \"data\": { \"distribution\": { \"type\": \"docker\", \"image\": \"elasticio/code-component:7bc2535df2f8a35c3653455e5becc701b010d681\" }, \"access\": \"public\", \"name\": \"Node.js code\", \"description\": \"Node.js code component that executes the provided code\" } } . The timer component is used to trigger flows on a provided interval, while the code component executes the code that was provided by the flow creator. In order to add those components, visit the web ui (web-ui.localoih.com) and navigate to the Components section. Now click on the ADD+ button. A popup window will appear where you can add the code provided above. GREAT! You created your first component. Repeat this step for the second component. !! In order to create the flow in the next step you have to copy the ids of the components you just created. !! . ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#creating-components",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#creating-components"
  },"134": {
    "doc": "GCP Installation",
    "title": "Creating Flows",
    "content": "Now that you successfully created two components it is time to create your first flow. Below you will find code snippets for an example flow. This excample flow periodically triggers the flow and sends request to webhook.site. For the beginning we recommend to use this flow but feel free to create your own. Please replace the ADD COMPONENT ID HERE with the ids you copied in the previous step. Furthermore please go to and copy the link to you clipboard. Afterwards please replace the ADD WEBHOOK URL HERE with the link in your clipboard. { \"name\":\"Timer To Code Component Example\", \"description:\": \"This flow periodically triggers the flow and sends request to webhook.site\", \"graph\":{ \"nodes\":[ { \"id\":\"step_1\", \"componentId\":\"ADD COMPONENT ID HERE\", \"function\":\"timer\" }, { \"id\":\"step_2\", \"componentId\":\"ADD COMPONENT ID HERE\", \"function\":\"execute\", \"fields\":{ \"code\":\"function* run() {console.log('Calling external URL');yield request.post({uri: 'ADD WEBHOOK URL HERE', body: msg, json: true});}\" } } ], \"edges\":[ { \"source\":\"step_1\", \"target\":\"step_2\" } ] }, \"cron\":\"*/2 * * * *\" } . In order to add the flow, navigate to the Flows section. Now click on the ADD+ button. A popup window will appear where you can add the code provided above. GREAT! You created your first flow. ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#creating-flows",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#creating-flows"
  },"135": {
    "doc": "GCP Installation",
    "title": "Starting Flows",
    "content": "Now that you have created two components and a flow, it is time to start this flow. Stay in the flows section and look for the flow you just created. On the right side you will the a “play” symbol. Click on it and the how the status changes from inactive to starting. After some time the status changes to active and the flow is running (you may have to refresh the site). ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#starting-flows",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#starting-flows"
  },"136": {
    "doc": "GCP Installation",
    "title": "Lessons Learned",
    "content": "In this tutorial you have learned… . | How to create components via the web ui | How to create a flow within the Open Integration Hub using existing components | How to start a flow and track its status | . | . ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html#lessons-learned",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html#lessons-learned"
  },"137": {
    "doc": "GCP Installation",
    "title": "GCP Installation",
    "content": ". ",
    "url": "http://localhost:4000/docs/Getting%20Started/GCPInstallationGuide.html",
    "relUrl": "/docs/Getting%20Started/GCPInstallationGuide.html"
  },"138": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "Open Integration Hub is a framework which you can run on your own infrastructure and in different compositions, depending on your use case. This flexibility can be challenging in the beginning. To help you getting started, we created two guides for running Open Integration Hub locally and on Google Cloud Platform. We also walk you trough creating your first flow. If you first want to understand more about how it actually works, we suggest you start with the Service Collaboration Overview. In case you need help or want to give us some feedback, head over to GitHub. Local Setup Google Cloud Platform Setup Service Collaboration ",
    "url": "http://localhost:4000/docs/Getting%20Started/GettingStarted.html#getting-started",
    "relUrl": "/docs/Getting%20Started/GettingStarted.html#getting-started"
  },"139": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": ". ",
    "url": "http://localhost:4000/docs/Getting%20Started/GettingStarted.html",
    "relUrl": "/docs/Getting%20Started/GettingStarted.html"
  },"140": {
    "doc": "Identity and Access Management",
    "title": "Identity and Access Management ",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#identity-and-access-management-",
    "relUrl": "/docs/Services/IdentityManagement.html#identity-and-access-management-"
  },"141": {
    "doc": "Identity and Access Management",
    "title": "Introduction",
    "content": "Identity and Access Management (short: IAM) provides secure authentication and authorization of users/clients. Further features of an IAM system are user and roles management as well as audit logging. API Reference Implementation Service File . ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#introduction",
    "relUrl": "/docs/Services/IdentityManagement.html#introduction"
  },"142": {
    "doc": "Identity and Access Management",
    "title": "Technologies used",
    "content": "JWT JSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. This information can be verified and trusted because it is digitally signed. JWTs can be signed using a secret (with the HMAC algorithm) or a public/private key pair using RSA. See JWT Introduction for more details. OAuth 2.0 OAuth 2.0 is the industry-standard protocol for authorization. OpenId Connect OpenID Connect is a simple identity layer on top of the OAuth 2.0 protocol, which allows computing clients to verify the identity of an end-user based on the authentication performed by an authorization server, as well as to obtain basic profile information about the end-user in an interoperable and REST-like manner. ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#technologies-used",
    "relUrl": "/docs/Services/IdentityManagement.html#technologies-used"
  },"143": {
    "doc": "Identity and Access Management",
    "title": "How it works",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#how-it-works",
    "relUrl": "/docs/Services/IdentityManagement.html#how-it-works"
  },"144": {
    "doc": "Identity and Access Management",
    "title": "Basic IAM setup",
    "content": "This approach aims at a very basic and simple operation of OIH using only a minimal set of microservices. We assume a scenario where an individual or a company uses OIH in the intranet without the need for external identity providers and using a simple authentication and authorization mechanisms. The basic parameters of this setup are: . | Authentication is a dedicated service (AuthService) | Authorization mechanisms are either part of the said AuthService or is a utility library integrated into a separate gateway or microservice | Use of HS256 or HS512 symmetric encryption with a strong secret key | . We also assume that in a basic use case there isn’t necessary a user interface for the OIH. And although the OIH does provide a REST-API, the client is most likely to be curl, Postman or some other (Micro)-Service interacting with the REST-API. The later could make use of an OAuth Library and thus have an easy way to use access and refresh tokens. But in the case of using curl or Postman, it would be more convenient and easy to have a long living JWT access token (e.g. with an expiration time of 1h) without the need to refresh the token every few minutes. This of course comes at the cost of risks when using long living tokens similar to those risks when using solely Basic Authentication. The AuthService is intended to support both cookie and Authorization Header based authentication. In case of a browser, the AuthService will set the JWT as a cookie and automatically prolong it, if the user makes a HTTP request to the AuthServer before the JWT expires. ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#basic-iam-setup",
    "relUrl": "/docs/Services/IdentityManagement.html#basic-iam-setup"
  },"145": {
    "doc": "Identity and Access Management",
    "title": "Identity Store",
    "content": "For a basic setup we will be using a MongoDB with Passport.js to store and authenticate users. Account management is done by the AuthService.  . In a more advanced approach, one could use LDAP or Active Directory as an identity store. If no other information except a unique identifier of the user (e.g. user_id) is needed, we can store all user data (e.g. Name, E-Mail) in the AuthService. Should this data also be required in other microservices as well, for example for data aggregation, performance, etc., then we will have to introduce an identity/user provisioning mechanism. This provisioning mechanism would allow synchronization and merging of user data in all affected services. A very simplified solution for user provisioning could be accomplished through the message queue – all subscribes are notified with new data set, when an account has been modified. The subscribers can then merge/import the data into their own system. Same goes for the de-provisioning. ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#identity-store",
    "relUrl": "/docs/Services/IdentityManagement.html#identity-store"
  },"146": {
    "doc": "Identity and Access Management",
    "title": "Authorization and Roles",
    "content": "The precise role definitions of OIH may change at a later stage, for the sake of simplicity, we will use the current state of roles and privileges of elastic.io platform, which are Admin, Organization-Guest and Organization-Admin. | A user can be part of more than one organization and can have different roles in each of her organizations | An Admin can manage organizations | Organization-User can create and start her own flows and organization flows (organization flows are accessible for all organization users, but each user may have their own private flows) | Organization-Admin can do everything what Organization-Users can do, plus manage her organization data and Organization-Users (invite, change role, remove from organization) | . ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#authorization-and-roles",
    "relUrl": "/docs/Services/IdentityManagement.html#authorization-and-roles"
  },"147": {
    "doc": "Identity and Access Management",
    "title": "User management",
    "content": "In contrast to a more tenant centric user management where a user can only be assigned to a single tenant, we propose a user centric IAM. This means that users can be created in OIH and can be assigned to more than one organization/tenant and have different roles in each organization. Every user can edit her personal data and password individually. The current elastic.io platform adopts a similar approach. The Admin can create organizations, create/invite users and assign them to an organization, whereas an Organization-Admin can only invite users to her organization or exclude them from it. ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#user-management",
    "relUrl": "/docs/Services/IdentityManagement.html#user-management"
  },"148": {
    "doc": "Identity and Access Management",
    "title": "Authentication and authorization flows",
    "content": "The following diagram visualizes a simple authentication flow from a client perspective. The client either has an access token (JWT) or will be redirected to the AuthService for authentication. The authentication can be a simple username + password process, returning the JWT token. The client has to save this token and use it in the Authorization Bearer header for all future requests.  . The access token is a JWT token which contains at least the following data: . | user_id | username | firstname | lastname | roles – an array of objects, containing organization_id and the role name within that organization | . As mentioned earlier, a user can be part of more than one organization, which is why we propose the use of context, similar to the context of kubernetes. Upon authentication, the client can fetch all contexts which it has, e.g.: . curl -i https://heimdal.example-oih.com/api/v1/context \\ -H \"Authorization: Bearer mytoken123\" . This will return the list of contexts the client/user is assigned to, e.g.: . [ { \"org\": \"some-org-id-1\", \"role\": \"ORG_ADMIN\" }, { \"org\": \"some-org-id-2\", \"role\": \"USER\" } ] . The cient an then choose the desired role and will receive a new JWT containing the claims for that specific context: . curl -X POST -i https://heimdal.example-oih.com/api/v1/context/some-org-id-1 \\ -H \"Authorization: Bearer mytoken123\" . Corresponsing JWT payload: . { \"sub\": \"1234567890\", \"name\": \"John Doe\", \"role\": \"ORG_ADMIN\", \"org_id\": \"some-org-id-1\" } . The advantage of setting a context is that, the underlying services can focus on the current user role and her current tenant/ogranization without knowledge of 1 to n relationsgip between a user and organizations. The Gateway in this diagram is representative for any other gatekeeper module or service. Due to the fact that OIH consists of many services, it would be an obvious choice to have the authentication and authorization validation in one module or service and limited to one distrinct location instead of distributing it accross many services and thus requiring the synchronization/updates at many points. We will view the alternatives in the advanced examples. The core idea of this basic approach is to validate the token at gateway level and pass the token as an HTTP-Header to the services behind the gateway. These service can react to the claims, assuming that these cannot have been tampered with if the request has passed the gateway. Different approaches are possible in order for the gateway to validate the token: . | Gateway and AuthService share a secret . | Gateway does not have to communicate with the AuthService | Changing the secret must be propagated to AuthService and Gateway | . | Gateway uses the public JWS key from AuthService to validate the tokens . | See the JWS documentation for details  | . | Gateway calls the AuthService to validate a token and caches that token (or the “jti”-Id of a JWT) for a short time | . For a very simple and basic scenario, we will be using the first approach with a shared secret. This basic scenario does not deal with token revocation, which is why the tokens might be long living (at least 1 hour). Upon expiry, the user needs to re-authenticate to receive a new access token. ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#authentication-and-authorization-flows",
    "relUrl": "/docs/Services/IdentityManagement.html#authentication-and-authorization-flows"
  },"149": {
    "doc": "Identity and Access Management",
    "title": "List of supported Methods and Routes",
    "content": "| endpoint | method | description | comments | . | /healthcheck | GET | Returns 200 OK when Service is up | Could be used for Kubernetes and liveliness checks | . | /login | POST | Returns 200 and a JWT. JWT ist returned and additionally set as a cookie. The JWT Token is used to authenticate to other Services | JWT can have role information in it | . | /logout | POST | Returns a 200 OK and will remove the corresponding cookie(s) |   | . ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#list-of-supported-methods-and-routes",
    "relUrl": "/docs/Services/IdentityManagement.html#list-of-supported-methods-and-routes"
  },"150": {
    "doc": "Identity and Access Management",
    "title": "Connectors and external services",
    "content": "Specific connectors like Adapters can have read/write access to the Smart Data Framework API (short: SDF). The OIH is a multi-tenant environment, which is why there should also be means to limit the data a connector can access. Given the fact that a separate Docker container is launched for each component of an integration flow, we could generate an access token for each connector on start-up. Such access tokens will contain the current tenant/organization id and allow the SDF to enforce it’s authorization mechanisms.  . Important notice We refrain from generating refresh tokens and create the access tokens for connectors without an expiration time. Typically, refresh tokens are stored by an AuthService permanently and do not change. This would mean, that we would need to create a separate refresh token for each connector on start-up and to delete that refresh token when the connector shuts down. Any given connector may crash, hang, be manually or atomatically restarted, the OIH plattform could have an unexpected outage, etc. If the AuthService should store refresh tokens, there there should be a mechanism or a dedicated daemon in the OIH, which instructs the AuthService to remove old refresh tokens. From a security perspective, the sole purpose of this connector access token is to access OIH internal services like SDF. In this environment, we do not see the risk of leaking a long-living access token greater than the risk of leaking a long-living refresh token. ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#connectors-and-external-services",
    "relUrl": "/docs/Services/IdentityManagement.html#connectors-and-external-services"
  },"151": {
    "doc": "Identity and Access Management",
    "title": "Advanced setup",
    "content": "In a more complex scenario we will make use of an external identity provider on OpenId Connect basis (e.g. Basaas Marketplace) to authenticate with the OIH. Other mentionable difference to the basic setup are: . | Use of RS265 or RS512 for assymetric encryption | JWT is signed using the private key from AuthService | Gateway can validate the authenticity of the JWT signature using the public from AuthService | . ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#advancedsetup",
    "relUrl": "/docs/Services/IdentityManagement.html#advancedsetup"
  },"152": {
    "doc": "Identity and Access Management",
    "title": "IAM in context of OIH and microservices",
    "content": "In a microservice architecture like the OIH a client (or a service on behalf of a user) can communicate to multiple other services. In terms of multi-tenancy, we require a sophisticated authorization, for example allowing access only to those integration flows and credentials the current user is actually eligibile for. As described in the basic scenario, our approach is to make use of signed JWT tokens containing the claims/grants of the user/client. The following example illustrates the access to credentials store. Flow: . | Client authenticates | Client receive the JWT containing the claims and current context | Client accesses the credentials for org1 in OIH. Gateway validates, that the JWT is valid. | Credentials Service can either use . | a middleware/library to verify that the client is authorized to access the ressource (business logic within credentials service) | or make a request to the authorization service, which maps all authorization policies | . | . TODO &gt; Our preferred version ist the use of middleware for authorization and a model which allows adding attriibute based access control (ABAC) to role based (RBAC). Our preferred version ist the use of middleware for authorization and a model which allows adding attriibute based access control (ABAC) to role based (RBAC). See the AccessControl documentation for more details. The validation of JWT can be done by the Gateway, but also by any service/module, which has access to the JWS public key of the AuthService.  . Authentication/Authorization Middleware . The authentication middleware is a depedency, e.g. node module, which can be used by any (micro)-service, in order to validate the JWT. For this, the middleware would only require the endpoint uri of the AuthService to fetch the public JWS key. This can be done on startup of the service and be automatically refreshed with a configurable TTL. TODO &gt; Define a scenario with rotating public keys and a fault tolerant approach, to refresh the public key on-demand. Authorization . For the OIH, we consider the authorization within each microservice as the best alternative: . | We have a very limited set of roles and applying rules and also a low number of APIs accessible from outside (currently only two: Integration Framework and Smart Data Framework) | A seperate authorization policy service would require much coordination within different working groups | Each microservice could have their own interpretation of what an specific combination of user role and attributes mean | . All these points could lead to inconsistent policies when managing them all at one point instead of doing it in the dedicated services. ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#iam-in-context-of-oih-and-microservices",
    "relUrl": "/docs/Services/IdentityManagement.html#iam-in-context-of-oih-and-microservices"
  },"153": {
    "doc": "Identity and Access Management",
    "title": "Single Sign-On",
    "content": "There are scenarios, for example in context of the Basaas marketplace, where we want the identity provider to be outside of OIH and allow the users to login into OIH using their external credentials. We will use OpenId Connect to achieve this. ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#single-sign-on",
    "relUrl": "/docs/Services/IdentityManagement.html#single-sign-on"
  },"154": {
    "doc": "Identity and Access Management",
    "title": "Authentication in the context of a generic ecosystem",
    "content": "In this section we will define a concept for user authentication to OIH inside a generic ecosystem. It utilizes OpenID Connect (http://openid.net/specs/openid-connect-core-1_0.html) which extends the OAuth2 protocol (https://tools.ietf.org/html/rfc6749). The core idea is to just authenticate the user and separate this process from authorization. Authentication Flow . This abstract flow illustrates the interaction between the three roles and includes the following steps: . (precondition) A registered account containing a valid OIH-uid is required. (A) A User requests access to OIH in order to interact via interface. Since OIH is Relying Party (http://openid.net/specs/openid-connect-core-1_0.html#Authentication) it is initializing OpenID Connect flow.  . (B) OIH sends an authentication request (AuthN) to specified OpenID Provider (OP). The HTTP request parameter “response_type” holds “id_token token”, so it corresponds to implicit flow according to the specifications (http://openid.net/specs/openid-connect-core-1_0.html#id_tokenExample). Example request . (values need to be url encoded) . https://127.0.0.1:3002/op/auth? client_id=react&amp; redirect_uri=https%3A%2F%2F127.0.0.1%3A3000%2Fcallback&amp; response_type=id_token%20token&amp; scope=openid%20email&amp; state=716195036c014abaace6ff4b7e3df427&amp; nonce=ea0da03dddf7453f9ad1e3c01c01df2e . (C) If this auth request is verified by OP, the User will be forwarded to login page. After he entered sufficient credentials, OP authenticates him. (D) Once he is logged in and has been authenticated locally, the OP creates id token (JWT) and request token. Thereafter OP signs the id token and redirects the User back to OIH with both tokens stored in redirect URIs fragment (http://openid.net/specs/openid-connect-implicit-1_0.html#ImplicitCallback). Example redirect URI . https://127.0.0.1:3000/callback # id_token=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InIxTGtiQm8zOTI1UmIyWkZGckt5VTNNVmV4OVQyODE3S3gwdmJpNmlfS2MifQ.eyJzdWIiOiI1YWI5MThmMjA0NDI1NGNjYjM2MjM0NDQiLCJlbWFpbCI6ImFkbWluQGJhc2Fhcy5jb20iLCJub25jZSI6ImVhMGRhMDNkZGRmNzQ1M2Y5YWQxZTNjMDFjMDFkZjJlIiwic2lkIjoiZTA5YjI2N2EtMDk2Yi00YWMxLTg2NjAtNTU0NzY3OWE2YmY0IiwiYXRfaGFzaCI6IjRCTi1YT1R2VzdtSGRDUndTOTNqSVEiLCJzX2hhc2giOiJMY3dYbXNEM1lvbTRvZnRmSzJsSXFRIiwiYXVkIjoicmVhY3QiLCJleHAiOjE1MjMyODU5OTIsImlhdCI6MTUyMzI4MjM5MiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDozMDAyIn0.BsYNuSFjIkg0SnNq6Euvsm6D-x5pQL29crHX0ol0_bHHi3Eh9ObCQ2Zd-LpJAGcw8vudXNBeCshihlSCAFuD94Zl7Vv9L5YYltL94-20knd8kXGq9-5vTn-LuJe8y64P558qHEFnlD4pCu9MOv6iwjCAxYoAZJhaq3Mr5ZT3dP6w49U0H9Va0fVNPDaqq-US1ILHI9uQCKrtWPZb_G-InTOZ8Gt7z7ib7Oq7tEgCShnwZ7XQdejuJmuiyK7be0gzqkb0I-0DOOKY2QWMgLB1araGPx9FIZCRuqJsdV5GJqxCNGGmEM_o8ys26UJFZgBZ6wa4LnV1CmUdoUNyJF6Y6A&amp; access_token=MzEyNWY0YjEtMGNmMy00MmNkLWE3MGYtOWRkMjZmNjg1ZWMwnMF_21ukS1eGBMvkOfmoYtLW1ZmnDnQ7WdyY9aOynLj897CINblj3gx8K4kB3seETd5Z5GiMzy6xbM25Ik_SUg&amp; expires_in=3600&amp;token_type=Bearer&amp;state=716195036c014abaace6ff4b7e3df427&amp; session_state=2710dbb0eb0e5c7cf7e28ef125e637ddb9124c9aa0e6d78047ac0fbc313da057.f9d7df278636a29a . Decoded id token . Header . { \"alg\": \"RS256\", \"typ\": \"JWT\", \"kid\": \"r1LkbBo3925Rb2ZFFrKyU3MVex9T2817Kx0vbi6i_Kc\" } . Payload . { \"sub\": \"5ab918f2044254ccb3623444\", \"email\": \"admin@basaas.com\", \"nonce\": \"ea0da03dddf7453f9ad1e3c01c01df2e\", \"sid\": \"e09b267a-096b-4ac1-8660-5547679a6bf4\", \"at_hash\": \"4BN-XOTvW7mHdCRwS93jIQ\", \"s_hash\": \"LcwXmsD3Yom4oftfK2lIqQ\", \"aud\": \"react\", \"exp\": 1523285992, \"iat\": 1523282392, \"iss\": \"http://localhost:3002\" } . (E*) After the signature of id token has been verified or userinfo has been obtained (http://openid.net/specs/openid-connect-core-1_0.html#UserInfo), the User authentication succeeded. This authentication flow brings several benefits. It relies on established standards, so an implementation will be covered by detailed documentation. In addition, Users get an interface that should already be known to them, since large providers (e. g. maintained by Google or Facebook) have been around for a couple of years. ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html#authentication-in-the-context-of-a-generic-ecosystem",
    "relUrl": "/docs/Services/IdentityManagement.html#authentication-in-the-context-of-a-generic-ecosystem"
  },"155": {
    "doc": "Identity and Access Management",
    "title": "Identity and Access Management",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/IdentityManagement.html",
    "relUrl": "/docs/Services/IdentityManagement.html"
  },"156": {
    "doc": "Integration Layer Service",
    "title": "Integration Layer Service (ILS) ",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/IntegrationLayerService.html#integration-layer-service-ils-",
    "relUrl": "/docs/Services/IntegrationLayerService.html#integration-layer-service-ils-"
  },"157": {
    "doc": "Integration Layer Service",
    "title": "Introduction",
    "content": "The Integration Layer Service receives data objects from one or several incoming flows, applies some business logic (such as a merge or split), validates them against a supplied schema, and provides the resulting valid objects as input to other flows. ILS can temporarily store these objects. Objects can be posted to the ILS through a REST-API, and be retrieved the same way. API Reference Implementation . ",
    "url": "http://localhost:4000/docs/Services/IntegrationLayerService.html#introduction",
    "relUrl": "/docs/Services/IntegrationLayerService.html#introduction"
  },"158": {
    "doc": "Integration Layer Service",
    "title": "Technologies used",
    "content": "MongoDB: MongoDB is used as the ILS’s storage solution. ",
    "url": "http://localhost:4000/docs/Services/IntegrationLayerService.html#technologies-used",
    "relUrl": "/docs/Services/IntegrationLayerService.html#technologies-used"
  },"159": {
    "doc": "Integration Layer Service",
    "title": "How it works",
    "content": "Use case: Merging objects . A core functionality is merging of incomplete objects from separate sources until all required data has been added. For this purpose, all incoming POST requests must supply a common identifier (cid) by which incoming objects can be matched against each other. Each time a new object comes in, the ILS first looks up its database to check whether another object with a matching cid is already stored. If one is found, the new data is merged into the existing object. If not, the incoming object is saved as-is. In either case, the resulting object is then validated against a supplied definition, which includes a list of required fields. A valid object will be marked as such, and can then be retrieved by another component. If an object is invalid, it cannot be retrieved, and will instead be stored until the necessary data has been merged into it and it passes validation. Use case: Splitting objects . ILS comes into play when the user would like to split an object into smaller or other objects containing properties from the main object. Let’s say that the user want to split the object containing employee’s firstName, lastName, salutation, organization, email, street and streetNumber into two separate objects. The first one should have the properties firstName, lastName and salutation ant the second one should contain the rest. In this case the user must provide splitSchema array containing two objects and each of them must have the properties meta and payload. Meta has the special property splitKey which servers as an identifier and payload consists of the fields which the new splitted object must contain. After successful splitting each new object should hold the splitKey and payload properties. On the other hand the user would like to GET a new/splitted object. In such a case an ilaId and a splitKey must be provided. Then ILS will return an array of all objects which have the same splitKey. Use case: Validating objects . ILS is able to validate an object against a certain schema. This means that the incoming object could be validated against a custom schema, which the user can provide in schema object or the object could be validated against a schema which is already stored in Meta Data Repository as well. Then the user should provide an IAM token, domainId and schemaUri. The last two properties could be requested from Meta Data Repository. Integration Layer Service API . The ILS offers a REST API through which chunks can be stored, splitted, validated and retrieved. To interact with this API, the user must supply a valid bearer token generated by the Identity Management. List of supported Methods and Routes . | endpoint | method | description | comments | . | /chunks | POST | Stores a new chunk | Depending on validation result, the chunk is saved with a boolean value of the property valid | . | /chunks/split | POST | Splits a chunk | An incoming object is splitted in other objects depending on given criteria | . | /chunks/validate | POST | Validates a chunk | The incoming object is validated against a schema from Meta Data Repository | . | /chunks/{ilaId}?key={splitKey} | GET | Returns chunks by ilaId and splitKey. | SplitKey is on optional parameter for fetching chunks which are splitted by the same splitSchema | . These operations ought to be conducted via the ILA, but can also be targeted manually for testing and development purposes. For POST /chunks, the body format is expected to match this format: . { \"ilaId\": \"string\", \"cid\": \"string\", \"def\": {}, \"payload\": {} } . | ilaId: Identifies which combination of flows this object belongs to. Must match among all connected flows. | cid: A common identifier designating which fields are used to match objects to one another. Must be a key within the supplied payload | def: The definition against which objects are validated. Currently expected to be a JSON schema. | payload: The actual data object, in JSON format. | . For POST /chunks/split, the body format should have the following format: . { \"ilaId\": \"string\", \"payload\": {}, \"splitSchema\": [ { \"meta\": { \"splitKey\": \"string\" }, \"payload\": {} } ] } . | ilaId: Identifies which combination of flows this object belongs to. Must match among all connected flows. | payload: The actual data object, in JSON format. | splitSchema: A schema object which could contain more then one schema. The main purpose is each schema has a meta object with an unique splitKey. This identifies the splitting model. In payload is the actual schema with all properties. | . For POST /chunks/validate, the body format should have the following format: . { \"ilaId\": \"string\", \"token\": \"string\", \"cid\": \"string\", \"def\": { \"domainId\": \"string\", \"schemaUri\": \"string\", }, \"payload\": {} } . | ilaId: Identifies which combination of flows this object belongs to. Must match among all connected flows. | token: An IAM Bearer token | cid: A common identifier designating which fields are used to match objects to one another. Must be a key within the supplied payload | def: The definition against which objects are validated. Currently expected to be a JSON schema. | domainId - A domainId from Meta Data Repository | schemaUri - A schemaUri for a certain schema from Meta Data Repository | payload: The actual data object, in JSON format. | . To GET /chunks/${ilaId}, an ilaId must be supplied. This will return all objects marked as valid that have been saved with this ilaId. To GET /chunks/${ilaId}?key=${splitKey} the user should provide the ilaId and key as a parameter for fetching objects which are splitted by a splitSchema with the certain splitKey. Stored objects are endowed with a Time To Live of one hour, which is refreshed every time new data is merged into them. Integration Layer Adapter (ILA) . The ILA is a generic component used to allow flows to communicate with the ILS. In posting mode, it automatically passes on any data objects received by other flow components, and endows them with the metadata listed above. In polling mode, it will automatically fetch all valid combined objects and pass them on to other components just like any other flow component. For further information about the ILA, see its GitHub Repository . Interaction with other Services . The Integration Layer Service interacts with this Service: . | Meta Data Repository: If the user wants to validate an object against a schema from Meta Data Repository, this schema must be fetched. For this purpose a bearertoken, domainId and schemaUri must be provided. | . ",
    "url": "http://localhost:4000/docs/Services/IntegrationLayerService.html#how-it-works",
    "relUrl": "/docs/Services/IntegrationLayerService.html#how-it-works"
  },"160": {
    "doc": "Integration Layer Service",
    "title": "Integration Layer Service",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/IntegrationLayerService.html",
    "relUrl": "/docs/Services/IntegrationLayerService.html"
  },"161": {
    "doc": "Local Installation",
    "title": "Local Installation Guide",
    "content": ". In addition to setting up the Open Integration Hub on a cloud infrastructure such as GCP it is also possible to setup a local version of the framework. Make sure to perform the following to set up a local version of the OIH within your own minikube: . | Requirements | Installation . | Install Minikube | Basic Open Integration Hub Infrastructure Setup | Host Rules Setup | Identity and Access Management Deployment | Service Account Creation . | Login as Admin | Create a Service Account | Create persistent Service Token | . | Shared Secret Application | Service Deployment | Service Availability | . | Usage . | Creating Components | Creating Flows | Starting Flows | Lessons Learned | . | . ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#local-installation-guide",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#local-installation-guide"
  },"162": {
    "doc": "Local Installation",
    "title": "Requirements",
    "content": "Please make sure to clone the monorepo before you start. Make sure that minikube is endowed with sufficient resources. We suggest at least: . | 8GB of memory | 4 CPUs | . If you're using Windows we suggest to use virtual box. In order to use it, Hyper-V must be disabled Enable/Disable Hyper-V on Windows 10. You may also have to enable virtualisation features in your BIOS. ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#requirements",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#requirements"
  },"163": {
    "doc": "Local Installation",
    "title": "Installation",
    "content": " ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#installation",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#installation"
  },"164": {
    "doc": "Local Installation",
    "title": "Install Minikube",
    "content": "Make sure minikube is installed, configured, and started. The command for allocating sufficient resources is . minikube start --memory 8192 --cpus 4 . If you already have an installed minikube instance that is using the virtualbox driver you can do . minikube stop . and then . VBoxManage modifyvm \"minikube\" --memory 8192 --cpus 4 . to adjust the resource limits before starting again. In particular, ensure that its ingress module is enabled (minikube addons enable ingress). Also make sure that kubectl is configured to use minikube. To see if its correctly configured use . `kubectl config current-context or cluster info` . For further information about how to set up minikube, see here: . | Install Minikube | Installing Kubernetes with Minikube | . If you're using Docker for Windows it overwrites the acutal kubectl version. In order to fix this download the `kubectl.exe` from Install kubectl on Windows. Navigate to the docker directory (e.g. Program Files\\Docker\\Docker\\resources\\bin) and replace the kubectl.exe in this folder with the one you just downloaded. ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#install-minikube",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#install-minikube"
  },"165": {
    "doc": "Local Installation",
    "title": "Basic Open Integration Hub Infrastructure Setup",
    "content": "Please make sure to clone the monorepo before you start. You will need the files in the minikube folder. Set up the basic Open Integration Hub infrastructure. To do this, simply execute . kubectl apply -f ./1-Platform . This may take a while to finish. You can use minikube dashboard to check the status of the various deployments. Once they are all ready, you can proceed. ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#basic-open-integration-hub-infrastructure-setup",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#basic-open-integration-hub-infrastructure-setup"
  },"166": {
    "doc": "Local Installation",
    "title": "Host Rules Setup",
    "content": "To actually reach the services, you need to add an entry in your hosts file for each service. You can retrieve the IP with minikube ip and need to create an entry for each host listed in the ingress.yaml file (e.g. iam.localoih.com). If you are using… . a Linux distribution, you can automate this by using this terminal command: . echo \"$(minikube ip) app-directory.localoih.com iam.localoih.com skm.localoih.com flow-repository.localoih.com auditlog.localoih.com metadata.localoih.com component-repository.localoih.com dispatcher-service.localoih.com webhooks.localoih.com attachment-storage-service.localoih.com data-hub.localoih.com ils.localoih.com web-ui.localoih.com\" | sudo tee -a /etc/hosts . a Windows distribution, you can find the host files under: . c:\\windows\\system32\\etc\\hosts or c:\\windows\\system32\\drivers\\etc\\hosts then add your_minikube_ip app-directory.localoih.com your_minikube_ip iam.localoih.com your_minikube_ip skm.localoih.com your_minikube_ip flow-repository.localoih.com your_minikube_ip auditlog.localoih.com your_minikube_ip dispatcher-service.localoih.com your_minikube_ip metadata.localoih.com your_minikube_ip component-repository.localoih.com your_minikube_ip webhooks.localoih.com your_minikube_ip attachment-storage-service.localoih.com your_minikube_ip data-hub.localoih.com your_minikube_ip ils.localoih.com your_minikube_ip web-ui.localoih.com . ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#host-rules-setup",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#host-rules-setup"
  },"167": {
    "doc": "Local Installation",
    "title": "Identity and Access Management Deployment",
    "content": "Deploy the OIH Identity and Access Management. To do so, simply execute kubectl apply -f ./2-IAM. Again, wait until the service is fully deployed and ready. ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#identity-and-access-management-deployment",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#identity-and-access-management-deployment"
  },"168": {
    "doc": "Local Installation",
    "title": "Service Account Creation",
    "content": "Create a service account and token for the other services in the IAM. Using Postman (or another similar tool of choice), send these POST requests to the IAM. Base URL: iam.localoih.com . Header: Content-Type: application/json: . Login as Admin . Path: . /login . Request Body: . { \"username\": \"admin@openintegrationhub.com\", \"password\": \"somestring\" } . Response Body Structure: . { \"token\": \"string\" } . Use the returned token as a Bearer token for the remaining requests. Create a Service Account . Path: . /api/v1/users . Request Body: . { \"username\":\"test@test.de\", \"firstname\":\"a\", \"lastname\":\"b\", \"role\":\"SERVICE_ACCOUNT\", \"status\":\"ACTIVE\", \"password\":\"asd\", \"permissions\":[ \"all\" ] } . Response Body Structure: . { \"id\": \"string\", \"username\": \"string\", \"firstname\": \"string\", \"lastname\": \"string\", \"status\": \"active\", \"tenant\": \"string\", \"roles\": [ { \"name\": \"string\", \"permissions\": [ \"all\" ], \"scope\": \"string\" } ], \"permissions\": [ \"all\" ], \"confirmed\": true, \"img\": \"string\" } . Use the returned id in the following request to create the token. Create persistent Service Token . Path: . /api/v1/tokens . Request Body: . { \"accountId\": \"PASTE SERVICE ACCOUNT ID HERE\", \"expiresIn\": -1, \"initiator\": \"PASTE SERVICE ACCOUNT ID HERE\", \"inquirer\": \"PASTE SERVICE ACCOUNT ID HERE\" } . The returned token is the service token that will be used by the other services to authenticate themselves to the IAM. Copy the value, encode it in base64 (for encoding you can use online tools such as: https://www.base64encode.org/), and then past it into the file found at ./3-Secret/SharedSecret.yaml at the indicated position (REPLACE ME). ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#service-account-creation",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#service-account-creation"
  },"169": {
    "doc": "Local Installation",
    "title": "Shared Secret Application",
    "content": "Apply the shared secret via . kubectl apply -f ./3-Secret. Ordinarily, each service would have its own secret for security reasons, but this is simplified for ease of use in a local context . ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#shared-secret-application",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#shared-secret-application"
  },"170": {
    "doc": "Local Installation",
    "title": "Service Deployment",
    "content": "Deploy the remaining services via the following command. This may take a while. kubectl apply -Rf ./4-Services . ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#service-deployment",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#service-deployment"
  },"171": {
    "doc": "Local Installation",
    "title": "Service Availability",
    "content": "The Open Integration Hub is now running and ought to function just as it would in an online context. You can reach the various services via the following URLS: . | Identity and Access Management. Create and modify users, tenants, roles, and permissions. | iam.localoih.com | . | Secret Service. Securely store authentication data for other applications. | skm.localoih.com | . | Flow Repository. Create, modify, and start/stop integration flows. | flow-repository.localoih.com | . | Audit Log. View event logs spawned by the other services. | auditlog.localoih.com | . | Metadata Repository. Create and modify master data models used by your connectors. | metadata.localoih.com | . | Component Repository. Store and modify connector components. | component-repository.localoih.com | . | Attachment Storage Service. Temporarily store larger files for easier handling in flows. | attachment-storage-service.localoih.com | . | Data Hub. Long-term storage for flow content. | data-hub.localoih.com | . | Integration Layer Service. Perform data operations such as merging or splitting objects. | ils.localoih.com | . | Web UI. A basic browser-based UI to control certain other services. | web-ui.localoih.com | . | . Most of these services have an OpenAPI documentation of their API available through the path /api-docs. You can also check the API Reference Documentation. If you want to learn more about the services, check the Service Documentation or their readmes in the services folder of the GitHub Repository: Open Integration Hub Services . ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#service-availability",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#service-availability"
  },"172": {
    "doc": "Local Installation",
    "title": "User Tutorial",
    "content": "The following step-by-step guide will show you how you can add your first components and create a flow with these components via the web ui which you deployed already. All actions are also performable via postman or similar tools. ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#user-tutorial",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#user-tutorial"
  },"173": {
    "doc": "Local Installation",
    "title": "Creating Components",
    "content": "First, we have to create two components in order to have a source and target component. Below you will find code snippets for two exemplary components. For the beginning we recommend to use those but feel free to use your own. Example 1: . { \"data\":{ \"distribution\":{ \"type\":\"docker\", \"image\":\"elasticio/timer:ca9a6fea391ffa8f7c8593bd2a04143212ab63f6\" }, \"access\":\"public\", \"name\":\"Timer\", \"description\":\"Timer component that periodically triggers flows on a given interval\" } } . Example 2: . { \"data\": { \"distribution\": { \"type\": \"docker\", \"image\": \"elasticio/code-component:7bc2535df2f8a35c3653455e5becc701b010d681\" }, \"access\": \"public\", \"name\": \"Node.js code\", \"description\": \"Node.js code component that executes the provided code\" } } . The timer component is used to trigger flows on a provided interval, while the code component executes the code that was provided by the flow creator. In order to add those components, visit the web ui (web-ui.localoih.com) and navigate to the Components section. Now click on the ADD+ button. A popup window will appear where you can add the code provided above. GREAT! You created your first component. Repeat this step for the second component. !! In order to create the flow in the next step you have to copy the ids of the components you just created. !! . ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#creating-components",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#creating-components"
  },"174": {
    "doc": "Local Installation",
    "title": "Creating Flows",
    "content": "Now that you successfully created two components it is time to create your first flow. Below you will find code snippets for an example flow. This excample flow periodically triggers the flow and sends request to webhook.site. For the beginning we recommend to use this flow but feel free to create your own. Please replace the ADD COMPONENT ID HERE with the ids you copied in the previous step. Furthermore please go to and copy the link to you clipboard. Afterwards please replace the ADD WEBHOOK URL HERE with the link in your clipboard. { \"name\":\"Timer To Code Component Example\", \"description:\": \"This flow periodically triggers the flow and sends request to webhook.site\", \"graph\":{ \"nodes\":[ { \"id\":\"step_1\", \"componentId\":\"ADD COMPONENT ID HERE\", \"function\":\"timer\" }, { \"id\":\"step_2\", \"componentId\":\"ADD COMPONENT ID HERE\", \"function\":\"execute\", \"fields\":{ \"code\":\"function* run() {console.log('Calling external URL');yield request.post({uri: 'ADD WEBHOOK URL HERE', body: msg, json: true});}\" } } ], \"edges\":[ { \"source\":\"step_1\", \"target\":\"step_2\" } ] }, \"cron\":\"*/2 * * * *\" } . In order to add the flow, navigate to the Flows section. Now click on the ADD+ button. A popup window will appear where you can add the code provided above. GREAT! You created your first flow. ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#creating-flows",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#creating-flows"
  },"175": {
    "doc": "Local Installation",
    "title": "Starting Flows",
    "content": "Now that you have created two components and a flow, it is time to start this flow. Stay in the flows section and look for the flow you just created. On the right side you will the a “play” symbol. Click on it and the how the status changes from inactive to starting. After some time the status changes to active and the flow is running (you may have to refresh the site). ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#starting-flows",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#starting-flows"
  },"176": {
    "doc": "Local Installation",
    "title": "Lessons Learned",
    "content": "In this tutorial you have learned… . | How to create components via the web ui | How to create a flow within the Open Integration Hub using existing components | How to start a flow and track its status | . ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html#lessons-learned",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html#lessons-learned"
  },"177": {
    "doc": "Local Installation",
    "title": "Local Installation",
    "content": ". ",
    "url": "http://localhost:4000/docs/Getting%20Started/LocalInstallationGuide.html",
    "relUrl": "/docs/Getting%20Started/LocalInstallationGuide.html"
  },"178": {
    "doc": "Logging Service",
    "title": "Logging service",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/LoggingService.html#logging-service",
    "relUrl": "/docs/Services/LoggingService.html#logging-service"
  },"179": {
    "doc": "Logging Service",
    "title": "Introduction",
    "content": "This service is responsible for providing a convenient API to access flow execution logs. API Reference Implementation . ",
    "url": "http://localhost:4000/docs/Services/LoggingService.html#introduction",
    "relUrl": "/docs/Services/LoggingService.html#introduction"
  },"180": {
    "doc": "Logging Service",
    "title": "Technologies used",
    "content": ". | Node.js | Typescript | GCP Stack Driver | . ",
    "url": "http://localhost:4000/docs/Services/LoggingService.html#technologies-used",
    "relUrl": "/docs/Services/LoggingService.html#technologies-used"
  },"181": {
    "doc": "Logging Service",
    "title": "How it works",
    "content": "Each of flow components running in its own Docker container and write its logs to stdout and stderr. These log entries are being collected by GCP Stack Driver. This service allows to get the logs page by page using a convenient API. Interaction with other Services . | Interacts with IAM to introspect provided IAM token. | . ",
    "url": "http://localhost:4000/docs/Services/LoggingService.html#how-it-works",
    "relUrl": "/docs/Services/LoggingService.html#how-it-works"
  },"182": {
    "doc": "Logging Service",
    "title": "Logging Service",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/LoggingService.html",
    "relUrl": "/docs/Services/LoggingService.html"
  },"183": {
    "doc": "Message Oriented Middleware",
    "title": "Message Oriented Middleware ",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/MessageOrientedMiddleware.html#message-oriented-middleware-",
    "relUrl": "/docs/Services/MessageOrientedMiddleware.html#message-oriented-middleware-"
  },"184": {
    "doc": "Message Oriented Middleware",
    "title": "Introduction",
    "content": "The Message Oriented Middleware stores and routes messages while transferring them from senders to receivers. ",
    "url": "http://localhost:4000/docs/Services/MessageOrientedMiddleware.html#introduction",
    "relUrl": "/docs/Services/MessageOrientedMiddleware.html#introduction"
  },"185": {
    "doc": "Message Oriented Middleware",
    "title": "Technologies used",
    "content": ". | RabbitMQ | . ",
    "url": "http://localhost:4000/docs/Services/MessageOrientedMiddleware.html#technologies-used",
    "relUrl": "/docs/Services/MessageOrientedMiddleware.html#technologies-used"
  },"186": {
    "doc": "Message Oriented Middleware",
    "title": "How it works",
    "content": "An integration flow is represented by a directed acyclic graph in which nodes are represented by integration components communicating with a particular API or executing some custom logic. The edges of the integration graph define which of two components are connected. An integration flow is executed by number of Pods, each representing a flow’s node, also called a flow step. The steps communicate which each other through a messaging queue, such as RabbitMQ. The following diagram displays an example of an integration flow using a message broker. In the diagram above Step 1 is a trigger component producing data by polling an API periodically. The produced messages are sent to a queue connecting Step 1 and Step 2. Because the component in Step 2 is very slow, its consumption rate is lower than the publish rate. The result is that the queue is growing. That’s why 2 instances of Step 2 are started, each consuming messages from the same queue. The message broker makes sure that the messages are sent to a single consumer only. Interaction with other Services . ",
    "url": "http://localhost:4000/docs/Services/MessageOrientedMiddleware.html#how-it-works",
    "relUrl": "/docs/Services/MessageOrientedMiddleware.html#how-it-works"
  },"187": {
    "doc": "Message Oriented Middleware",
    "title": "Message Oriented Middleware",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/MessageOrientedMiddleware.html",
    "relUrl": "/docs/Services/MessageOrientedMiddleware.html"
  },"188": {
    "doc": "Meta Data Repository",
    "title": "Meta Data Repository ",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/MetaDataRepository.html#meta-data-repository-",
    "relUrl": "/docs/Services/MetaDataRepository.html#meta-data-repository-"
  },"189": {
    "doc": "Meta Data Repository",
    "title": "Introduction",
    "content": "The Meta Data Repository is responsible for storing domains and their master data models. The models stored within this service are consulted for different tasks such as data validation. The meta models are also used by the transformer to map the incoming data onto the Open Integration Hub standard. API Reference Implementation Event Controller . ",
    "url": "http://localhost:4000/docs/Services/MetaDataRepository.html#introduction",
    "relUrl": "/docs/Services/MetaDataRepository.html#introduction"
  },"190": {
    "doc": "Meta Data Repository",
    "title": "Technologies used",
    "content": ". | Node.js | MongoDB | JSON Schema | . ",
    "url": "http://localhost:4000/docs/Services/MetaDataRepository.html#technologies-used",
    "relUrl": "/docs/Services/MetaDataRepository.html#technologies-used"
  },"191": {
    "doc": "Meta Data Repository",
    "title": "Purpose of the Microservice Meta Data Repository",
    "content": "If we talk about metadata in this context, we mean the description of the domains and their corresponding Master Data Models. An Open Integration Hub Master Data Model (OMDM) describes the data of a certain domain in a depth which is sufficient enough to map and synchronize the specific data of multiple applications in that domain. The meta data delivers all the information a user or customer needs to work with data within a specific domain. The domain models are specified by special workgroups. Please see the specific domain model repository for further informations on a domain and its master data model. ",
    "url": "http://localhost:4000/docs/Services/MetaDataRepository.html#purpose-of-the-microservice-meta-data-repository",
    "relUrl": "/docs/Services/MetaDataRepository.html#purpose-of-the-microservice-meta-data-repository"
  },"192": {
    "doc": "Meta Data Repository",
    "title": "Basic Version",
    "content": "Model Structure . The service is mainly responsible for storing meta models and domains. In order to unify the meta data to describe domains and models we need a model structure for both objects. Domain Object . The domain object is responsible describing the domain itself. Thus, the following object structure is proposed: . { \"$schema\":\"http://json-schema.org/draft-06/schema#\", \"$id\":\"http://json-schema.org/draft-06/schema#\", \"title\":\"Domain object description\", \"properties\":{ \"id\":{ \"type\":\"string\", \"description\":\"Unique identifier of the domain\", \"examples\":[ 1 ] }, \"name\":{ \"type\":\"string\", \"description\":\"Name of the domain\", \"examples\":[ \"products\" ] }, \"description\":{ \"type\":\"string\", \"description\":\"Short description of the domain\", \"examples\":[ \"This domain includes all product related models\" ] }, \"owners\":{ \"type\":\"array\", \"description\":\"List of owners, who have access to this domain\", \"items\": { \"properties\": { \"id\": { \"type\": \"string\" }, \"type\": { \"type\": \"string\" } } }, \"examples\":[ { \"id\": \"5bffec99a43c7f3ca95b09e6\", \"type\": \"tenant\" } ] } } } . Model Object . The model object is responsible for describing the meta model and should have a reference to the superordinated domain. Therefore, the following structure is supposed: . { \"$schema\":\"http://json-schema.org/draft-06/schema#\", \"$id\":\"http://json-schema.org/draft-06/schema#\", \"title\":\"Domain object description\", \"properties\":{ \"id\":{ \"type\":\"string\", \"description\":\"Unique identifier of the model\", \"examples\":[ \"13\" ] }, \"domaindId\":{ \"type\":\"string\", \"description\":\"Unique identifier of the domain the model belongs to\", \"examples\":[ \"1\" ] }, \"description\":{ \"type\":\"string\", \"description\":\"Short description of the model\", \"examples\":[ \"Master Data Model Products v1\" ] }, \"owners\":{ \"type\":\"array\", \"description\":\"List of owners, who have access to this domain\", \"items\": { \"properties\": { \"id\": { \"type\": \"string\" }, \"type\": { \"type\": \"string\" } } }, \"examples\":[ { \"id\": \"5bffec99a43c7f3ca95b09e6\", \"type\": \"tenant\" } ] }, \"model\":{ \"type\":\"object\", \"description\":\"A JSON schema of the actual model\", \"examples\":[ { \"$schema\":\"http://json-schema.org/schema#\", \"$id\":\"https://github.com/openintegrationhub/Data-and-Domain-Models/blob/master/src/main/schema/addresses/personV2.json\", \"title\":\"Person\", \"description\":\"Describes a natural person\", \"type\":\"object\", \"allOf\":[ { \"$ref\":\"../oih-data-record.json\" } ], \"properties\":{ \"title\":{ \"type\":\"string\", \"description\":\"Title of the person\", \"examples\":[ \"Dr.\" ] }, \"salutation\":{ \"type\":\"string\", \"description\":\"Salutation of the person\", \"examples\":[ \"Mr.\" ] }, \"firstName\":{ \"type\":\"string\", \"description\":\"Given name of the person\", \"examples\":[ \"Max\" ] } } } ] } } } . Interaction with other Services . Meta Data Repository can receive events from any service, but only directly interacts with two of them: . | Message Oriented Middleware: It receives all events through the Message Oriented Middleware. It publishes several events for most important actions e.g. “metadata.domain.deleted”. | Identity Management: It requires a bearer token created by the Identity Management to determine current user and check required permissions. | . ",
    "url": "http://localhost:4000/docs/Services/MetaDataRepository.html#basic-version",
    "relUrl": "/docs/Services/MetaDataRepository.html#basic-version"
  },"193": {
    "doc": "Meta Data Repository",
    "title": "Meta Data Repository",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/MetaDataRepository.html",
    "relUrl": "/docs/Services/MetaDataRepository.html"
  },"194": {
    "doc": "Description Table",
    "title": "Objects",
    "content": "Product . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | articleNo | String | required | Product or article no (can be changes while life cycle) | - | - | . | matchCode | String | - | Name of the product for fast and easy search and find | - | - | . | description | String | - | Long description of the product | - | - | . | extraDescription | String | - | More text space for description | - | - | . | gtin | String | - | Global trade item number, a worldwide unique number for the product, can be used for barcode and identification in foreign systems | - | - | . | costCalc | Number | - | Calculated costs of one unit of the product, is set manually or by a custom algorithm | - | - | . | costAvg | Number | - | Average costs of one unit of the product, is calculated each time an amount of units of the product is entering or leaving the stock | - | - | . | costLast | Number | - | Last purchase price of one unit of the product | - | - | . | costList | Number | - | List purchase price of one unit of the product | - | - | . | costInventory | Number | - | Cost for one unit of the product for inventory / stacktaking | - | - | . | dtCostLast | String. Format = date-time | - | Date of last purchse | - | - | . | status | String | enum | Status of the product | new | “new”, “expiring”, “blocked” | . | drawingNo | String | - | Drawing number of the product | - | - | . | version | String | - | Version of the product | - | - | . | dinStandard | String | - | DIN-standard number of the product (choicer) | - | - | . | netWeight | Number | - | Net weight of one unit of the product measured in kg | - | - | . | grossWeight | Number | - | Gross weight of one unit of the product measured in kg | - | - | . | density | Number | - | Specific gravity of the product | - | - | . | length | Number | - | Length of one unit of the product | - | - | . | width | Number | - | Width of one unit of the product | - | - | . | strength | Number | - | Strength of one unit of the product | - | - | . | innerDiameter | Number | - | Inner diameter of one unit of the product | - | - | . | outerDiameter | Number | - | Outer diameter of one unit of the product | - | - | . | volume | Number | - | Volume of one unit of the product | - | - | . | customTariff | String | - | Customs traiff number of the product | - | - | . | replacementTime | Number | - | Replacement time in days, how long does it take to get the product to stock | - | - | . | active | Number | - | Is the product active (1) or archived (2) | - | - | . | taxInformation | Array | - | IdAccount 0..4, 10..14, 0R - are pointing to tax-accounts for selling and purchasing | - | - | . | isSale | Boolean | - | Can this product be sold? | - | - | . | isPurchase | Boolean | - | Can this product be purchased? | - | - | . | isProduction | Boolean | - | Is the a product that has to be produced? | - | - | . | isWorkStep | Boolean | - | Has this product to be treated as a work-step? | - | - | . | isStockItem | Boolean | - | Must all stock related in and out of this product be related to a storage location? | - | - | . | isStockTracing | Boolean | - | Shall all stock related in and out of this product be protocoled? | - | - | . | isCheckStock | Boolean | - | May the product be stored to an inspection stock on receive and before delivery? (for quality management) | - | - | . | isService | Boolean | - | Has this product to be treated as a service? | - | - | . | isPacking | Boolean | - | Has this product to be treated as a packing? | - | - | . | isConsumable | Boolean | - | Has this product to be treated as a consumable? | - | - | . | isDivisible | Boolean | - | Is this a divisible product? | - | - | . | isShipping | Boolean | - | Has this product to be treated as a shipping? | - | - | . | isCharges | Boolean | - | Is this product managed in, and related to charges | - | - | . | isSerialNo | Boolean | - | Is this product managed in, and related to serial numbers | - | - | . | isRohsCompliant | Boolean | - | Is this product compiant to the RoHS-policy 2011/65/EU of the European Union? | - | - | . | isReverseCharge | Boolean | - | Has this product to be treated as a reverse charge product (VAT has to be payed by the buyer) | - | - | . | isDiscount | Boolean | - | Is it allowed to give a discount when selling the product? | - | - | . | isReduction | Boolean | - | Is the price for the product reduced? | - | - | . | isIntrastat | Boolean | - | Is the selling and buying of this product relevant to the EU-policy 638/2004, named “intrastat”? | - | - | . | isPreferential | Boolean | - | Is the country of origin qualified for preference according to EU-policy 1207/2001, 1617/2006, 75/2008 | - | - | . | articleGroups | Object | - | To which product- or article-group is the product related? | article group objec array | - | . | units | Array(Object) | - | Measuring of the product | unit object array | - | . | country | Object | - | Where has a product been produced | country object | - | . | storages | Array(Object) | - | Where and how much is in the physical inventory of the stock of the product | storage object array | - | . | charges | Object | - | If the product stock-holding is managed in charges (isCharges=TRUE) then the related charges can be treated here | charge object array | - | . | serial numbers | Object | - | If the product stock-holding is managed in serial-numbers (isSerialNo=TRUE) then the related serial-numbers can be treated here | serial number object array | - | . | pictures and documents | Object | - | Related pictures and documents to a product from out of the DMS | picture and document object array | - | . | dynamic attributes | Array(Object) | - | User defined product attributes | dynamic attribute object array | - | . | sets | Array(Object) | - | Set-list of products related to a header-product | set object array | - | . | boms | Array(Object) | - | Bill of material-list of products related to a header-product | bom object array | - | . | choicers | Array(Object) | - | Selection-lists for all kind of attributes that can be chosen out of a list or set of possible values - this lists can be extended by the user | choicer object array | - | . ArticleGroup . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | type | String | Required, enum | What kind of article-group is this? |   | “default”,”shop” | . | code | String | - | Number for the article-group | - | - | . | description | String | - | Description or name of the article-group | - | - | . Unit . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | isPurchase | Boolean | - | Is this unit used in purchase-processes? | - | - | . | isSale | Boolean | - | Is this unit used in sale-processes? | - | - | . | isProduction | Boolean | - | Is this unit used in production-processes? | - | - | . | description | String | Required | What is the name of the unit? | Pieces, meter, litres, seconds, bottles etc. (choicer) | - | . | abbreviation | String | - | What is the short-name of the unit? | Pcs, m, l, sec, btl etc. (choicer) | - | . Currency . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | code | String | - | Abbreviation of the currency | Eur | - | . | description | String | Required | Complete name of the currency | Euro | - | . | currencySymbol | String | - | Currency symbol | € | - | . | decimalDigits | Number | - | How many digits are behind the comma of the currency | 2 | - | . | subCode | String | - | Abbreviation of the digit-part of the currency | Cnt | - | . | subDescription | String | - | Complete name of the digit-part of the currency | Cent | - | . | divisor | Number | - | Amount of subCode within one code | 100 (100 Cnt = 1 €) | - | . | factor | Number | - | Conversion factor to calculate to the system currency (may change dayly) | 1 | - | . | base | Number | - | Conversion base to calculate to the system currency | 1 | - | . | isSystemCurrency | Boolean | - | Is this the system currency? | - | - | . Country . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | code | String | Required | IdRecord of the country | DE | - | . | numberPlate | String | - | What is the international number-plate sign for the country? | D=Germany GB=Great Britain F=France DK=Danmark etc. | - | . | description | String | - | A short description of the country (national use) | Germany | - | . | areaCode | String | - | Local code that is used in the country for the country | D | - | . | isoCode | String | - | ISO-3166 standard number for the country | 276 | - | . | isoDescription | String | - | A short description of the country (international use) | DE | - | . | countryCodePhone | String | - | International phone prefix | +49 | - | . | intrastatCode | String | - | Used in Europe for EU-trade tax control | DE |   | . | language | Object | - | Official language of the country | German | - | . | currency | Object | - | Official currency of the country | Euro | - | . Language . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | code | String | - | Language Code following IETF language tags | en-US | - | . | description | String | - | Name of the language | Deutsch | - | . | abbreviation | String | - | Short name of the language | DE | - | . The language attribute follows the IETF language tags format. In this format each language tag is composed of one or more \"subtags\" separated by hyphens (-). Each subtag is composed of basic Latin letters or digits only.(Source: Wikipedia. In the following a short list of the most used language tags is presented. | English name for language | Tag | . | English | en | . | English (United States) | en-US | . | English (Great Britain) | en-GB | . | French | fr | . | German | de | . | Polish | pl | . | Dutch | nl | . | Finnish | fi | . | Swedish | sv | . | Italian | it | . | Spanish (Spain) | es | . | Portuguese (Portugal) | pt | . | Russian | ru | . | Portuguese (Brazil) | pt-BR | . | Spanish (Mexico) | es-MX | . | Chinese (PRC) | zh-CN | . | Chinese (Taiwan) | zh-TW | . | Japanese | ja | . | Korean | ko | . Storage . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | name | String | Required | Name of the storage | E.g. SKW040206 | - | . | description1 | String | - | Description of the storage | - | - | . | description2 | String | - | Description of the storage | - | - | . | description3 | String | - | Description of the storage | - | - | . | description4 | String | - | Description of the storage | - | - | . | hall | String | - | In which hall is the storage? | Name of the building e.g. Hauptlager | - | . | corridor | String | - | In which corridor is the storage? | Name of the floor e.g. Reihe 4 | - | . | area | String | - | In which area is the storage? | Name of the area | - | . | complex | String | - | In which complex is the storage? | Name of the complex | - | . | height | String | - | Height of the storage | - | - | . | frame | String | - | In which frame is the storage? | Name of the rack e.g. Regal 2 | - | . | group | Number | - | IdRecord of a choicer record | storage group relation (choicer) | - | . | site | Number | - | IdRecord of a choicer record | storage site relation (choicer) | - | . | address | Number | - | IdRecord of an address | storage address relation (supplier or customer or…) | - | . | companyUnit | Number | - | IdRecord of a companyUnit | storage company-unit relation | - | . | isStorageReceipt | Boolean | - | Is this a storage for incoming goods? | Wareneingangs-Lager | - | . | isStorageCheck | Boolean | - | Is this a storage for goods in quality check? | Prüflager | - | . | isStorageSalvage | Boolean | - | Is this a storage for blocked goods? | Sperrlager | - | . | isStorageCommission | Boolean | - | Is this a storage for commissions? | Kommissionslager | - | . | isStorageProducts | Boolean | - | Is this a storage for self produced goods? | Fertigwarenlager | - | . | isStorageDispatch | Boolean | - | Is this a storage for the dispatch of goods? | Versandlager | - | . | isStorageConsignment | Boolean | - | Is this a storage for consignment goods? | Konsignationslager | - | . | isStorageExternal | Boolean | - | Is this storage in an external warehouse? | Aussenlager | - | . | isSupermarket | Boolean | - | Is this a storage a supermarket for kanban storage? | Supermarktlager (Kanban) | - | . | isDisposable | Boolean | - | Is this a storage readily available? | Disponierbares Lager | - | . Charge . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | chargeNo | String | Required | Number / name of the charge |   | - | . | lotNo | String | Required | External charge no of the supplier |   | - | . | flag | Number | - | Origin of the charge | Manually added, purchased, inventory | - | . | dtIn | Date | - | Date when the charge was entered into the storage |   | - | . | dtProduction | Date | - | Date when the charge was produced |   | - | . | dtExpiration | Date | - | Date when the charge expires |   | - | . | qty | Number | - | Amount of the charge | - | - | . | status | Number | - | Status of the charge | availiable, reserved, blocked etc. | - | . | storage | Object | - | Where and how much is in the physical inventory of the stock of the charge | storage object | - | . SerialNumber . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | serialNo | String | Required | Number / name of the serial-no | - | - | . | lotNo | String | Required | External serial-no no of the supplier | - | - | . | flag | Number | - | Origin of the serial-no | Manually added, purchased, inventory | - | . | dtIn | Date |   | Date when the serial-no was entered into the storage | - | - | . | dtProduction | Date | - | Date when the serial-no was produced | - | - | . | dtExpiration | Date | - | Date when the serial-no expires | - | - | . | status | Number | - | Status of the serial-no | availiable, reserved, blocked etc. | - | . | storage | Object | - | Where and how much is in the physical inventory of the stock of the seial number | storage object | - | . PictureAndDocument . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | fileName | String | - | File name |   | - | . | type | String | - | Type of dms-entry | picture, document etc. | - | . | data | Blob | Required | File content | - | - | . Dynamic attributes . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | name | String | Required | Name of the attribute | - | - | . | fileContent | Boolean | - | Is this attribute mandatory? | - | - | . | dataType | Number | - | IdRecord of a choicer record | choicer, record, text, float, int, date, time, bool | - | . | dataTypeParameter | String | - | Additional parameters for the dataType | - | - | . | strValue | String | - | Value of the attribute | content depends on the dataType | - | . | numValue | Number | - | Value of the attribute | content depends on the dataType | - | . | fltValue | Number of type: Float | - | Value of the attribute | content depends on the dataType | - | . | dtValue | Date | - | value of the attribute | Content depends on the dataType | - | . | tmValue | Time | - | value of the attribute | Content depends on the dataType | - | . | bvValue | Boolean | - | value of the attribute | Content depends on the dataType | - | . Set . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | name | string | Required | Name of the set | - | - | . | version | String | Required | Version number of the set-list | - | - | . | setItems | Object Array (of type SetItem) | - | Set list items | SetItem object | - | . SetItem . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | amount | Number | - | How many times is the product needed in the list | - | - | . | factor | Number | - | Factor that multiplies the amount | 1 | - | . Bom . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | name | string | Required | Name of the set | - | - | . | version | String | Required | Version number of the bom-list | - | - | . | bomItems | Object Array (of type BomItem) | - | Bom list items | BomItem object | - | . BomItem . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | amount | Number | - | How many times is the product needed in the list | - | - | . | factor | Number | - | Factor that multiplies the amount | 1 | - | . Choicer . | Attribute | Type | Properties | Description | Example | Possible Enumeration Options | . | description | String | Required | Name of the choicer-list | - | - | . | codeNo | Number | - | Number of the choicer-list | - | - | . | isManual | Boolean | - | Has the choicer-entry been added manually | - | - | . | isSystem | Boolean | - | Is this a system-choicer-entry (can not be changed or deleted by user) | - | - | . | choice | String | Required | Selectable item from the list | - | - | . | sort | Number | - | Sort order of the choicer-list-entry | - | - | . | strVal | String | - | Can be returned on selection | - | - | . | numVal | Number | - | Can be returned on selection | - | - | . | bcVal | Boolean | - | Can be returned on selection | - | - | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Products/ProductDescriptionTable.html#objects",
    "relUrl": "/docs/Data%20Models/Products/ProductDescriptionTable.html#objects"
  },"195": {
    "doc": "Description Table",
    "title": "Description Table",
    "content": "Table of Contents: . | Product Model Description . | Objects . | Product | ArticleGroup | Unit | Currency | Country | Language | Storage | Charge | SerialNumber | PictureAndDocument | Dynamic attributes | Set | SetItem | Bom | BomItem | Choicer | . | . | . ",
    "url": "http://localhost:4000/docs/Data%20Models/Products/ProductDescriptionTable.html",
    "relUrl": "/docs/Data%20Models/Products/ProductDescriptionTable.html"
  },"196": {
    "doc": "Products",
    "title": "Documents Model",
    "content": " ",
    "url": "http://localhost:4000/docs/Data%20Models/Products/Products%20Model.html#documents-model",
    "relUrl": "/docs/Data%20Models/Products/Products%20Model.html#documents-model"
  },"197": {
    "doc": "Products",
    "title": "Resources",
    "content": "UML Diagram . Basic Version . JSON Schema . Document . Description Table . Description Table . ",
    "url": "http://localhost:4000/docs/Data%20Models/Products/Products%20Model.html#resources",
    "relUrl": "/docs/Data%20Models/Products/Products%20Model.html#resources"
  },"198": {
    "doc": "Products",
    "title": "General Structure",
    "content": "While talking about products, we generally refer to goods, which are purchased, produced or sold. Those products can be both, physical objects or immaterial values. Products often require industry or goods specific properties and may vary strongly form one type of good to another. For example, perishable goods from the food industry are very different from construction materials, textile or services. This data model intents to cover the most common properties across all products and serve as a generic base model. It is however extendable for individual applications. ",
    "url": "http://localhost:4000/docs/Data%20Models/Products/Products%20Model.html#general-structure",
    "relUrl": "/docs/Data%20Models/Products/Products%20Model.html#general-structure"
  },"199": {
    "doc": "Products",
    "title": "Further information",
    "content": "As an open standard we are of course trying to be compatible with existing models out there. So for this model other industry standards were analyzed. If you are interested in learning more about the concepts behind this model, check out the workgroup on GitHub. GitHub . Do you have ideas, requirements or suggestions? Let us know and help to make this model more powerful. Contribution Guide . ",
    "url": "http://localhost:4000/docs/Data%20Models/Products/Products%20Model.html#further-information",
    "relUrl": "/docs/Data%20Models/Products/Products%20Model.html#further-information"
  },"200": {
    "doc": "Products",
    "title": "Products",
    "content": ". ",
    "url": "http://localhost:4000/docs/Data%20Models/Products/Products%20Model.html",
    "relUrl": "/docs/Data%20Models/Products/Products%20Model.html"
  },"201": {
    "doc": "Scheduler",
    "title": "Scheduler ",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/Scheduler.html#scheduler-",
    "relUrl": "/docs/Services/Scheduler.html#scheduler-"
  },"202": {
    "doc": "Scheduler",
    "title": "Introduction",
    "content": "The service scheduler is responsible for a periodical execution of integration flows. Implementation Service File . ",
    "url": "http://localhost:4000/docs/Services/Scheduler.html#introduction",
    "relUrl": "/docs/Services/Scheduler.html#introduction"
  },"203": {
    "doc": "Scheduler",
    "title": "Technologies used",
    "content": ". | Cron-Deamon | . ",
    "url": "http://localhost:4000/docs/Services/Scheduler.html#technologies-used",
    "relUrl": "/docs/Services/Scheduler.html#technologies-used"
  },"204": {
    "doc": "Scheduler",
    "title": "How it works",
    "content": "In the Open Integration Hub there is a great number of active integration flows to be executed periodically. Each integration can be configured with a Cron expression defining flow’s execution interval. Let’s consider the following Cron expression: . */3 * * * * . The cron expression above executes an integration flow at every 3rd minute starting from the flow’s start time. With the following Cron expression a flow is executed every Sunday at 6:00 am: . 0 6 * * 7 . The Scheduler iterates over all active integration flows and evaluates their Cron expressions. Is a flow due to be executed, Scheduler tells the Resource Coordinator to deploy the integration flow for execution. Interaction with other Services . ",
    "url": "http://localhost:4000/docs/Services/Scheduler.html#how-it-works",
    "relUrl": "/docs/Services/Scheduler.html#how-it-works"
  },"205": {
    "doc": "Scheduler",
    "title": "Scheduler",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/Scheduler.html",
    "relUrl": "/docs/Services/Scheduler.html"
  },"206": {
    "doc": "Secret Service",
    "title": "Secret Service ",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#secret-service-",
    "relUrl": "/docs/Services/SecretService.html#secret-service-"
  },"207": {
    "doc": "Secret Service",
    "title": "Introduction",
    "content": "The Secret Service is used to store and access securely client/user credentials . API Reference Implementation Service File . ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#introduction",
    "relUrl": "/docs/Services/SecretService.html#introduction"
  },"208": {
    "doc": "Secret Service",
    "title": "Technologies used",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#technologies-used",
    "relUrl": "/docs/Services/SecretService.html#technologies-used"
  },"209": {
    "doc": "Secret Service",
    "title": "How it works",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#how-it-works",
    "relUrl": "/docs/Services/SecretService.html#how-it-works"
  },"210": {
    "doc": "Secret Service",
    "title": "Introduction",
    "content": "In an environment similar to Open Integration Hub different services communicate with each other but also with external cloud services. Such external services normally require some sort of authentication and authorization in order to identify the communicating party and the it’s privileges. Typically RESTful APIs tend to secure the API with API-Keys and tokens (e.g. OAuth 2.0 tokens). The usage of certificate in combination with a private certificate authority is also possible, but this document does not focus on the concept of a Public Key Infrastructure. From a security point of view, it is reasonable to store API-Keys and tokens encrypted and have strong authorization mechanisms to access them. In a multi-tenant environment – ideally using different encryption keys for each tenant. In Open Integration Hub we need an appropriate storage for tenant or user specific key value pairs, whereupon the value can be an encrypted string, certificate, etc. All stored keys and tokens should be accessible via an API if a valid authorization exists. ice) . ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#introduction-1",
    "relUrl": "/docs/Services/SecretService.html#introduction-1"
  },"211": {
    "doc": "Secret Service",
    "title": "Use Cases and requirements in Open Integration Hub",
    "content": "When dealing with integration flows there are different scenarios when secrets are required. Here are some examples: . | Store an API-Key and use it for a specific solution in context of a specific integration flow. | Store the access &amp; refresh tokens (OAuth 2.0) acquired through user consent. | Allow sharing of secrets with other users/groups but limit read &amp; write access to the keys only to authorized users. | A connector which runs in context of an integration flow created by the user/tenant must be able to receive the required keys/tokens. | . Additionally, it is essential to have a sophisticated audit logging for all operations involving these secrets. ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#use-cases-and-requirements-in-open-integration-hub",
    "relUrl": "/docs/Services/SecretService.html#use-cases-and-requirements-in-open-integration-hub"
  },"212": {
    "doc": "Secret Service",
    "title": "Secret Service",
    "content": "A dedicated service should carry out the management of keys and secrets. Keys need to have at least one owner and the ownership should be validated through OIH IAM. Additionally, the underlying vault framework should be interchangeable through an abstraction layer, thus ensuring a stable public API. The Secret-Service will provide a CRUD-API for keys, which can be accessed by other privileged services, e.g. flow operator or even connector, as well as the users. The API may look similar to current elastic.io credentials API https://api.elastic.io/v2/docs/#credentials . Proposal for secret structure . { \"_id\": \"59f9f2ba112f28001921f274\", \"type\":\"credential\", \"name\": \"MySecret\", \"attributes\": { \"keys\":{ \"host\":\"sftp.company.org\", \"username\":\"testuser\", \"password\":\"testpassword\" } }, \"owners\": [ { \"entityType\": \"USER\", \"entityId\": \"59f747c33f1d3c001901a44e\" }, { \"entityType\": \"USER\", \"entityId\": \"59f747c33f1d3c001901a44f\" }, { \"entityType\": \"TENANT\", \"entityId\": \"59f747c33f1d3c001901a43e\" } ], \"meta\":{} } . ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#secret-service",
    "relUrl": "/docs/Services/SecretService.html#secret-service"
  },"213": {
    "doc": "Secret Service",
    "title": "Access control",
    "content": "To enforce authentication and authorization, the Secret-Service service must also store the owner or owner group, e.g. secret is owned by user with id XYZ. Similar to current elastic.io model, flow and credentials can have a group (workspace in elastic.io documentation) as an owner. If each user has at least her own private group and each tenant likewise, then the credentials owner can be described as an array of groups. Credential ownership ensures, that only users belonging to the correct group/workspace are allowed to read/modify the secrets. Apart from users, connectors also require access to credentials, e.g. when the user creates a flow and configures her credentials to be used in a specific flow step. There are two alternatives to solve this . | The service responsible for connector instantiation provides the connector with all required secrets | Each connector receives a token with which it can fetch the secrets from Secret-Service | . With the second alternative we can make sure, that each connector has access only to specific secrets by providing them with a corresponding JWT token from IAM service. This JWT token would contain as a claim the user’s group-id. This in return allows the Secret-Service to verify that the connector accesses only the credentials it actually is authorizes to read. Still, this seems a bit over-engineered compared to the first one. The second alternative does however make sense if it combined with the feature of AccessToken Auto-Refreshing. As IAM currently does not have or support the concept of groups, we will implement the access control in the first phase as a tuple of entityType and entityId, e.g. { \"entityType\": \"USER\", \"entityId\": \"59f747c33f1d3c001901a44e\" }, { \"entityType\": \"TENANT\", \"entityId\": \"59f747c33f1d3c001901a44f\" } . and later as only a groupId, which must also exist in IAM. The following diagram illustrates both alternatives. ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#access-control",
    "relUrl": "/docs/Services/SecretService.html#access-control"
  },"214": {
    "doc": "Secret Service",
    "title": "Flows, Connectors and Credentials",
    "content": "In an integration flow, each connector represented as a node, could receive it’s own JWT token containing the claims to access those credentials, which are required in this specific step. This could be accomplished for example through scheduler or flow manager, which ever is responsible for creation of connector instances and passing of environment variables to them. The connector then can fetch all required secrets from Secret-Service and provide the JWT token containing the claims. The Secret-Service validates the request and returns the secrets, if the connector is authorized to access them. The JWT payload could contain as less as the secret ids required for this specific integration step. The same ids could be present in the flow data payload, which the connector receives from the queue upon initialization. ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#flows-connectors-and-credentials",
    "relUrl": "/docs/Services/SecretService.html#flows-connectors-and-credentials"
  },"215": {
    "doc": "Secret Service",
    "title": "Summary",
    "content": ". | Service account(s) or JWT for a connector to fetch secrets | Adapter communicates with an external application and requires user’s secret . | Adapter either fetches the secrets itself from secrets service using it’s own service account and context (context may be the flow or flow step, and thus limit the access only to secrets of flow owner) | or receive the secret as env vars through a superordinate priveleged service | . | In case of OAuth access tokens, a separate service could be used to refresh access tokens (singleton) and all services using an access token must request it from this singleton service . | Advantage: only one token refresh, all depending connectors fetch the new access token from this service | . | Secrets are stored encrypted | Access control to secrets is based on IAM authentication and authorization mechanisms | Secret-Service has it’s own mongodb, where it stores: secret_id (secret is stored in vault, but referenced through secret_id), owner (user or tenant) | Secret-Service can fetch and return a new access token using the refresh token it stored in vault | . ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#summary",
    "relUrl": "/docs/Services/SecretService.html#summary"
  },"216": {
    "doc": "Secret Service",
    "title": "Secrets management framework",
    "content": "Our research for a suitable and mature solution lead us to HashiCorp Vault, which we will be using for our prototypical implementation. Other solutions should be possible to integrate, as the Secret-Service acts also as an abstraction layer of the underlying vault framework. This allows to interchange the vault framework through other solutions, as long as the Secret-Service API persists. For a list of secrets managements solutions (albeit focused more on infrastructure and possible not up-to-date) please see this comparison list: . https://gist.github.com/maxvt/bb49a6c7243163b8120625fc8ae3f3cd . https://www.vaultproject.io/intro/vs/index.html . ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#secrets-management-framework",
    "relUrl": "/docs/Services/SecretService.html#secrets-management-framework"
  },"217": {
    "doc": "Secret Service",
    "title": "Requirements",
    "content": ". | Strong Encryption of stored secrets | HA capability | An open source project with a large community and activity | Maturity of the framework | Good documentation | Flexible storage choice | Enterprise ready | . Compared to other solutions, we find that HashiCorp Vault fits best with our requirements. ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#requirements",
    "relUrl": "/docs/Services/SecretService.html#requirements"
  },"218": {
    "doc": "Secret Service",
    "title": "Access Token Auto-Refreshing",
    "content": "In the section Access Control we mentioned an alternative where connectors fetch the secrets directly from the Secret-Service. This approach has an advantage if at some point either Secret-Service or a correlating service also manages OAuth access tokens. In practice, this means that a connector would call Secret-Service an request an access token of a specific OAuth secret. The Secret-Service can then either return the access token, if it has a valid one or fetch the access token using for example client id and client secret. If more than one connector rely on a single access token (an identical access token), then fetching and refreshing of an access token is done ideally by a singleton – in our case Secret-Service or a correlating service responsible for this types of requests. Interaction with other Services . Secret Service can receive events from any service, but only directly interacts with two of them: . | Message Oriented Middleware: It receives all events through the Message Oriented Middleware. It publishes several events for most important actions e.g. “secrets.secret.created”. | Identity Management: It requires a bearer token created by the Identity Management to determine current user and check required permissions. | . ",
    "url": "http://localhost:4000/docs/Services/SecretService.html#access-token-auto-refreshing",
    "relUrl": "/docs/Services/SecretService.html#access-token-auto-refreshing"
  },"219": {
    "doc": "Secret Service",
    "title": "Secret Service",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/SecretService.html",
    "relUrl": "/docs/Services/SecretService.html"
  },"220": {
    "doc": "Service Collaboration",
    "title": "Service Collaboration",
    "content": "This document is designed to describe different service collaboration examples. It acts as a starting point to easily understanding the architecture of the Open Integration Hub. Most of the examples are triggered by user interactions (e.g. starting a flow) and only the “happy path” i.e. success scenario is described. Each example is described through a graphical overview, a textual description and pre-conditions. For further information for a specific version please have a look at the services. | Starting a flow . | Flow repository | Webhooks | Scheduler | . | Execute Polling Flow | Execute Webhook Flow . | POST Request | GET Request | . | Request Resources | Creating audit log records | . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html#service-collaboration",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html#service-collaboration"
  },"221": {
    "doc": "Service Collaboration",
    "title": "Starting a flow",
    "content": "Pre-Conditions: None. This example describes the scenario of starting a flow. Once the user starts a flow the following steps are processed: . | Client starts a flow using flow repository’s REST API. | Flow Repository sets the flow’s status to starting and raises the event flow.starting. | There are 3 services listening to the event flow.starting: Webhooks, Scheduler and Component Orchestrator. Webhooks and Scheduler examine the event’s payload and decide if they need to react appropriately. We will discuss the exact reaction of both services later in this document. | Upon receiving flow.starting event the Component Orchestrator starts deploying the containers. Once all containers were deployed, Component Orchestrator raises the flow.started event. | Flow Repository receives the flow.started event and switches flow’s status property from starting to started. | Webhooks receives the flow.started event and starts receiving incoming HTTP calls for the given flow. | Scheduler receives the flow.started event and starts scheduling the flow, according to it’s cron property. | When a client stops a running flow using flow repository’s REST API, the event flow.stopping is raised which is causing an inverse reaction chain of events. | . Figure: startFlow . Now let’s discuss the individual services in detail: . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html#starting-a-flow",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html#starting-a-flow"
  },"222": {
    "doc": "Service Collaboration",
    "title": "Flow repository",
    "content": ". | POST /flows/{id}/start: Used to start a flow | POST /flows/{id}/stop: Used to stop a flow | . Upon receiving the HTTP call for starting a flow, Flow Repository sets the flows status to starting. Upon receiving stopping request it sets the status to stopping. If the flow has been started and flow repsitory receives flow.started event it sets the status to active while it sets it to inactive upon receiving flow.stopped. The schema of the event payload is shown below. Event: type: object required: - headers - payload properties: headers: type: object required: - name - createdAt - serviceName properties: name: type: string createdAt: type: string format: date-time serviceName: type: string payload: type: object . The payload property is an arbitrary object to be sent with the event. Flow repository will send the entire flow as payload. ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html#flow-repository",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html#flow-repository"
  },"223": {
    "doc": "Service Collaboration",
    "title": "Webhooks",
    "content": "Upon receiving flow.starting event the service checks if the cron property is not set. If so, the service persist a data record in his local DB but doesn’t start receiving HTTP requests for the given flow yet. The following table demonstrates an example of such records. | flowId | queue | . | 58b41f5da9ee9d0018194bf3 | queue_58b41f5da9ee9d0018194bf3 | . | 5b62c91afd98ea00112d5404 | queue_5b62c91afd98ea00112d5404 | . After receiving the flow.started event, the service starts accepting incoming messages from the flow’s webhook URL and sends them to the corresponding queues to be handled by flow nodes. This is actually how it is accomplished today. The only difference is that webhooks service is retrieving all the required data about a webhook flow from its local DB. Please note that the webhooks service ignores the event if the following condition is met: . | cron property is set in the event | . Upon receiving the flow.stopping event, the service deletes the record for the given flow and stops accepting requests. ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html#webhooks",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html#webhooks"
  },"224": {
    "doc": "Service Collaboration",
    "title": "Scheduler",
    "content": "Upon receiving flow.starting event the service checks if the cron property is set. If so, the service persist a data record in his local DB, but doesn’t start scheduling the given flow yet. The following table demonstrates an example of such records. | flowId | cron | dueExecution | . | 58b41f5da9ee9d0018194bf3 | */3 * * * * | 2019-01-25T13:39:28.172 | . | 5b62c91afd98ea00112d5404 | 15 14 * * 1-5 | 2019-01-27T14:15:00.00 | . Upon receiving the flow.started event the service starts scheduling the flow executions by retrieving the flow data from its local DB. Please note that the scheduler service ignores the event if the following condition is met: . | cron property is not set in the event | . Upon receiving the flow.stopping event, the service deletes the record for the given flow and stops scheduling flow executions. ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html#scheduler",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html#scheduler"
  },"225": {
    "doc": "Service Collaboration",
    "title": "Execute Polling Flow",
    "content": "Pre-Conditions: Starting a flow. As described in scheduler section when a flow is started the service starts scheduling the flow executions. Once the scheduler finds a flow that is ready for execution it pushed a message including the relating flow ID to the queue. The recipient is the first node of the flow which is the application specific adapter. This adapter then makes a GET request to the aplications API to get the payload. Afterwards it pushes the message including the payload onto the queue. The message format of the messages emitted by scheduler have the following structure: . { \"id\" : //some record uuid, \"attachments\":{ //empty }, \"body\": { //empty }, \"headers\": { //empty }, \"metadata\": { //empty } } . Figure: executePollingFlow . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html#execute-polling-flow",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html#execute-polling-flow"
  },"226": {
    "doc": "Service Collaboration",
    "title": "Execute Webhook Flow",
    "content": " ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html#execute-webhook-flow",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html#execute-webhook-flow"
  },"227": {
    "doc": "Service Collaboration",
    "title": "POST Request",
    "content": "Pre-Conditions: Starting a flow. Once Webhooks receives a POST request it pushes the message to the queue. The recipient is the first node of the flow which is the application specific adapter. In contrast to the GET request, this request already includes the payload. The following example shows the message format of Webhooks messages: . { \"headers\": { //GET request headers }, \"query\": { //POST request query parameters }, \"body\": { //POST request body }, //other properties } . Figure: executeWebhookFlowPost . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html#post-request",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html#post-request"
  },"228": {
    "doc": "Service Collaboration",
    "title": "GET Request",
    "content": "Pre-Conditions: Starting a flow. Once Webhooks receives a GET request it takes the url parameters and request headers and put it into the message. This means in particular the headers go to headers while query string parameters go to body. It then pushes the message to the queue. The recipient is the first node of the flow which is the application specific adapter. This adapter then makes a GET request to the aplications API to get the payload. Afterwards it pushes the message including the payload onto the queue. The following example shows the message format of Webhooks messages: . { \"headers\": { //GET request headers }, \"body\": { //GET request query string parameters }, //other properties } . An examplary webhook GET request could look like the following: GET /hook/&lt;flow-id&gt;?param1=value&amp;param2=value . Figure: executeWebhookFlowGet . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html#get-request",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html#get-request"
  },"229": {
    "doc": "Service Collaboration",
    "title": "Request Resources",
    "content": "The following example shows how a user can request a resource using IAM. The graphic below shows how this example would look like if a user request a resource from the flow repository. | User logs in into IAM. | IAM responds with an ephemeral token. | User uses the ephemeral token to request a cetrain resource (e.g. a specific flow by id). | Flow repository introspects the ephemeral token at IAM (services accounts receive a permanent token when they first register) using IAM utils (middleware). | IAM responds with user information such as username, tenant, tenant specific role and user permissions related to this token. | Flow Repsitory checks if the user has the permission to request the resource. | Flow repository responds with the requested information. | . Illustration of this process: (Figur requestResourceSuccess). Figure: requestResourceSuccess . 1: Ephemeral token 2: Service makes request with service account token 3: User information e.g.: username, tenant, tenant specific role, permissions . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html#request-resources",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html#request-resources"
  },"230": {
    "doc": "Service Collaboration",
    "title": "Creating audit log records",
    "content": "To create a record that should be stored in the audit log a service simply has to put a message onto the queue with a predefined topic. Each service decides on its own, which events should be stored in the audit log service. Audit log listens to all events having audit.* as topic. ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html#creating-audit-log-records",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html#creating-audit-log-records"
  },"231": {
    "doc": "Service Collaboration",
    "title": "Service Collaboration",
    "content": ". ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/ServiceCollaborationOverview.html",
    "relUrl": "/docs/Service%20Collaboration/ServiceCollaborationOverview.html"
  },"232": {
    "doc": "Services",
    "title": "Service Overview",
    "content": "The Open Integration Hub framework consists of a variety of service. This will give you a good overview. Integration Framework Identity Access Management Secret-Service Flow Repository Component Repository Component Orchestrator Scheduler Attachment-Storage-Service Webhooks Smart Data Framework Reporting and Analytics APIs Audit Log Logging &amp; Monitoring Metadata Repository Conflict Management Data Hub Dispatcher Service Integration Layer Service Queue / Message Oriented Middleware Local Setup Google Cloud Platform Setup Service Collaboration ",
    "url": "http://localhost:4000/docs/Services/Services.html#service-overview",
    "relUrl": "/docs/Services/Services.html#service-overview"
  },"233": {
    "doc": "Services",
    "title": "Services",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/Services.html",
    "relUrl": "/docs/Services/Services.html"
  },"234": {
    "doc": "Snapshots Service",
    "title": "Snapshots service",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/SnapshotsService.html#snapshots-service",
    "relUrl": "/docs/Services/SnapshotsService.html#snapshots-service"
  },"235": {
    "doc": "Snapshots Service",
    "title": "Introduction",
    "content": "This service is responsible for storing flow snapshots. What is a snapshot? . Snapshot is a feature of the OIH platform, which provides flows components a possibility to store some state between executions. A typical use case is to store some timestamp and use it to query records from a certain system, created after this timestamp, so that you would not need to query full list of records each time. API Reference Implementation . ",
    "url": "http://localhost:4000/docs/Services/SnapshotsService.html#introduction",
    "relUrl": "/docs/Services/SnapshotsService.html#introduction"
  },"236": {
    "doc": "Snapshots Service",
    "title": "Technologies used",
    "content": ". | Node.js | Typescript | MongoDB | RabbitMQ | . ",
    "url": "http://localhost:4000/docs/Services/SnapshotsService.html#technologies-used",
    "relUrl": "/docs/Services/SnapshotsService.html#technologies-used"
  },"237": {
    "doc": "Snapshots Service",
    "title": "How it works",
    "content": "It connects to the RabbitMQ queue, which is in turn subscribed for the snapshot events, fired by flow components. The service consume the messages from the queue and writes it to the DB. The snapshots are then accessible via REST API. Interaction with other Services . | Interacts with IAM to introspect provided IAM token. | Consumes snapshot events from flow components via RabbitMQ. | Component Orchestrator queries the service and provides snapshots to each flow component. | . ",
    "url": "http://localhost:4000/docs/Services/SnapshotsService.html#how-it-works",
    "relUrl": "/docs/Services/SnapshotsService.html#how-it-works"
  },"238": {
    "doc": "Snapshots Service",
    "title": "Snapshots Service",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/SnapshotsService.html",
    "relUrl": "/docs/Services/SnapshotsService.html"
  },"239": {
    "doc": "Standardized Examples",
    "title": "Descriptions of standardized actions or triggers",
    "content": " ",
    "url": "http://localhost:4000/docs/Connectors/StandardizedActionsAndTriggers.html#descriptions-of-standardized-actions-or-triggers",
    "relUrl": "/docs/Connectors/StandardizedActionsAndTriggers.html#descriptions-of-standardized-actions-or-triggers"
  },"240": {
    "doc": "Standardized Examples",
    "title": "Table of Contents",
    "content": ". | Actions . | Upsert Object | Lookup Object (at most 1) | Lookup Objects (Plural) | Delete Object | Make RAW Request | Lookup Set Of Objects By Unique Criteria | Update Object | Create Object | Linking/Unlinking Objects | Execute Query or Statement in Query Language | Perform Action/Evaluate Function | Assert Option(s) in Set(s) | Merge Objects | . | Batch Actions | Triggers . | Get New and Updated Objects Polling | Webhooks | Bulk Extract | . | . It is important to define common rules on how an adapter responds to changes and performs actions on generic domain objects. If adapters follow common behaviours, then it is possible to build integrations by combining adapters which are developed by different developers. ",
    "url": "http://localhost:4000/docs/Connectors/StandardizedActionsAndTriggers.html#table-of-contents",
    "relUrl": "/docs/Connectors/StandardizedActionsAndTriggers.html#table-of-contents"
  },"241": {
    "doc": "Standardized Examples",
    "title": "Actions",
    "content": "Upsert Object . Example Use Case . I have some contact data that I want to add to my CRM. I don’t necessarily know if there is already a contact in my CRM, so I want the connector to be smart and determine if the data needs to be matched to an existing contact or added to a new contract. Iteration 1: Upsert Object By ID . Input Metadata . | One input per field in the ID that is optional. This field is marked as being the ID. | Inputs for other fields on the body. All fields that are not nullable and can’t be populated by the system on create should be required. | . Pseudo-Code . function upsertObjectById(obj) { // If the object's ID is split across more than one field, we should check // that either all ID fields are populated or that none are. Otherwise we // should throw an exception. const objectToUpdate = GetObjectById(obj.id); // Usually GET verb if(objectToUpdate == null) { const createdObject = CreateObject(obj); // Usually POST verb EmitData(createdObject); } else { const updatedObject = UpdateObject(obj, id); // Usually POST/PATCH verb EmitData(updatedObject); } } . Output Data . | The object post creation/update as reported by the system | . Gotcha’s to lookout for . | Updates should be partial updates . | Make sure to Url Encode IDs appearing in HTTP urls . | . Iteration 2: Update Object By Unique Criteria . Input Metadata Changes . | The fields that are part of the upsert criteria are marked as being part of the criteria. If the criteria is something other than the ID, they should be marked as required. (There is a hypothetical edge case here where the system auto-populates the unique criteria) . | . Pseudo-Code . function upsertObjectByUniqueCriteria(obj, uniqueCriteria) { // Ensure unique criteria are all populated (unless ID) // If criteria is th the object's ID and it is split across more than one field, we should check // that either all ID fields are populated or that none are. Otherwise we // should throw an exception. const objectsToUpdate = GetObjectsByCriteria(uniqueCriteria); // Usually GET verb if(objectsToUpdate.length == 0) { const createdObject = CreateObject(obj); // Usually POST verb EmitData(createdObject); } else if (objectsToUpdate.length == 1) { const updatedObject = UpdateObject(obj, objectsToUpdate[0].id); // Usually POST/PATCH verb EmitData(updatedObject); } else { throw new Error(`More than one matching object found.`); } } . Lookup Object (at most 1) . Example Use Case . I have a contact who works for a company. I have an ID or other distinguishing characteristic (e.g. legal name) of the company and I want to learn some detail about the company (e.g. number of employees). Iteration 1: Lookup Object By ID . Input Metadata . | One input per field in the ID. Depending on the value of allowIdToBeOmitted this is optional or required. | . Pseudo-Code . function lookupObjectById(id) { if(!id) { if(allowCriteriaToBeOmitted) { emitData({}); return; } else { throw new Error('No ID provided'); } } try { const foundObject = GetObjectById(id); // Usually GET verb emitData(foundObject); } catch (NotFoundException e) { if(allowZeroResults) { emitData({}); } else { throw e; } } } . Output Data . | The object as reported by the system | . Gotcha’s to lookout for . | Make sure to Url Encode IDs appearing in HTTP urls | . Not defined now . | How to handle populating linked objects. | . Iteration 2: Lookup Object By Unique Criteria . Input Metadata Changes . | The input matches the selected criteria | . Pseudo-Code . function lookupObjectByUniqueCriteria(uniqueCriteria) { if(!uniqueCriteria) { if(allowCriteriaToBeOmitted) { emitData({}); return; } else { throw new Error('No unique criteria provided'); } } const foundObjects = GetObjectsByCriteria(uniqueCriteria); // Usually GET verb if(foundObjects.length == 0) { if(allowZeroResults) { emitData({}); } else { throw new Error('Not found'); } } else if (foundObjects.length ==1) { emitData(foundObjects[0]); } else { throw new Error('More than one object found.'); } } . Lookup Objects (Plural) . Example Use Case . I want to search my CRM for data based on some criteria. Input Metadata . | Page size: optional positive integer that defaults to 100 (only if fetch page mode) | page number: required non-negative integer that is 0 based (only if fetch page mode) | order: optional array of fieldname + sort direction pairs (only if fetch page mode) | max result size: optional positive integer that defaults to 1000 (only if fetch all mode) | For each search term: . | fieldName | fieldValue | condition (equal, not equal, &gt;=, &lt;=, &gt;, &lt;, like (if supported), possibly more in the future) | . | For each search term - 1: (iteration 2) . | criteriaLink (and/or) | . | . Pseudo-Code . function lookupObjects(criteria) { switch(mode) { case 'fetchAll': const results = GetObjectsByCriteria(criteria); if(results.length &gt;= maxResultSize) { throw new Error('Too many results'); } emitData({results: results}); break; case 'emitIndividually': const results = GetObjectsByCriteria(criteria); results.forEach(result =&gt; { emitData(result); } break; case 'fetchPage': const results = GetObjectsByCritieria(criteria, top: pageSize, skip: pageSize * pageNumber, orderBy: orderByTerms); emitData({results: results}); break; } } . Output Data . | An object, with key results that has an array as its value. | . Gotcha’s to lookout for . | Make sure to Url Encode field values appearing in HTTP urls | . Not Handled . | Order of operations in multiple terms | How to get total number of matching objects | . Delete Object . Example Use Case . I know the ID of a customer that I want to delete. Iteration 1: Delete Object By ID . Input Metadata . | One input per field in the ID that is required. | . Pseudo-Code . function deleteObjectById(id) { try { DeleteObjectById(id); // Usually DELETE verb } catch (NotFoundException e) { emitData({}); return; } emitData({id: id}); } . Output Data . | The id of the object deleted. | . Gotcha’s to lookout for . | If zero objects are deleted, then the empty object should be emitted | Make sure to Url Encode IDs appearing in HTTP urls | . Iteration 2: Delete Object By Unique Criteria . Input Metadata Changes . | The input matches the selected criteria | . Pseudo-Code . function deleteObjectByUniqueCriteria(uniqueCriteria) { const foundObjects = GetObjectsByCritieria(uniqueCriteria); // Usually GET verb if(foundObjects.length == 0) { emitData({}); } else if (foundObjects.length == 1) { DeleteObjectById(foundObjects[0].id); // Usually DELETE verb emitData({id: foundObjects[0].id}); } else { throw new Error('More than one object found.'); } } . Make RAW Request . This action has not been fully standardized. A simple action to allow integrators to assemble requests to be sent to the system. The component should expose the parts that vary in a typical request. The component should handle authentication and error reporting. Example Use Case . I’m a technically advanced user who wants to interact with a system in a way not permissible by the existing component actions but would like some simplification relative to using the REST component. Lookup Set Of Objects By Unique Criteria . Given an array of information where each item in the array uniquely describes exactly one object. It can be assumed that the array is short enough to reasonably fit the results in a single message. Example Use Case . I salesperson is responsible for 0 to N accounts. I would like to look up a piece of information for each account associated with the salesperson. Iteration 1: Lookup Object By ID . Iteration 2: Lookup Object By Unique Criteria . Input Metadata . | An array where each item has one input per field in the ID. If the ID is a single field, this input can be a simple array (as opposed to an array of objects). Required. | . Pseudo-Code (Multi-Field ID Case) . function lookupSetOfObjects(itemUniqueCriteriaListToLookup) { const results = itemUniqueCriteriaListToLookup.map(itemUniqueCriteria =&gt; { const matchingItems = GetObjectsByCriteria(itemUniqueCriteria); if(matchingItems.length != 1) { throw new Error(`Lookup failed for ${itemUniqueCriteria}`); } return { key: itemCriteria, value: matchingItems[0] } }) EmitData(results); } . Pseudo-Code (Single-Field ID Case Where IN Operator is Supported) . function lookupSetOfObjects(itemIdsToLookup) { if(itemIds.length = 0) { EmitData({}); return; } const searchResults = FetchObjectsWhereIdIn(itemIdsToLookup); const resultDictionary = {}; for each (let itemId of itemIdsToLookup) { const matchingItems = searchResults.filter(result.Id = itemId); if(matchingItems.length != 1) { throw new Error(`Lookup failed for ${itemUniqueCriteria}`); } resultDictionary[itemId] = matchingItems[0]; } EmitData(resultDictionary); } . Output Data . | In the case of a single field criteria, a dictionary of the form lookupCriteria: matchingItem | In the case of multi-field criteria, an array where each item in the array has the form {\"key\": lookupCriteria, \"value\": matchingItem} | . Gotcha’s to lookout for . | Make sure to Url Encode IDs appearing in HTTP urls | . Not defined now . | Encode any IDs in URLs | Rebounds when an object is not found | There are different structures depending on the input structure | . Update Object . | Similar to upsert object (both iteration 1 &amp; 2) but: . | We will not create the object if it does not exist | The ID/other unique criteria is required | No other fields are required | . | . Example Use Case . I want to update the price of a product based on its SKU but I don’t want to look up other required attributes such as name since I know those have already been set and are not changing. Create Object . Similar to upsert object but needed for the following cases: . | Objects that can be created but can not be updated after creation (e.g. Invoices) | Cases where you want to create an object and its children | Cases where the id of the object includes information in the object (e.g. The ID of a sales line is the sales order ID + SKU). | . Example Use Case . See above. Linking/Unlinking Objects . | Given a many-to-many relationship in a system: create/update/remove a relationship between two objects. | In order to do this, the inbound metadata needs to include: . | the types of the two objects | two sets of unique criteria which describe the two objects | Information about the relationship (e.g. if assigning user to company membership, identify the role of the user) . function linkObjects(obj1, obj2, linkMetadata) { const matchingObjects1 = lookupObjectByCriteria(obj1.type, obj1.uniqueCriteria); if (matchingObjects1.length != 1) { throw new Error('Not found/too many found.'); } const object1Id = matchingObjects1[0].id; const matchingObjects2 = lookupObjectByCriteria(obj2.type, obj2.uniqueCriteria); if (matchingObjects2.length != 1) { throw new Error('Not found/too many found.'); } const object2Id = matchingObjects2[0].id; createLink(object1Id, object2Id, linkMetadata); } . | . | . Example Use Case . A student can be a participant in a class and a class can have many students. Given a student ID and a course ID I want to enroll that student in that course. Execute Query or Statement in Query Language . Examples of this include constructing a query or statement in SQL, Salesforce’s SOQL, etc. Queries return a table of data when executed. Statements do not reutrn results (other than execution statistics). Example Use Case . Execute SQL query in SQL database . Input Metadata . | For each parameterized variable in the query, there should be an input | If Emit Behavior is Expect Single: a boolean input “Allow Zero Results” | . Pseudo-Code . function executeQuery(query, params, mode, allowZeroResults) { const results = executeQueryOrStatementOnSystem(query, params); if(results is not QueryResultsTable) { emitData(results || {}); return; } switch(mode) { case 'fetchAll': if(results.length &gt;= maxResultSize) { throw new Error('Too many results'); } emitData({results: results}); break; case 'emitIndividually': results.forEach(result =&gt; { emitData(result); } break; case 'expectSingle': if(results.length = 1) { emitData(results[0]); } else if(results.length = 0 &amp;&amp; allowZeroResults) { emitData({}); } else { throw new Error('Incorrect Number of Results Found'); } break; } } . Output Data . | If Query: Depends on mode | If Statement: Execution statistics if available. Otherwise an empty object as a result. | . Perform Action/Evaluate Function . This action has not been fully standardized. Examples of this include sendEmail, calculatePrice, etc. Assert Option(s) in Set(s) . Given a field which can be set to a fixed list of options, ensure that this option exists in the list of selectable options. Example Use Case . You run a store where each product has a color. The list of colors are finite (e.g. red, green, blue). One day, you decide to add a new color option yellow. Input Metadata . For each field in the object type where the populated data is a dropdown/multi-select . | One input which identifies the option value that needs to be in the set. Optional | Inputs for any data related to the option being added. Not applicable for all systems Iteration 2: Have the above receive an array | . Pseudo-Code . let existingOptions = populateExistingOptions(); function assertOptionInSet(toAssert) { const optionsDictionary = {}; for([field, option] of Object.entries(toAssert)) { let existingOption = existingOptions[field][option.value]; if(existingOption &amp;&amp; existingOption.additionalData = option.additionalData) { // Nothing to do. Publish info on the option optionsDictionary[field] = existingOption; break; } // Check for stale cached data existingOptions = populateExistingOptions(); let existingOption = existingOptions[field][option.value]; if(existingOption &amp;&amp; existingOption.additionalData = option.additionalData) { // Nothing to do. Publish info on the option optionsDictionary[field] = existingOption; break; } if(!existingOption) { // Need to add option const addedOption = AddOptionToSet(field, option.value, option.additionalData); optionsDictionary[field] = addedOption; } else { // Need to update option const updatedOption = UpdateOptionInSet(field, option.value, option.additionalData); optionsDictionary[field] = updatedOption; } } EmitData(optionsDictionary); } . Output Data . | An object with all the options and associated data | . Gotcha’s to lookout for . | This action can not be run in parallel | . Merge Objects . This action has not been fully standardized. Example: Contact merge. There are usually two contacts, A and B with different IDs. At the end of the merge, one ID remains with all external references to the other contact now pointing to the remaining contact. ",
    "url": "http://localhost:4000/docs/Connectors/StandardizedActionsAndTriggers.html#actions",
    "relUrl": "/docs/Connectors/StandardizedActionsAndTriggers.html#actions"
  },"242": {
    "doc": "Standardized Examples",
    "title": "Batch Actions",
    "content": "This set of actions has not been fully standardized. It is possible to make batch variants for many of the above actions. The batch action should perform operations which behave as the other options described above. ",
    "url": "http://localhost:4000/docs/Connectors/StandardizedActionsAndTriggers.html#batch-actions",
    "relUrl": "/docs/Connectors/StandardizedActionsAndTriggers.html#batch-actions"
  },"243": {
    "doc": "Standardized Examples",
    "title": "Triggers",
    "content": "Get New and Updated Objects Polling . Example Use Case . I want to learn about changes to contacts in my CRM when they happen. Input Metadata . N/A . Pseudo-Code . function getObjectsPolling(cfg, snapshot) { const previousLastModified = snapshot.previousLastModified || cfg.startTime || new Date(0); const maxTime = cfg.endTime || Date.MaxDate(); let hasMorePages = true; snapshot.pageNumber = snapshot.pageNumber || 0; let lastSeenTime = previousLastModified; do { let whereCondition; if (previousLastModified === cfg.startTime || new Date(0)){ whereCondition = [ lastModified &gt;= previousLastModified, lastModified &lt;= maxTime ]; } else { whereCondition = [ lastModified &gt; previousLastModified, lastModified &lt;= maxTime ]; } const pageOfResults = GetPageOfResults({ orderBy: Time ascending where: whereCondition, top: sizeOfPollingPage, skip: snapshot.pageNumber * sizeOfPollingPage }); pageOfResults.forEach(result =&gt; { emitData(result); }; snapshot.pageNumber++; hasMorePages = pageOfResults.length == pageSize; if(pageOfResults.length &gt; 0) { lastSeenTime = pageOfResults[pageOfResults.length - 1].lastModified; } emitSnapshot(snapshot); if(singlePagePerInterval &amp;&amp; hasMorePages) { return; } } while (hasMorePages) delete snapshot.pageNumber; snapshot.previousLastModified = lastSeenTime; emitSnapshot(snapshot); } . Output Data . | Each object emitted individually. | . Gotcha’s to lookout for . | If previousLastModified is set to lastSeenTime and we have lastModified &gt;= previousLastModified then each execution will include records from previous execution. But if at the first execution previousLastModified could be equal cfg.startTime and we have lastModified &gt; previousLastModified then we will lose objects whose last modified date is equal to the cfg.startTime. This is why we compare previousLastModified and cfg.startTime || new Date(0) and if they are equal, use condition lastModified &gt;= previousLastModified, else: lastModified &gt; previousLastModified, | We use lastModified &lt;= maxTime as it is more understandable for user. | We have Single Page per Interval default to yes because it is slightly safer. | TODO | . Webhooks . Receives data pushed to the Open Integration Hub from an external system. The webhook URL is being provided by the Webhook Service for each flow. Bulk Extract . Useful for: . | Systems that do no track last_modified | Systems that don’t support filtering by timestamp range | Systems which have dedicated bulk export functionality | . ",
    "url": "http://localhost:4000/docs/Connectors/StandardizedActionsAndTriggers.html#triggers",
    "relUrl": "/docs/Connectors/StandardizedActionsAndTriggers.html#triggers"
  },"244": {
    "doc": "Standardized Examples",
    "title": "Standardized Examples",
    "content": " ",
    "url": "http://localhost:4000/docs/Connectors/StandardizedActionsAndTriggers.html",
    "relUrl": "/docs/Connectors/StandardizedActionsAndTriggers.html"
  },"245": {
    "doc": "Transformer",
    "title": "Transformer",
    "content": "As already mentioned the transformer transforms one JSON object into another. Prior to this transformation a semantic mapping has to take place where the entities of the source model are mapped against the entities of the Open Integration Hub master data model. ",
    "url": "http://localhost:4000/docs/Connectors/Transformer.html",
    "relUrl": "/docs/Connectors/Transformer.html"
  },"246": {
    "doc": "Transformer",
    "title": "Table of Contents",
    "content": ". | Table of Contents | Transformer Concept | Transformation Language | Transformer Evolution | . ",
    "url": "http://localhost:4000/docs/Connectors/Transformer.html#table-of-contents",
    "relUrl": "/docs/Connectors/Transformer.html#table-of-contents"
  },"247": {
    "doc": "Transformer",
    "title": "Transformer Concept",
    "content": "As can be seen in the connector overview a transformer should transform data in both directions: . | From source model to Open Integration Hub master data model | From Open Integration Hub master data model to source model | . A transformer expects a JSON object as an input. Depending on the direction of the transformation the input either represents the structure of the proprietary data model (flow direction 1) or the structure of the Open Integration Hub master data model (flow direction 2). Afterwards it transforms the incoming JSON object into another JSON object. This can also be done via a transformation language like JSONata. Depending on the transformation direction, the transformer’s output is then send either to the Open Integration Hub and is validated against a deposited JSON schema or to the corresponding adapter. _Note: As the Open Integration Hub is feasible of storing different data models, it is also possible that the mapping of the source model is done against another model than the Open Integration Hub master data model. ",
    "url": "http://localhost:4000/docs/Connectors/Transformer.html#transformer-concept",
    "relUrl": "/docs/Connectors/Transformer.html#transformer-concept"
  },"248": {
    "doc": "Transformer",
    "title": "Transformation Language",
    "content": "One way of transforming the JSON objects is the usage of a transformation language. As already mentioned one possible transformation language is JSONata as it is especially built to transform one JSON object into another. For detailed information on transformation language and JSONata as well as a general example please have a look at TransformationLanguage . ",
    "url": "http://localhost:4000/docs/Connectors/Transformer.html#transformation-language",
    "relUrl": "/docs/Connectors/Transformer.html#transformation-language"
  },"249": {
    "doc": "Transformer",
    "title": "Transformer Evolution",
    "content": "Sometimes there is a need to change the existing data model to adjust to different requirements. In this case, a transformer needs to be adjusted/updated in order to be compatible with the newest version of the data model. Some changes do not affect existing mappings/transformations and transformers can still be used (although they are not referring to the newest model version). Thus, backward compatibility is automatically given. According to the OData specification (shortened/modified list) the following changes do not require a change within a transformer: . | Adding an attribute that is nullable | Adding an object to the model | Adding a new complex type to the model | Adding an option to an enumeration | . Apart from this various changes require transformer adjustments as they break existing mappings/transformations. Thus, backward compatibility is not automatically given (in case the Open Integration Hub operator does not run both model versions in parallel). The following list provides an overview of changes that require a transformer adjustment: . | Renaming an existing attribute | Changing the type of an existing attribute | Changing the properties of an existing attribute from nullable to not nullable | Deleting an existing attribute | Renaming an existing object | Deleting an existing object | Adding an attribute that is not nullable | . If a new version of a model is created, two options exist to create a transformer that is compatible with the newest model version: . | The existing transformer is adjusted/updated according to the structure of the new data model. | A completely new transformer is built. | . ",
    "url": "http://localhost:4000/docs/Connectors/Transformer.html#transformer-evolution",
    "relUrl": "/docs/Connectors/Transformer.html#transformer-evolution"
  },"250": {
    "doc": "Webhooks",
    "title": "Webhooks ",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/Webhooks.html#webhooks-",
    "relUrl": "/docs/Services/Webhooks.html#webhooks-"
  },"251": {
    "doc": "Webhooks",
    "title": "Introduction",
    "content": "Receives http calls and passes messages to execution. Implementation Service File . ",
    "url": "http://localhost:4000/docs/Services/Webhooks.html#introduction",
    "relUrl": "/docs/Services/Webhooks.html#introduction"
  },"252": {
    "doc": "Webhooks",
    "title": "Technologies used",
    "content": " ",
    "url": "http://localhost:4000/docs/Services/Webhooks.html#technologies-used",
    "relUrl": "/docs/Services/Webhooks.html#technologies-used"
  },"253": {
    "doc": "Webhooks",
    "title": "How it works",
    "content": "It listens for incoming HTTP connections, serializes incoming data and puts it to the queue of the first node of the flow. The message is being consumed and processed by the component. Available endpoints . | HEAD /hook/{flowId} - returns 200 if a flow is found and ready for receiving messages, otherwise 404. This endpoint doesn’t process any incoming data. | GET /hook/{flowId} - this endpoint processes incoming data. It’s possible to pass message arguments as query params and headers. | POST /hook/{flowId} - this endpoint processes incoming data. It allows to pass data in request body, headers or query params. | . ",
    "url": "http://localhost:4000/docs/Services/Webhooks.html#how-it-works",
    "relUrl": "/docs/Services/Webhooks.html#how-it-works"
  },"254": {
    "doc": "Webhooks",
    "title": "Prerequisites",
    "content": ". | RabbitMQ | MongoDB | . ",
    "url": "http://localhost:4000/docs/Services/Webhooks.html#prerequisites",
    "relUrl": "/docs/Services/Webhooks.html#prerequisites"
  },"255": {
    "doc": "Webhooks",
    "title": "How to build",
    "content": "docker build -t openintegrationhub/communication-router:latest -f Dockerfile ../../ . or . VERSION=latest npm run build:docker . ",
    "url": "http://localhost:4000/docs/Services/Webhooks.html#how-to-build",
    "relUrl": "/docs/Services/Webhooks.html#how-to-build"
  },"256": {
    "doc": "Webhooks",
    "title": "How to deploy",
    "content": "Kubernetes descriptors for Communication Router along with the other core platform microservices can be found in the k8s directory. ",
    "url": "http://localhost:4000/docs/Services/Webhooks.html#how-to-deploy",
    "relUrl": "/docs/Services/Webhooks.html#how-to-deploy"
  },"257": {
    "doc": "Webhooks",
    "title": "Environment variables",
    "content": "General . | Name | Description | . | LISTEN_PORT | Port for HTTP interface. | . | LOG_LEVEL | Log level for logger. | . | MONGODB_URI | MongoDB connection string. | . | PAYLOAD_SIZE_LIMIT | Maximum request’s payload size that could be handled. | . | RABBITMQ_URI | RabbitMQ connection URI for the Resource Coordinator application. | . ",
    "url": "http://localhost:4000/docs/Services/Webhooks.html#environment-variables",
    "relUrl": "/docs/Services/Webhooks.html#environment-variables"
  },"258": {
    "doc": "Webhooks",
    "title": "Webhooks",
    "content": ". ",
    "url": "http://localhost:4000/docs/Services/Webhooks.html",
    "relUrl": "/docs/Services/Webhooks.html"
  },"259": {
    "doc": "Development Example for Node.js",
    "title": "Deprecated :warning:",
    "content": "This petstore example is only compatible with the legacy release of the Open Integration Hub. An updated example will be released in the future. Please see one of the existing connectors for an updated example. Snazzy Contacts Adapter . Snazzy Contacts Transformer . ",
    "url": "http://localhost:4000/docs/Connectors/building-nodejs-component.html#deprecated-warning",
    "relUrl": "/docs/Connectors/building-nodejs-component.html#deprecated-warning"
  },"260": {
    "doc": "Development Example for Node.js",
    "title": "Node.js Development Example",
    "content": "Open Integration Hub supports Node.js programming language for building integration components such as Adapters and Transformers. To help you create an Adapter in Node.js we have created a simple Petstore component in Node.js which connects to the Petstore API. This component is specified for the elastic.io iPaaS platform, which uses many Open Integration Hub services. Using elastic.io for learning the concepts makes it easier for you, as you can interact with the components via user interface - which is just more comfortable than using the framework alone. But don’t worry, the technological basics are the same. For now lets start with understanding the different parts a typical adapter: . ",
    "url": "http://localhost:4000/docs/Connectors/building-nodejs-component.html#nodejs-development-example",
    "relUrl": "/docs/Connectors/building-nodejs-component.html#nodejs-development-example"
  },"261": {
    "doc": "Development Example for Node.js",
    "title": "Petstore Component",
    "content": "Let us have a look at the structure of the Petstore Component in Node.js: . petstore-component-nodejs ├── component.json (1) ├── lib │   ├── actions (2) │   │   ├── createPetWithGenerators.js │   │   └── createPetWithPromise.js │   ├── schemas (3) │   │   ├── createPet.in.json │   │   ├── createPet.out.json │   │   └── getPetsByStatus.out.json │   └── triggers (4) │   ├── getPetsByStatusWithDynamicSelectModel.js │   ├── getPetsByStatusWithGenerators.js │   └── getPetsByStatusWithPromises.js ├── logo.png (5) ├── package.json (6) └── verifyCredentials.js (7) . The Node.js components get built by NPM run-script which checks first the package.json (6) configuration file and starts initialising the node and npm versions and builds them. Next, the dependencies get downloaded and build. All node.js components must use the following dependencies: . \"dependencies\": { \"elasticio-sailor-nodejs\": \"^2.2.0\", \"elasticio-node\": \"^0.0.8\" } . Sailor is the Node.js SDK. It makes your component a citizen of the platform by providing a simple programming model for components and ensuring a smooth communication with the platform. The component.json file (1) is the component descriptor interpreted by the platform to gather all the required information for presenting to the user in the platform user interface (UI). For example, you can define the component’s title in the component descriptor but also the component’s authentication mechanism. The descriptor is the only place to list the functionality provided by the component, the so called triggers and actions. The directory lib together with its sub-directories actions (2), schemas (3) and triggers (4) get defined in the component.json file. The Node.js sources are in the sub-directories lib/actions (2) and lib/triggers (4). The JSON schemas defining the component’s metadata are in the sub-directory lib/schemas (3). We discuss them in more details later in this article. If you have a logo for the component, you can place the file called logo.png (5) in the root directory of the component. Typically the logo of the API vendor gets used as component logo. If you did not provide any logo, the component will show a generic logo for your component. Last but not least, the verifyCredentails.js (7) file should contain the main verification mechanism for all the Node.js components built for the platform. We cover this topic later in the article. ",
    "url": "http://localhost:4000/docs/Connectors/building-nodejs-component.html#petstore-component",
    "relUrl": "/docs/Connectors/building-nodejs-component.html#petstore-component"
  },"262": {
    "doc": "Development Example for Node.js",
    "title": "Component descriptor",
    "content": "As mentioned above, the component.json file is the component descriptor interpreted by the platform to gather all the required information about the component. Let’s explore the descriptor of the Petstore component: . { \"title\": \"Petstore API (Node.js)\", (1) \"description\": \"Component for the Petstore API\", (2) \"credentials\": { (3) \"fields\": { \"apiKey\": { \"label\": \"API key\", \"required\": true, \"viewClass\": \"TextFieldWithNoteView\", \"note\": \"Please use &lt;b&gt;secret&lt;/b&gt; as API key.\" } } }, \"triggers\": { (4) ... }, \"actions\": { (5) ... } } } . The component descriptor above defines the component title (1) and description (2). It also defines the fields used to ask the user to provide input for authentication (3). In this case a single field is define in which the user will input the API key for the Petstore API so that the component can communicate with the API on user’s behalf. The verification process is in the verifyCredentails.js file as mentioned above. The triggers (4) and actions (5) properties define the component’s triggers and actions. Triggers . Now let’s have a closer look on how to define the triggers. The example below demonstrates the triggers section from the component.json component descriptor file: . \"triggers\": { \"getPetsByStatusWithGenerators\": { (1) \"main\": \"./lib/triggers/getPetsByStatusWithGenerators.js\", (2) \"type\": \"polling\", (3) \"title\": \"Get Pets By Status With Generators\", \"fields\": { (4) \"status\": { \"label\": \"Pet Status\", \"required\": true, \"viewClass\": \"SelectView\", \"model\": { \"available\": \"Available\", \"pending\": \"Pending\", \"sold\": \"Sold\" }, \"prompt\": \"Select Pet Status\" } }, \"metadata\": { \"out\": \"./lib/schemas/getPetsByStatus.out.json\" (5) } } } . The example above demonstrates that the trigger with id getPetsByStatusWithGenerators (1) gets implemented by the function described in the getPetsByStatusWithGenerators.js file (2). The trigger is of polling type (3) meaning it will wake up periodically to poll for changes in the Petstore API. The triggers configuration is in the fields (4) and the out-metadata is in the file getPetsByStatus.out.json (5). Actions . Let us check also the actions. The example below demonstrates the actions section from the component.json component descriptor file: . \"actions\": { \"createPetWithPromise\": { (1) \"main\": \"./lib/actions/createPetWithPromise.js\", (2) \"title\": \"Create a Pet With Promise\", \"metadata\": { \"in\": \"./lib/schemas/createPet.in.json\", (3) \"out\": \"./lib/schemas/createPet.out.json\" (4) } } } . In the example above the action with id createPetWithPromise (1) gets implemented by the function described in the createPetWithPromise.js file (2). It is possible to give actions in the fields configurations like in the triggers part, however, this component has none of it. Compared to the triggers, the action has 2 metadata files. One is for in-metadata createPet.in.json (3) and the other is for out-metadata createPet.out.json (4). ",
    "url": "http://localhost:4000/docs/Connectors/building-nodejs-component.html#component-descriptor",
    "relUrl": "/docs/Connectors/building-nodejs-component.html#component-descriptor"
  },"263": {
    "doc": "Development Example for Node.js",
    "title": "Verify Credentials",
    "content": "When creating an component you may allow users to check entered credentials for validity, during the integration flow creation. This feature is useful when users need to type-in passwords, host-names, IP addresses, etc. Credentials verification is an optional step but it makes the user flow much more reliable and usable. For node.js component you can use any libraries and/or functionality at your disposal to authorise the user credentials. But for the platform to find and call your verification code you have to make sure that: . | Your component has a verifyCredentails.js file in the root of the folder structure | Your verifyCredentials.js file is a common.js module | It returns one function that accepts two parameters: credentials and cb (callback). | All the credentials for verification gets passed through credentials parameter which is an object. This object can contain the properties that match or correspond to the account definition parameters from the component.json. | . Here is the skeleton structure for the verify Credentials for a reference. You are welcome to use it as a starting point to write your own verifyCredentails.js: . // here you can place your variables // This function gets called by the platform to verify credentials module.exports = function verifyCredentials(credentials, cb) {     // In credentials you will find what users entered in account form     console.log('Credentials passed for verification %j', credentials)     if (true) {         // Verified         return cb(null, {verified: true});     } else {        // Verification failed        return cb(null , {verified: false});     } } . The use of any specific verification method depends on the project and the third party API that gets communicated the credentials with. We will not dive into details of every possible solution for your chosen third party API here. ",
    "url": "http://localhost:4000/docs/Connectors/building-nodejs-component.html#verify-credentials",
    "relUrl": "/docs/Connectors/building-nodejs-component.html#verify-credentials"
  },"264": {
    "doc": "Development Example for Node.js",
    "title": "Start your own",
    "content": "Now that you have the basics down, we suggest to start with determining what functionality your connector should include. Essential for that is the API you are connecting with. Actions and Triggers . For a quick start, you can check out to our contacts adapter template. It contains a baseline for all the functions you’ll need, which you can then adapt and expand for your own requirements. In the future, the sailor utility described earlier will be replaced by a new OIH-specific version called the Ferryman. The actual connector architecture and functionality will not be impacted by this. For more information about the Ferryman, check the Ferryman documentation . More advanced and optional additions to any connector include Conflict Management. ",
    "url": "http://localhost:4000/docs/Connectors/building-nodejs-component.html#start-your-own",
    "relUrl": "/docs/Connectors/building-nodejs-component.html#start-your-own"
  },"265": {
    "doc": "Development Example for Node.js",
    "title": "Development Example for Node.js",
    "content": " ",
    "url": "http://localhost:4000/docs/Connectors/building-nodejs-component.html",
    "relUrl": "/docs/Connectors/building-nodejs-component.html"
  },"266": {
    "doc": "Home",
    "title": "Home",
    "content": ". Getting Started How to install, run and create your first flows Services Overview and docs for each service Service Collaboration Basic service interactions API Reference API of each service Connector Guide How to build your own connector Data models Standardized data models How to contribute Help the open source community grow! Get in touch! Give us your questions and feedback ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },"267": {
    "doc": "Service Events",
    "title": "Service Events",
    "content": "Open Integration Hub services mostly communicate asynchronously and via message queues. Most services emitts and consumes several events. This document is designed to list all events per service. The currently used Event format provided by the Event Bus library was adopted for overall use. The schema is: . { \"headers\": { \"serviceName\": \"string\", \"createdAt\": \"date\", \"name\": \"string\", }, \"payload\": { \"user\": \"string\", \"tenant\": \"string\" } } . The field contents are: . | headers: An object containing metadata about the event itself . | serviceName: The name of the spawning service. If using the event-bus module, this will be filled in automatically | createdAt: A timestamp of the event’s creation. If using the event-bus module, this will be filled in automatically. | name: The name of the event. Also doubles as its routing key. | . | payload: An arbitrary JSON object, containing the content of the event. Two optional fields are reserved for logging purposes: . | user: The IAM-ID of a user who spawned the event | tenant: The IAM-ID of a tenant in which this event occurred | . | . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/serviceEvents.html",
    "relUrl": "/docs/Service%20Collaboration/serviceEvents.html"
  },"268": {
    "doc": "Service Events",
    "title": "IAM",
    "content": "General Events . | User created - iam.user.created . | User removed - iam.user.deleted . | Tenant created - iam.tenant.created . | Tenant removed - iam.tenant.deleted . | . AuditLog Events . | User: | . | Created, modified, deleted, assigned/removed to/from tenant, failed login attempt | e.g. iam.user.[operation] | . | Tenant: | . | Created, modified, deleted | e.g. iam.tenant.[operation] | . | Token / Roles / Permissions: | . | Created, modified, deleted | e.g. iam.role.[operation] | . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/serviceEvents.html#iam",
    "relUrl": "/docs/Service%20Collaboration/serviceEvents.html#iam"
  },"269": {
    "doc": "Service Events",
    "title": "Secret-Service",
    "content": "AuditLog Events . | Secret: | . | Created, deleted | e.g. secret-service.secret.[operation], secret-service.auth-client.[operation] | . | Access Token requested by Account: | . | e.g. secret-service.token.get | . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/serviceEvents.html#secret-service",
    "relUrl": "/docs/Service%20Collaboration/serviceEvents.html#secret-service"
  },"270": {
    "doc": "Service Events",
    "title": "Metadata",
    "content": "AuditLog Events . | Domain / Schema | . | Created, modified, deleted | E.g. metadata.schema.[operation] | . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/serviceEvents.html#metadata",
    "relUrl": "/docs/Service%20Collaboration/serviceEvents.html#metadata"
  },"271": {
    "doc": "Service Events",
    "title": "Flow Repository",
    "content": "AuditLog Events . | flowrepo.flow.created | flowrepo.flow.modified | flowrepo.flow.deleted | flowrepo.flow.starting | flowrepo.flow.stopping | . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/serviceEvents.html#flow-repository",
    "relUrl": "/docs/Service%20Collaboration/serviceEvents.html#flow-repository"
  },"272": {
    "doc": "Service Events",
    "title": "Component Orchestrator",
    "content": "AuditLog Events . | flow.started | flow.stopped | . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/serviceEvents.html#component-orchestrator",
    "relUrl": "/docs/Service%20Collaboration/serviceEvents.html#component-orchestrator"
  },"273": {
    "doc": "Update Propagation",
    "title": "Introduction",
    "content": "If a message is sent to the Open Integration Hub via a connector flow there are several services within the Open Integration Hub that collaborate in order to propagate this message. A connector flow always contains an adapter, transformer and Smart Data Framework (SDF) adapter. The following graphic illustrates how update propagation works while using the hub and spoke architecture within the Open Integration Hub: . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/updatePropagation.html#introduction",
    "relUrl": "/docs/Service%20Collaboration/updatePropagation.html#introduction"
  },"274": {
    "doc": "Update Propagation",
    "title": "Source Flow",
    "content": "In the source flow the triggering event is the reception of data by the adapter. This data is then passed to the transformer for a mapping and afterwards passed to the SDF adapter. The SDF adapter then sends the data to the Open Integration Hub (For more information: SDF Adapter). ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/updatePropagation.html#source-flow",
    "relUrl": "/docs/Service%20Collaboration/updatePropagation.html#source-flow"
  },"275": {
    "doc": "Update Propagation",
    "title": "Smart Data Framework",
    "content": "Once the message is received by the Data Hub it creates or updates an oihDataRecord. After the successful operation it emits an event which is then received by the Dispatcher Service. The Dispatcher looks up configurations in which the source-flowId matches the incoming flowId. After all targets have been identified, the Dispatcher sends the new/updated dataset to the SDF Adapter . Furthermore, it sends start request(s) to the flow repository which itself emits an event for component orchestrator (For more information: Dispatcher Service, Flow Repository). The component orchestrator send starts all needed containers. ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/updatePropagation.html#smart-data-framework",
    "relUrl": "/docs/Service%20Collaboration/updatePropagation.html#smart-data-framework"
  },"276": {
    "doc": "Update Propagation",
    "title": "Target Flows",
    "content": "Target flows always start with an instance of the SDF adapter. The SDF Adapter subscribes to a certain topic and passes the incoming message to transformer. After after a successful mapping, the Transformer passes the message to the adapter. The adapter then sends the data to the application and passes the recordUid of the entity to SDF adapter which sends it back to the Open Integration Hub for ID Linking (More on ID Linking: ID Linking in Open Integration Hub). ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/updatePropagation.html#target-flows",
    "relUrl": "/docs/Service%20Collaboration/updatePropagation.html#target-flows"
  },"277": {
    "doc": "Update Propagation",
    "title": "Create and Update Operations",
    "content": "Create Events . The following figure shows what happens within the Smart Data Framework if a create event is received: . Delete or Update Events . The following figure shows what happens within the Smart Data Framework if a delete or update event is received: . ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/updatePropagation.html#create-and-update-operations",
    "relUrl": "/docs/Service%20Collaboration/updatePropagation.html#create-and-update-operations"
  },"278": {
    "doc": "Update Propagation",
    "title": "Update Propagation",
    "content": " ",
    "url": "http://localhost:4000/docs/Service%20Collaboration/updatePropagation.html",
    "relUrl": "/docs/Service%20Collaboration/updatePropagation.html"
  }
}
